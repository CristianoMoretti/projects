{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5mb8FvvsbmuzGJrsfPA/e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristianoMoretti/projects/blob/main/PyTorch_Tutorials.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1-Main**"
      ],
      "metadata": {
        "id": "mUU7TjfXqfBY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CeIHPMjqSAz",
        "outputId": "3b1195ae-6594-4e02-f576-dd55a495b495"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n",
            "tensor([[0.8971, 0.8265, 0.6634],\n",
            "        [0.2373, 0.7803, 0.3374],\n",
            "        [0.1973, 0.6194, 0.8149],\n",
            "        [0.1256, 0.7065, 0.3133],\n",
            "        [0.3798, 0.2026, 0.9327]])\n",
            "False\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Using cpu device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "\n",
        "print(torch.__version__)\n",
        "\n",
        "x = torch.rand(5, 3)\n",
        "\n",
        "print(x)\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "\n",
        " # Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break\n",
        "\n",
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2-Tensor Basics**"
      ],
      "metadata": {
        "id": "znj7q9t5rJ2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.youtube.com/watch?v=exaWOE8jvy8&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=2\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "x = torch.rand(3, 2)\n",
        "print(x)\n",
        "\n",
        "x = torch.empty(2, 2, 3)\n",
        "print(x)\n",
        "\n",
        "x = torch.ones(2, 2)\n",
        "print(x)\n",
        "\n",
        "print(x.dtype)\n",
        "\n",
        "x = torch.ones(2,2, dtype=torch.int16)\n",
        "print(x)\n",
        "print(x.size())\n",
        "\n",
        "x = torch.tensor([2.5, 0.1])\n",
        "print(x)\n",
        "\n",
        "x = torch.rand(2,2)\n",
        "y = torch.rand(2,2)\n",
        "print(x)\n",
        "print(y)\n",
        "z = x + y\n",
        "print(z)\n",
        "\n",
        "z = torch.add(x, y)\n",
        "print(z)\n",
        "\n",
        "y.add_(x)\n",
        "print(y)\n",
        "\n",
        "z = x - y\n",
        "z = torch.sub(x, y)\n",
        "print(z)\n",
        "\n",
        "z = x * y\n",
        "z = torch.mul(x, y)\n",
        "y = y.mul_(x)\n",
        "print(z)\n",
        "print(y)\n",
        "\n",
        "x = torch.rand(5, 3)\n",
        "print(x)\n",
        "print(x[1, :])\n",
        "\n",
        "x = torch.rand(4, 4)\n",
        "print(x)\n",
        "\n",
        "y = x.view(16)\n",
        "print(y)\n",
        "y = x.view(-1)\n",
        "print(y)\n",
        "\n",
        "y = x.view(2, 8)\n",
        "print(y)\n",
        "y = x.view(-1, 8)\n",
        "print(y)\n",
        "\n",
        "# convert from numpy to torch and viceversa\n",
        "a = torch.ones(5)\n",
        "print(a)\n",
        "b = a.numpy()\n",
        "print(b)\n",
        "print(type(b))\n",
        "\n",
        "# on the cpu they both point to teh same memory location and if I chnage one I also change the other\n",
        "a.add_(1)\n",
        "print(a)\n",
        "print(b)\n",
        "\n",
        "a = np.ones(5)\n",
        "print(a)\n",
        "b = torch.from_numpy(a)\n",
        "print(b)\n",
        "a += 1\n",
        "print(a)\n",
        "print(b)\n",
        "\n",
        "# we can work on the gpu\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    # this will create the tensor on the GPU\n",
        "    x = torch.ones(5, device = device)\n",
        "    # or we can use the following\n",
        "    y = torch.ones(5)\n",
        "    # this will move the tensor from cpu to gpu\n",
        "    y = y.to(device)\n",
        "    # perform the computation on the gpu which is much faster\n",
        "    z = x + y\n",
        "    # z.numpy() wil retrun an error as numpy() an only be called on the cpu\n",
        "    z = z.to(\"cpu\")\n",
        "    z = z.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8s6mXgWarP5x",
        "outputId": "4dfa286c-6a21-4a2f-9d03-ae68674f3c30"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.7924, 0.5914],\n",
            "        [0.8501, 0.1672],\n",
            "        [0.3454, 0.9692]])\n",
            "tensor([[[1.6164e-04, 4.4608e-41, 1.6164e-04],\n",
            "         [4.4608e-41, 1.4013e-45, 0.0000e+00]],\n",
            "\n",
            "        [[1.4013e-45, 0.0000e+00, 1.4013e-45],\n",
            "         [0.0000e+00, 1.4013e-45, 0.0000e+00]]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "torch.float32\n",
            "tensor([[1, 1],\n",
            "        [1, 1]], dtype=torch.int16)\n",
            "torch.Size([2, 2])\n",
            "tensor([2.5000, 0.1000])\n",
            "tensor([[0.8463, 0.1601],\n",
            "        [0.6713, 0.7309]])\n",
            "tensor([[0.5710, 0.4537],\n",
            "        [0.0257, 0.2698]])\n",
            "tensor([[1.4173, 0.6137],\n",
            "        [0.6970, 1.0007]])\n",
            "tensor([[1.4173, 0.6137],\n",
            "        [0.6970, 1.0007]])\n",
            "tensor([[1.4173, 0.6137],\n",
            "        [0.6970, 1.0007]])\n",
            "tensor([[-0.5710, -0.4537],\n",
            "        [-0.0257, -0.2698]])\n",
            "tensor([[1.1995, 0.0982],\n",
            "        [0.4679, 0.7314]])\n",
            "tensor([[1.1995, 0.0982],\n",
            "        [0.4679, 0.7314]])\n",
            "tensor([[0.8894, 0.1077, 0.7245],\n",
            "        [0.6614, 0.6336, 0.1976],\n",
            "        [0.9692, 0.8613, 0.4344],\n",
            "        [0.8051, 0.8498, 0.0613],\n",
            "        [0.4724, 0.8592, 0.8871]])\n",
            "tensor([0.6614, 0.6336, 0.1976])\n",
            "tensor([[0.4399, 0.5727, 0.2434, 0.2969],\n",
            "        [0.8566, 0.0910, 0.5878, 0.7680],\n",
            "        [0.3390, 0.0804, 0.7585, 0.3865],\n",
            "        [0.4319, 0.7219, 0.9215, 0.0154]])\n",
            "tensor([0.4399, 0.5727, 0.2434, 0.2969, 0.8566, 0.0910, 0.5878, 0.7680, 0.3390,\n",
            "        0.0804, 0.7585, 0.3865, 0.4319, 0.7219, 0.9215, 0.0154])\n",
            "tensor([0.4399, 0.5727, 0.2434, 0.2969, 0.8566, 0.0910, 0.5878, 0.7680, 0.3390,\n",
            "        0.0804, 0.7585, 0.3865, 0.4319, 0.7219, 0.9215, 0.0154])\n",
            "tensor([[0.4399, 0.5727, 0.2434, 0.2969, 0.8566, 0.0910, 0.5878, 0.7680],\n",
            "        [0.3390, 0.0804, 0.7585, 0.3865, 0.4319, 0.7219, 0.9215, 0.0154]])\n",
            "tensor([[0.4399, 0.5727, 0.2434, 0.2969, 0.8566, 0.0910, 0.5878, 0.7680],\n",
            "        [0.3390, 0.0804, 0.7585, 0.3865, 0.4319, 0.7219, 0.9215, 0.0154]])\n",
            "tensor([1., 1., 1., 1., 1.])\n",
            "[1. 1. 1. 1. 1.]\n",
            "<class 'numpy.ndarray'>\n",
            "tensor([2., 2., 2., 2., 2.])\n",
            "[2. 2. 2. 2. 2.]\n",
            "[1. 1. 1. 1. 1.]\n",
            "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "[2. 2. 2. 2. 2.]\n",
            "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3-Autograd Gradient Calculation**"
      ],
      "metadata": {
        "id": "_T_M9J7JrYYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.youtube.com/watch?v=DbeIqrwb_dE&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=3\n",
        "\n",
        "import torch\n",
        "\n",
        "# the required_grad will tell pytorch to calculate the gradiens later on for the tensor during the optimization step\n",
        "x = torch.ones(5, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "# whenever we are doing operations with this tensor pytorch will create a computational graph for us\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "# this will create a computational graph\n",
        "# with a technique called back propagation we can calculated the gradients\n",
        "# first we do a forward pass where we calculate the output y. If required_grad = True pytorch will store a funciton for us called addbackwards.\n",
        "# this function is used in the backpropagation to get the gradients\n",
        "# gradients will be calculated in the backward pass\n",
        "y = x + 2\n",
        "# this will show the AddBackward fucntion has been added to the oupput ot the grad_fn attribute\n",
        "print(y)\n",
        "\n",
        "# our gradient function will be multiplication backwards function\n",
        "z = y*y*2\n",
        "print(z)\n",
        "# our gradient function will be the mean backwards function\n",
        "# z = z.mean()\n",
        "# print(z)\n",
        "\n",
        "# when we want to calculate the gradient we will call (if the output is a scalar like for z.mean())\n",
        "# z.backward() # dz/dx\n",
        "# x has an attribut grad where the gradients are stored\n",
        "# print(x.grad)\n",
        "\n",
        "# if we dont use the mean z has size 1 x 3, now we need to pass an argument as z.backward() can only we used with scalar outputs, to calculate the gradients we need to vreate a vector of the same size.\n",
        "# The gardients are the product of the Jacobian matrix and the gradient vector\n",
        "# if the output is not a scalar value we must give a vector\n",
        "# most of timres the output will be a scalar\n",
        "v = torch.tensor([0.1, 1.0, 0.001], dtype = torch.float32)\n",
        "z.backward(v)\n",
        "print(x.grad)\n",
        "\n",
        "# during trainibng we dont want to update the inputs just the weights, so we can call these different methods:\n",
        "# x.requires_grad_(False)\n",
        "# x.detach()\n",
        "# with torch.no_grad():\n",
        "\n",
        "# if the function has a trailing underscore in pythoin it means it will modify the variable in place\n",
        "# x.requires_grad_(False)\n",
        "# OR\n",
        "# y = x.detach() # this will create a new tensor with the same values\n",
        "print(y)\n",
        "# OR wrap in a with statement\n",
        "# with torch.no_grad():\n",
        "#    y = x + 2\n",
        "#    print(y)\n",
        "\n",
        "# whenever we all this function y = x + 2 the gradient for this function will accumulate in the .grad attribute\n",
        "\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "# say we have a trainign loop\n",
        "\n",
        "# first is only 1 iterations\n",
        "for epoch in range(3):\n",
        "    model_output = (weights*3).sum()\n",
        "    model_output.backward()\n",
        "    # all the gradients accumulate after each iteration which is somethign we dont want, so we must zero the gradients to ensure gradients to not accumulate\n",
        "    print(weights.grad)\n",
        "    weights.grad.zero_()\n",
        "\n",
        "\n",
        "# pytorch built in optimiser\n",
        "optimiser = torch.optim.SGD([weights], lr = 0.01)\n",
        "optimiser.step()\n",
        "# this will do exactly the same, it will reset the gradients.\n",
        "# we must empty the gradient before performign the next operation\n",
        "optimiser.zero_grad()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okNrXwiJriAp",
        "outputId": "8a23bf39-7479-4e7f-8cc7-ed7e79d01ee6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n",
            "tensor([ 0.6243,  0.6444, -1.6226], requires_grad=True)\n",
            "tensor([2.6243, 2.6444, 0.3774], grad_fn=<AddBackward0>)\n",
            "tensor([13.7741, 13.9858,  0.2849], grad_fn=<MulBackward0>)\n",
            "tensor([1.0497e+00, 1.0578e+01, 1.5096e-03])\n",
            "tensor([2.6243, 2.6444, 0.3774], grad_fn=<AddBackward0>)\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4-Back Propagation**"
      ],
      "metadata": {
        "id": "bkUP25jwr6q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.youtube.com/watch?v=3Kb0QS6z7WA&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=4\n",
        "\n",
        "import torch\n",
        "\n",
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "# forward pass and compute the loss\n",
        "y_hat = w * x\n",
        "loss = (y_hat-y)**2\n",
        "\n",
        "print(loss)\n",
        "\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n",
        "### updates weights\n",
        "### next forward and backwards"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaJs1S7bsAYN",
        "outputId": "f11ddf49-bc09-42e2-d9ed-dfea7a357d45"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5-Gradient Descent**"
      ],
      "metadata": {
        "id": "g9XVctZfsDTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.youtube.com/watch?v=E-I2DNVzQLg&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=5\n",
        "\n",
        "# this can be done in 4 different ways\n",
        "\n",
        "print('CASE 1')\n",
        "\n",
        "'''\n",
        "Prediction: Manually\n",
        "Gradient Computation: Manually\n",
        "Loss Computation: Manually\n",
        "Parameter updates: Manually\n",
        "'''\n",
        "\n",
        "# I am using linear regression\n",
        "import numpy as np\n",
        "\n",
        "# f = w * x\n",
        "# f = 2 * x\n",
        "\n",
        "X = np.array([1, 2, 3, 4], dtype = np.float32)\n",
        "# csince our formula is 2x\n",
        "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
        "\n",
        "# our weight initially is:\n",
        "w= 0.0\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "    return w * x\n",
        "# loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "    return ((y_predicted-y)**2).mean()\n",
        "\n",
        "\n",
        "# gradient\n",
        "# MSE = 1/N * (w*x - y)**2\n",
        "# dJ/dw = 1/N 2x (w*x -y)\n",
        "\n",
        "def gradient(x, y, y_predicted):\n",
        "    return np.dot(2*x, y_predicted-y).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    # prediction = forward pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # gradients\n",
        "    dw = gradient(X, Y, y_pred)\n",
        "\n",
        "    # update weights (we go in the negative direction of the training)\n",
        "    w -= learning_rate *dw\n",
        "\n",
        "    if epoch % 1 ==0: # I am printing every step)\n",
        "        print(f'epoch{epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Prediction: Manually\n",
        "Gradient Computation: Autograd\n",
        "Loss Computation: Manually\n",
        "Parameter updates: Manually\n",
        "'''\n",
        "print('CASE 2')\n",
        "\n",
        "# I am using linear regression\n",
        "import torch\n",
        "\n",
        "X = torch.tensor([1, 2, 3, 4], dtype = torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype = torch.float32)\n",
        "# our weight initially is 0:\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# f = w * x\n",
        "# f = 2 * x\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "    return w * x\n",
        "# loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "    return ((y_predicted-y)**2).mean()\n",
        "\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "# the backward propagation is not as exact as the numeric propagation, so I will need more iterations\n",
        "n_iters = 100\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    # prediction = forward pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # gradients = backward pass\n",
        "    # this wiull calculate the gradients respect to w\n",
        "    l.backward()\n",
        "\n",
        "    # update weights\n",
        "    # this should be not part of our gradient tracking graph (we need to use original w not w containing gradient updates), so I need to wrap with a torch.no_grad statement\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "\n",
        "    # zero gradients\n",
        "    # we must zero the gradients, otherwise each iteration the gradient will be calculted and accumulate in w.grad attribute\n",
        "    w.grad.zero_()\n",
        "\n",
        "    if epoch % 1 ==0: # I am printing every step)\n",
        "        print(f'epoch{epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "print('CASE 3')\n",
        "'''\n",
        "Prediction: Manually\n",
        "Gradient Computation: Autograd\n",
        "Loss Computation: PyTorch Loss\n",
        "Parameter updates: PyTorch Optimizer\n",
        "'''\n",
        "\n",
        "# respect to case 2 we need to replace the loss and optimizatio method\n",
        "import torch.nn as nn\n",
        "\n",
        "# General training pipeline in PyTorch, typically we have 3 steps\n",
        "# 1) Design our model (input size, output size, forward pass with all the operations and layers)\n",
        "# 2) Constract the loss and the optimizer\n",
        "# 3) Training Loop\n",
        "#     - forward pass: compute prediction\n",
        "#     - backward pass: we get the gradients\n",
        "#     - update weights\n",
        "#     - we iterate this a couple of times until we are done\n",
        "\n",
        "\n",
        "\n",
        "X = torch.tensor([1, 2, 3, 4], dtype = torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype = torch.float32)\n",
        "# our weight initially is 0:\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# f = w * x\n",
        "# f = 2 * x\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "    return w * x\n",
        "# loss = MSE\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "optimizer = torch.optim.SGD([w],  lr = learning_rate)\n",
        "\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "for epoch in range(n_iters):\n",
        "    # prediction = forward pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # gradients = backward pass\n",
        "    # this wiull calculate the gradients respect to w\n",
        "    l.backward()\n",
        "\n",
        "    # update weights\n",
        "\n",
        "    # we do not need to optimise the weights manually anymore, I can use an optimiser.step operation\n",
        "    '''\n",
        "        with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "\n",
        "    '''\n",
        "    optimizer.step()\n",
        "\n",
        "    # we still have to zero the gradients\n",
        "    # we must zero the gradients, otherwise each iteration the gradient will be calculted and accumulate in w.grad attribute\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if epoch % 1 ==0: # I am printing every step)\n",
        "        print(f'epoch{epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "print('CASE 4') # we replace our manaully implemented forward method with a PyTorch model\n",
        "'''\n",
        "Prediction: PyTorch Model\n",
        "Gradient Computation: Autograd\n",
        "Loss Computation: PyTorch Loss\n",
        "Parameter updates: PyTorch Optimizer\n",
        "'''\n",
        "\n",
        "# using the Torch model inputs need to be  different shape (2D array now)\n",
        "# the number of rows is the numbe of samples and for each row we have the number of features.\n",
        "# here we have 4 samples and 1 features\n",
        "# the same for our 4, it needs to have the same shape\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype = torch.float32)\n",
        "Y = torch.tensor([[2], [4], [6], [8]], dtype = torch.float32)\n",
        "\n",
        "X_test = torch.tensor([5], dtype = torch.float32) # we take a sample for testing, the sample is an array of values (in this case there is only 1). It has to be a Tensor as PyTorch works with tensors.\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples, n_features)\n",
        "\n",
        "# we define our model, we need an input size and an output size\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "# our weight initially is 0:\n",
        "# we don't need the weights tensor anymore as out model knows the parameters\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# f = w * x\n",
        "# f = 2 * x\n",
        "\n",
        "# model prediction, typically we have to designed it for ourselves but as this is very trivial we use the Linear model from PyTorch\n",
        "'''\n",
        "def forward(x):\n",
        "    return w * x\n",
        "'''\n",
        "# in this case we did not have to come up with a model ourselves and this model was already provided by PyTorch\n",
        "model = nn.Linear(in_features = input_size, out_features = output_size)\n",
        "\n",
        "\n",
        "# if we need a custom model, we need to derive it from nn.Module\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        # this is how we call the super constructor of the parent class\n",
        "        super(LinearRegression, self).__init__()\n",
        "        # define layers\n",
        "        self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin(x)\n",
        "\n",
        "model = LinearRegression(input_size, output_size)\n",
        "\n",
        "# loss = MSE\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "learning_rate = 0.01\n",
        "n_iters = 10\n",
        "# we repalce weights entered manually with model.parameters\n",
        "# optimizer = torch.optim.SGD([w],  lr = learning_rate)\n",
        "optimizer = torch.optim.SGD(model.parameters(),  lr = learning_rate)\n",
        "\n",
        "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}') # we use method item() to return a float\n",
        "\n",
        "# Training\n",
        "for epoch in range(n_iters):\n",
        "    # prediction = forward pass\n",
        "    # for the prediction we simply call the model\n",
        "    # y_pred = forward(X)\n",
        "    y_pred = model(X)\n",
        "\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # gradients = backward pass\n",
        "    # this wiull calculate the gradients respect to w\n",
        "    l.backward()\n",
        "\n",
        "    # update weights\n",
        "\n",
        "    # we do not need to optimise the weights manually anymore, I can use an optimiser.step operation\n",
        "    '''\n",
        "        with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "\n",
        "    '''\n",
        "    optimizer.step()\n",
        "\n",
        "    # we still have to zero the gradients\n",
        "    # we must zero the gradients, otherwise each iteration the gradient will be calculted and accumulate in w.grad attribute\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if epoch % 1 ==0: # I am printing every step)\n",
        "        # if we want to print the again we have to unpack them\n",
        "        [w, b] = model.parameters() # this will return a list of list for the weights and a list for the bias\n",
        "        # print('b =', b.item())\n",
        "        # print(f'epoch{epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "        print(f'epoch{epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwpxYvM_sGl6",
        "outputId": "aa10148e-2e23-4e1b-fa5d-e13a8df23a77"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CASE 1\n",
            "Prediction before training: f(5) = 0.000\n",
            "epoch1: w = 1.200, loss = 30.00000000\n",
            "epoch2: w = 1.680, loss = 4.79999924\n",
            "epoch3: w = 1.872, loss = 0.76800019\n",
            "epoch4: w = 1.949, loss = 0.12288000\n",
            "epoch5: w = 1.980, loss = 0.01966083\n",
            "epoch6: w = 1.992, loss = 0.00314574\n",
            "epoch7: w = 1.997, loss = 0.00050331\n",
            "epoch8: w = 1.999, loss = 0.00008053\n",
            "epoch9: w = 1.999, loss = 0.00001288\n",
            "epoch10: w = 2.000, loss = 0.00000206\n",
            "epoch11: w = 2.000, loss = 0.00000033\n",
            "epoch12: w = 2.000, loss = 0.00000005\n",
            "epoch13: w = 2.000, loss = 0.00000001\n",
            "epoch14: w = 2.000, loss = 0.00000000\n",
            "epoch15: w = 2.000, loss = 0.00000000\n",
            "epoch16: w = 2.000, loss = 0.00000000\n",
            "epoch17: w = 2.000, loss = 0.00000000\n",
            "epoch18: w = 2.000, loss = 0.00000000\n",
            "epoch19: w = 2.000, loss = 0.00000000\n",
            "epoch20: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) = 10.000\n",
            "CASE 2\n",
            "Prediction before training: f(5) = 0.000\n",
            "epoch1: w = 0.300, loss = 30.00000000\n",
            "epoch2: w = 0.555, loss = 21.67499924\n",
            "epoch3: w = 0.772, loss = 15.66018772\n",
            "epoch4: w = 0.956, loss = 11.31448650\n",
            "epoch5: w = 1.113, loss = 8.17471695\n",
            "epoch6: w = 1.246, loss = 5.90623236\n",
            "epoch7: w = 1.359, loss = 4.26725292\n",
            "epoch8: w = 1.455, loss = 3.08308983\n",
            "epoch9: w = 1.537, loss = 2.22753215\n",
            "epoch10: w = 1.606, loss = 1.60939169\n",
            "epoch11: w = 1.665, loss = 1.16278565\n",
            "epoch12: w = 1.716, loss = 0.84011245\n",
            "epoch13: w = 1.758, loss = 0.60698116\n",
            "epoch14: w = 1.794, loss = 0.43854395\n",
            "epoch15: w = 1.825, loss = 0.31684780\n",
            "epoch16: w = 1.851, loss = 0.22892261\n",
            "epoch17: w = 1.874, loss = 0.16539653\n",
            "epoch18: w = 1.893, loss = 0.11949898\n",
            "epoch19: w = 1.909, loss = 0.08633806\n",
            "epoch20: w = 1.922, loss = 0.06237914\n",
            "epoch21: w = 1.934, loss = 0.04506890\n",
            "epoch22: w = 1.944, loss = 0.03256231\n",
            "epoch23: w = 1.952, loss = 0.02352631\n",
            "epoch24: w = 1.960, loss = 0.01699772\n",
            "epoch25: w = 1.966, loss = 0.01228084\n",
            "epoch26: w = 1.971, loss = 0.00887291\n",
            "epoch27: w = 1.975, loss = 0.00641066\n",
            "epoch28: w = 1.979, loss = 0.00463169\n",
            "epoch29: w = 1.982, loss = 0.00334642\n",
            "epoch30: w = 1.985, loss = 0.00241778\n",
            "epoch31: w = 1.987, loss = 0.00174685\n",
            "epoch32: w = 1.989, loss = 0.00126211\n",
            "epoch33: w = 1.991, loss = 0.00091188\n",
            "epoch34: w = 1.992, loss = 0.00065882\n",
            "epoch35: w = 1.993, loss = 0.00047601\n",
            "epoch36: w = 1.994, loss = 0.00034392\n",
            "epoch37: w = 1.995, loss = 0.00024848\n",
            "epoch38: w = 1.996, loss = 0.00017952\n",
            "epoch39: w = 1.996, loss = 0.00012971\n",
            "epoch40: w = 1.997, loss = 0.00009371\n",
            "epoch41: w = 1.997, loss = 0.00006770\n",
            "epoch42: w = 1.998, loss = 0.00004891\n",
            "epoch43: w = 1.998, loss = 0.00003534\n",
            "epoch44: w = 1.998, loss = 0.00002553\n",
            "epoch45: w = 1.999, loss = 0.00001845\n",
            "epoch46: w = 1.999, loss = 0.00001333\n",
            "epoch47: w = 1.999, loss = 0.00000963\n",
            "epoch48: w = 1.999, loss = 0.00000696\n",
            "epoch49: w = 1.999, loss = 0.00000503\n",
            "epoch50: w = 1.999, loss = 0.00000363\n",
            "epoch51: w = 1.999, loss = 0.00000262\n",
            "epoch52: w = 2.000, loss = 0.00000190\n",
            "epoch53: w = 2.000, loss = 0.00000137\n",
            "epoch54: w = 2.000, loss = 0.00000099\n",
            "epoch55: w = 2.000, loss = 0.00000071\n",
            "epoch56: w = 2.000, loss = 0.00000052\n",
            "epoch57: w = 2.000, loss = 0.00000037\n",
            "epoch58: w = 2.000, loss = 0.00000027\n",
            "epoch59: w = 2.000, loss = 0.00000019\n",
            "epoch60: w = 2.000, loss = 0.00000014\n",
            "epoch61: w = 2.000, loss = 0.00000010\n",
            "epoch62: w = 2.000, loss = 0.00000007\n",
            "epoch63: w = 2.000, loss = 0.00000005\n",
            "epoch64: w = 2.000, loss = 0.00000004\n",
            "epoch65: w = 2.000, loss = 0.00000003\n",
            "epoch66: w = 2.000, loss = 0.00000002\n",
            "epoch67: w = 2.000, loss = 0.00000001\n",
            "epoch68: w = 2.000, loss = 0.00000001\n",
            "epoch69: w = 2.000, loss = 0.00000001\n",
            "epoch70: w = 2.000, loss = 0.00000001\n",
            "epoch71: w = 2.000, loss = 0.00000000\n",
            "epoch72: w = 2.000, loss = 0.00000000\n",
            "epoch73: w = 2.000, loss = 0.00000000\n",
            "epoch74: w = 2.000, loss = 0.00000000\n",
            "epoch75: w = 2.000, loss = 0.00000000\n",
            "epoch76: w = 2.000, loss = 0.00000000\n",
            "epoch77: w = 2.000, loss = 0.00000000\n",
            "epoch78: w = 2.000, loss = 0.00000000\n",
            "epoch79: w = 2.000, loss = 0.00000000\n",
            "epoch80: w = 2.000, loss = 0.00000000\n",
            "epoch81: w = 2.000, loss = 0.00000000\n",
            "epoch82: w = 2.000, loss = 0.00000000\n",
            "epoch83: w = 2.000, loss = 0.00000000\n",
            "epoch84: w = 2.000, loss = 0.00000000\n",
            "epoch85: w = 2.000, loss = 0.00000000\n",
            "epoch86: w = 2.000, loss = 0.00000000\n",
            "epoch87: w = 2.000, loss = 0.00000000\n",
            "epoch88: w = 2.000, loss = 0.00000000\n",
            "epoch89: w = 2.000, loss = 0.00000000\n",
            "epoch90: w = 2.000, loss = 0.00000000\n",
            "epoch91: w = 2.000, loss = 0.00000000\n",
            "epoch92: w = 2.000, loss = 0.00000000\n",
            "epoch93: w = 2.000, loss = 0.00000000\n",
            "epoch94: w = 2.000, loss = 0.00000000\n",
            "epoch95: w = 2.000, loss = 0.00000000\n",
            "epoch96: w = 2.000, loss = 0.00000000\n",
            "epoch97: w = 2.000, loss = 0.00000000\n",
            "epoch98: w = 2.000, loss = 0.00000000\n",
            "epoch99: w = 2.000, loss = 0.00000000\n",
            "epoch100: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) = 10.000\n",
            "CASE 3\n",
            "Prediction before training: f(5) = 0.000\n",
            "epoch1: w = 0.300, loss = 30.00000000\n",
            "epoch2: w = 0.555, loss = 21.67499924\n",
            "epoch3: w = 0.772, loss = 15.66018772\n",
            "epoch4: w = 0.956, loss = 11.31448650\n",
            "epoch5: w = 1.113, loss = 8.17471695\n",
            "epoch6: w = 1.246, loss = 5.90623236\n",
            "epoch7: w = 1.359, loss = 4.26725292\n",
            "epoch8: w = 1.455, loss = 3.08308983\n",
            "epoch9: w = 1.537, loss = 2.22753215\n",
            "epoch10: w = 1.606, loss = 1.60939169\n",
            "epoch11: w = 1.665, loss = 1.16278565\n",
            "epoch12: w = 1.716, loss = 0.84011245\n",
            "epoch13: w = 1.758, loss = 0.60698116\n",
            "epoch14: w = 1.794, loss = 0.43854395\n",
            "epoch15: w = 1.825, loss = 0.31684780\n",
            "epoch16: w = 1.851, loss = 0.22892261\n",
            "epoch17: w = 1.874, loss = 0.16539653\n",
            "epoch18: w = 1.893, loss = 0.11949898\n",
            "epoch19: w = 1.909, loss = 0.08633806\n",
            "epoch20: w = 1.922, loss = 0.06237914\n",
            "epoch21: w = 1.934, loss = 0.04506890\n",
            "epoch22: w = 1.944, loss = 0.03256231\n",
            "epoch23: w = 1.952, loss = 0.02352631\n",
            "epoch24: w = 1.960, loss = 0.01699772\n",
            "epoch25: w = 1.966, loss = 0.01228084\n",
            "epoch26: w = 1.971, loss = 0.00887291\n",
            "epoch27: w = 1.975, loss = 0.00641066\n",
            "epoch28: w = 1.979, loss = 0.00463169\n",
            "epoch29: w = 1.982, loss = 0.00334642\n",
            "epoch30: w = 1.985, loss = 0.00241778\n",
            "epoch31: w = 1.987, loss = 0.00174685\n",
            "epoch32: w = 1.989, loss = 0.00126211\n",
            "epoch33: w = 1.991, loss = 0.00091188\n",
            "epoch34: w = 1.992, loss = 0.00065882\n",
            "epoch35: w = 1.993, loss = 0.00047601\n",
            "epoch36: w = 1.994, loss = 0.00034392\n",
            "epoch37: w = 1.995, loss = 0.00024848\n",
            "epoch38: w = 1.996, loss = 0.00017952\n",
            "epoch39: w = 1.996, loss = 0.00012971\n",
            "epoch40: w = 1.997, loss = 0.00009371\n",
            "epoch41: w = 1.997, loss = 0.00006770\n",
            "epoch42: w = 1.998, loss = 0.00004891\n",
            "epoch43: w = 1.998, loss = 0.00003534\n",
            "epoch44: w = 1.998, loss = 0.00002553\n",
            "epoch45: w = 1.999, loss = 0.00001845\n",
            "epoch46: w = 1.999, loss = 0.00001333\n",
            "epoch47: w = 1.999, loss = 0.00000963\n",
            "epoch48: w = 1.999, loss = 0.00000696\n",
            "epoch49: w = 1.999, loss = 0.00000503\n",
            "epoch50: w = 1.999, loss = 0.00000363\n",
            "epoch51: w = 1.999, loss = 0.00000262\n",
            "epoch52: w = 2.000, loss = 0.00000190\n",
            "epoch53: w = 2.000, loss = 0.00000137\n",
            "epoch54: w = 2.000, loss = 0.00000099\n",
            "epoch55: w = 2.000, loss = 0.00000071\n",
            "epoch56: w = 2.000, loss = 0.00000052\n",
            "epoch57: w = 2.000, loss = 0.00000037\n",
            "epoch58: w = 2.000, loss = 0.00000027\n",
            "epoch59: w = 2.000, loss = 0.00000019\n",
            "epoch60: w = 2.000, loss = 0.00000014\n",
            "epoch61: w = 2.000, loss = 0.00000010\n",
            "epoch62: w = 2.000, loss = 0.00000007\n",
            "epoch63: w = 2.000, loss = 0.00000005\n",
            "epoch64: w = 2.000, loss = 0.00000004\n",
            "epoch65: w = 2.000, loss = 0.00000003\n",
            "epoch66: w = 2.000, loss = 0.00000002\n",
            "epoch67: w = 2.000, loss = 0.00000001\n",
            "epoch68: w = 2.000, loss = 0.00000001\n",
            "epoch69: w = 2.000, loss = 0.00000001\n",
            "epoch70: w = 2.000, loss = 0.00000001\n",
            "epoch71: w = 2.000, loss = 0.00000000\n",
            "epoch72: w = 2.000, loss = 0.00000000\n",
            "epoch73: w = 2.000, loss = 0.00000000\n",
            "epoch74: w = 2.000, loss = 0.00000000\n",
            "epoch75: w = 2.000, loss = 0.00000000\n",
            "epoch76: w = 2.000, loss = 0.00000000\n",
            "epoch77: w = 2.000, loss = 0.00000000\n",
            "epoch78: w = 2.000, loss = 0.00000000\n",
            "epoch79: w = 2.000, loss = 0.00000000\n",
            "epoch80: w = 2.000, loss = 0.00000000\n",
            "epoch81: w = 2.000, loss = 0.00000000\n",
            "epoch82: w = 2.000, loss = 0.00000000\n",
            "epoch83: w = 2.000, loss = 0.00000000\n",
            "epoch84: w = 2.000, loss = 0.00000000\n",
            "epoch85: w = 2.000, loss = 0.00000000\n",
            "epoch86: w = 2.000, loss = 0.00000000\n",
            "epoch87: w = 2.000, loss = 0.00000000\n",
            "epoch88: w = 2.000, loss = 0.00000000\n",
            "epoch89: w = 2.000, loss = 0.00000000\n",
            "epoch90: w = 2.000, loss = 0.00000000\n",
            "epoch91: w = 2.000, loss = 0.00000000\n",
            "epoch92: w = 2.000, loss = 0.00000000\n",
            "epoch93: w = 2.000, loss = 0.00000000\n",
            "epoch94: w = 2.000, loss = 0.00000000\n",
            "epoch95: w = 2.000, loss = 0.00000000\n",
            "epoch96: w = 2.000, loss = 0.00000000\n",
            "epoch97: w = 2.000, loss = 0.00000000\n",
            "epoch98: w = 2.000, loss = 0.00000000\n",
            "epoch99: w = 2.000, loss = 0.00000000\n",
            "epoch100: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) = 10.000\n",
            "CASE 4\n",
            "4 1\n",
            "Prediction before training: f(5) = 1.381\n",
            "epoch1: w = 0.378, loss = 19.98439026\n",
            "epoch2: w = 0.582, loss = 13.93971443\n",
            "epoch3: w = 0.752, loss = 9.74500465\n",
            "epoch4: w = 0.894, loss = 6.83395100\n",
            "epoch5: w = 1.012, loss = 4.81360054\n",
            "epoch6: w = 1.111, loss = 3.41129470\n",
            "epoch7: w = 1.193, loss = 2.43783832\n",
            "epoch8: w = 1.262, loss = 1.76195359\n",
            "epoch9: w = 1.319, loss = 1.29255080\n",
            "epoch10: w = 1.368, loss = 0.96642375\n",
            "Prediction after training: f(5) = 7.929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7-Linear Regression**"
      ],
      "metadata": {
        "id": "TmUCbMsnsgTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Design model (input, output size, forward pass)\n",
        "# 2) Construct loss and optimiser\n",
        "# 3) Training loop\n",
        "#   - forward pass: compute prediction and loss\n",
        "#   - backward pass: gradients\n",
        "#   - update weights\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn # we import the neural network module\n",
        "import numpy as np # for data transformation\n",
        "from sklearn import datasets # we want to generate a regression dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 0) prepare data\n",
        "# Let's generate a regression dataset\n",
        "\n",
        "X_numpy, y_numpy = datasets.make_regression(n_samples = 100, n_features = 1, noise=20, random_state=1)\n",
        "print(X_numpy)\n",
        "print(y_numpy)\n",
        "# we need to convert the numpy array from double to float\n",
        "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
        "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "# we need to re-shape our y as the whole shape from dataset is only one array containing 100 values.\n",
        "# we can do in 2 different ways\n",
        "# y = y.view(-1, 1)\n",
        "y = y.view(y.shape[0], 1)\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "\n",
        "# 1) model\n",
        "input_size = n_features\n",
        "output_size = 1\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "# 2) loss and optimizer\n",
        "# we use a built in loss function from PyTorch\n",
        "learning_rate = 0.01\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "\n",
        "# 3) training loop\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    # forward pass and loss\n",
        "    y_predicted = model(X)\n",
        "    loss = criterion(y_predicted, y)\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    #reset gradients because any time we call the backward function it will sum up the gradients in the .grad attribute\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        w, b = model.parameters()\n",
        "        # print(w.shape)\n",
        "        #' print(b.shape)\n",
        "        print(f'epoch{epoch+1}, loss = {loss.item():.4f}, weight = {w[0][0].item():.4f}')\n",
        "\n",
        "\n",
        "# plot\n",
        "# let's get all the predicted values, we call out final model.\n",
        "# in model we have the required_grad attribute set to true, so we wanto to generate a new tensor where the required_grad attribute is set to false\n",
        "# we convert to numpy\n",
        "\n",
        "predicted = model(X).detach().numpy()\n",
        "\n",
        "plt.plot(X_numpy, y_numpy,  'ro')\n",
        "plt.plot(X_numpy, predicted, 'b')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aHQq7pM4soW5",
        "outputId": "54472392-db91-4a2d-d44e-07861d5e44fc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.61175641]\n",
            " [-0.24937038]\n",
            " [ 0.48851815]\n",
            " [ 0.76201118]\n",
            " [ 1.51981682]\n",
            " [ 0.37756379]\n",
            " [ 0.51292982]\n",
            " [-0.67124613]\n",
            " [-1.39649634]\n",
            " [ 0.31563495]\n",
            " [-0.63699565]\n",
            " [-0.39675353]\n",
            " [-1.10061918]\n",
            " [ 0.90085595]\n",
            " [-1.09989127]\n",
            " [ 0.82797464]\n",
            " [-0.07557171]\n",
            " [-0.35224985]\n",
            " [-0.67066229]\n",
            " [-1.07296862]\n",
            " [-0.30620401]\n",
            " [ 2.18557541]\n",
            " [ 0.86540763]\n",
            " [ 0.19829972]\n",
            " [-0.38405435]\n",
            " [-0.68372786]\n",
            " [ 0.05080775]\n",
            " [ 0.58281521]\n",
            " [ 1.25286816]\n",
            " [-0.75439794]\n",
            " [-0.34934272]\n",
            " [-0.88762896]\n",
            " [ 0.18656139]\n",
            " [ 0.87616892]\n",
            " [ 0.83898341]\n",
            " [-0.50446586]\n",
            " [-0.34385368]\n",
            " [ 1.6924546 ]\n",
            " [-2.3015387 ]\n",
            " [ 0.93110208]\n",
            " [ 2.10025514]\n",
            " [ 1.46210794]\n",
            " [-0.84520564]\n",
            " [-0.87785842]\n",
            " [-0.3224172 ]\n",
            " [ 0.88514116]\n",
            " [ 0.16003707]\n",
            " [ 1.13162939]\n",
            " [-0.37528495]\n",
            " [ 0.50249434]\n",
            " [-0.20889423]\n",
            " [ 0.12015895]\n",
            " [ 0.58662319]\n",
            " [ 0.3190391 ]\n",
            " [-0.69166075]\n",
            " [ 0.69803203]\n",
            " [ 1.19891788]\n",
            " [-0.20075807]\n",
            " [ 0.53035547]\n",
            " [ 0.74204416]\n",
            " [ 0.41005165]\n",
            " [ 0.11900865]\n",
            " [-0.7612069 ]\n",
            " [ 0.42349435]\n",
            " [ 0.30017032]\n",
            " [-1.1425182 ]\n",
            " [ 0.18515642]\n",
            " [-0.93576943]\n",
            " [-0.62000084]\n",
            " [-1.11731035]\n",
            " [-1.44411381]\n",
            " [-0.22232814]\n",
            " [ 1.62434536]\n",
            " [ 0.61720311]\n",
            " [-0.6871727 ]\n",
            " [ 0.07734007]\n",
            " [-0.0126646 ]\n",
            " [-0.63873041]\n",
            " [ 1.13376944]\n",
            " [ 1.74481176]\n",
            " [ 0.90159072]\n",
            " [-2.06014071]\n",
            " [ 0.2344157 ]\n",
            " [-0.17242821]\n",
            " [ 0.12182127]\n",
            " [ 1.14472371]\n",
            " [-0.12289023]\n",
            " [-0.74715829]\n",
            " [ 0.28558733]\n",
            " [-2.02220122]\n",
            " [ 0.23009474]\n",
            " [-0.26788808]\n",
            " [-0.52817175]\n",
            " [ 1.12948391]\n",
            " [ 0.19091548]\n",
            " [-0.29809284]\n",
            " [ 1.65980218]\n",
            " [ 0.04359686]\n",
            " [ 0.04221375]\n",
            " [-0.19183555]]\n",
            "[-5.55385928e+01 -1.06619847e+01  2.27574081e+01  1.01096129e+02\n",
            "  1.44337558e+02  3.32888330e+01  3.30152710e+01 -2.58869694e+01\n",
            " -9.96391397e+01  2.38030714e+01 -4.55886864e+01 -8.33875709e+00\n",
            " -9.53154191e+01  3.64072963e+01 -8.72926036e+01  6.76693724e+01\n",
            " -1.36866100e+01 -5.54414224e+01 -6.53402399e+01 -5.44497141e+01\n",
            " -2.88351332e+01  1.78835048e+02  6.50839520e+01  2.66683131e+01\n",
            " -1.85459706e+01 -4.14990408e+01  8.55827764e-01  4.45616521e+01\n",
            "  1.15984811e+02 -6.46197993e+01 -2.59312718e+01 -6.08820426e+01\n",
            "  1.87195482e+01  7.50696998e+01  1.17203175e+02 -2.26982690e+01\n",
            " -5.63625811e+01  1.80837188e+02 -1.92574950e+02  6.85032358e+01\n",
            "  1.65522025e+02  1.05000391e+02 -7.04338757e+01 -5.87693362e+01\n",
            " -4.15757142e+01  7.32472269e+01  4.09664082e+01  8.04619460e+01\n",
            " -2.87939943e+01  3.42341054e+01 -4.17148764e+01  1.43547375e+01\n",
            "  7.93363240e+01  2.71292073e+01 -3.94873551e+01  6.68052070e+01\n",
            "  9.55308437e+01  3.56104075e+00  1.08568943e-01  5.64952893e+01\n",
            "  5.15753413e+01 -2.09741113e+00 -2.66559439e+01  3.97419819e+01\n",
            "  3.61014055e+01 -7.56019440e+01  1.97126065e+01 -7.16010331e+01\n",
            " -1.99035774e+01 -7.67084296e+01 -1.18338274e+02 -2.98246083e+01\n",
            "  1.51082783e+02  5.29226489e+01 -5.95516769e+01  3.07214747e+01\n",
            " -2.93550664e+01 -4.47861678e+01  1.00058362e+02  1.50576548e+02\n",
            "  1.22000422e+02 -1.81857166e+02  3.47392430e+00 -2.29801423e+01\n",
            "  4.51842772e+01  9.86063300e+01 -9.27788339e+00 -5.24778810e+01\n",
            "  3.85928318e+01 -1.99972423e+02 -9.52014653e+00 -3.47236288e+00\n",
            " -3.53122497e+01  7.54057582e+01  1.75701411e+01 -2.39600185e+01\n",
            "  1.32085955e+02  2.06075830e+01  5.11112097e+01 -2.63060397e+01]\n",
            "torch.Size([100, 1])\n",
            "torch.Size([100])\n",
            "epoch1, loss = 5699.0767, weight = 1.7256\n",
            "epoch2, loss = 5528.5073, weight = 3.0023\n",
            "epoch3, loss = 5363.4126, weight = 4.2587\n",
            "epoch4, loss = 5203.6152, weight = 5.4951\n",
            "epoch5, loss = 5048.9419, weight = 6.7119\n",
            "epoch6, loss = 4899.2280, weight = 7.9092\n",
            "epoch7, loss = 4754.3120, weight = 9.0875\n",
            "epoch8, loss = 4614.0376, weight = 10.2471\n",
            "epoch9, loss = 4478.2559, weight = 11.3882\n",
            "epoch10, loss = 4346.8203, weight = 12.5112\n",
            "epoch11, loss = 4219.5908, weight = 13.6163\n",
            "epoch12, loss = 4096.4307, weight = 14.7039\n",
            "epoch13, loss = 3977.2087, weight = 15.7741\n",
            "epoch14, loss = 3861.7981, weight = 16.8274\n",
            "epoch15, loss = 3750.0747, weight = 17.8639\n",
            "epoch16, loss = 3641.9197, weight = 18.8839\n",
            "epoch17, loss = 3537.2180, weight = 19.8878\n",
            "epoch18, loss = 3435.8584, weight = 20.8756\n",
            "epoch19, loss = 3337.7324, weight = 21.8478\n",
            "epoch20, loss = 3242.7366, weight = 22.8046\n",
            "epoch21, loss = 3150.7698, weight = 23.7461\n",
            "epoch22, loss = 3061.7341, weight = 24.6727\n",
            "epoch23, loss = 2975.5354, weight = 25.5846\n",
            "epoch24, loss = 2892.0818, weight = 26.4820\n",
            "epoch25, loss = 2811.2859, weight = 27.3652\n",
            "epoch26, loss = 2733.0618, weight = 28.2344\n",
            "epoch27, loss = 2657.3267, weight = 29.0897\n",
            "epoch28, loss = 2584.0007, weight = 29.9315\n",
            "epoch29, loss = 2513.0063, weight = 30.7599\n",
            "epoch30, loss = 2444.2683, weight = 31.5752\n",
            "epoch31, loss = 2377.7151, weight = 32.3776\n",
            "epoch32, loss = 2313.2756, weight = 33.1672\n",
            "epoch33, loss = 2250.8835, weight = 33.9444\n",
            "epoch34, loss = 2190.4717, weight = 34.7092\n",
            "epoch35, loss = 2131.9768, weight = 35.4618\n",
            "epoch36, loss = 2075.3384, weight = 36.2026\n",
            "epoch37, loss = 2020.4960, weight = 36.9316\n",
            "epoch38, loss = 1967.3928, weight = 37.6490\n",
            "epoch39, loss = 1915.9727, weight = 38.3551\n",
            "epoch40, loss = 1866.1821, weight = 39.0500\n",
            "epoch41, loss = 1817.9689, weight = 39.7339\n",
            "epoch42, loss = 1771.2827, weight = 40.4070\n",
            "epoch43, loss = 1726.0746, weight = 41.0694\n",
            "epoch44, loss = 1682.2972, weight = 41.7213\n",
            "epoch45, loss = 1639.9052, weight = 42.3629\n",
            "epoch46, loss = 1598.8538, weight = 42.9943\n",
            "epoch47, loss = 1559.1011, weight = 43.6157\n",
            "epoch48, loss = 1520.6050, weight = 44.2273\n",
            "epoch49, loss = 1483.3254, weight = 44.8292\n",
            "epoch50, loss = 1447.2238, weight = 45.4216\n",
            "epoch51, loss = 1412.2623, weight = 46.0047\n",
            "epoch52, loss = 1378.4052, weight = 46.5785\n",
            "epoch53, loss = 1345.6167, weight = 47.1432\n",
            "epoch54, loss = 1313.8633, weight = 47.6990\n",
            "epoch55, loss = 1283.1118, weight = 48.2460\n",
            "epoch56, loss = 1253.3302, weight = 48.7843\n",
            "epoch57, loss = 1224.4879, weight = 49.3142\n",
            "epoch58, loss = 1196.5551, weight = 49.8356\n",
            "epoch59, loss = 1169.5023, weight = 50.3488\n",
            "epoch60, loss = 1143.3025, weight = 50.8539\n",
            "epoch61, loss = 1117.9279, weight = 51.3511\n",
            "epoch62, loss = 1093.3524, weight = 51.8403\n",
            "epoch63, loss = 1069.5509, weight = 52.3219\n",
            "epoch64, loss = 1046.4984, weight = 52.7958\n",
            "epoch65, loss = 1024.1718, weight = 53.2622\n",
            "epoch66, loss = 1002.5472, weight = 53.7213\n",
            "epoch67, loss = 981.6031, weight = 54.1731\n",
            "epoch68, loss = 961.3176, weight = 54.6178\n",
            "epoch69, loss = 941.6699, weight = 55.0555\n",
            "epoch70, loss = 922.6398, weight = 55.4862\n",
            "epoch71, loss = 904.2078, weight = 55.9102\n",
            "epoch72, loss = 886.3549, weight = 56.3275\n",
            "epoch73, loss = 869.0626, weight = 56.7381\n",
            "epoch74, loss = 852.3135, weight = 57.1423\n",
            "epoch75, loss = 836.0902, weight = 57.5401\n",
            "epoch76, loss = 820.3762, weight = 57.9317\n",
            "epoch77, loss = 805.1552, weight = 58.3170\n",
            "epoch78, loss = 790.4119, weight = 58.6963\n",
            "epoch79, loss = 776.1309, weight = 59.0696\n",
            "epoch80, loss = 762.2980, weight = 59.4370\n",
            "epoch81, loss = 748.8987, weight = 59.7987\n",
            "epoch82, loss = 735.9193, weight = 60.1546\n",
            "epoch83, loss = 723.3468, weight = 60.5049\n",
            "epoch84, loss = 711.1683, weight = 60.8497\n",
            "epoch85, loss = 699.3713, weight = 61.1890\n",
            "epoch86, loss = 687.9436, weight = 61.5230\n",
            "epoch87, loss = 676.8740, weight = 61.8517\n",
            "epoch88, loss = 666.1508, weight = 62.1753\n",
            "epoch89, loss = 655.7633, weight = 62.4937\n",
            "epoch90, loss = 645.7008, weight = 62.8072\n",
            "epoch91, loss = 635.9532, weight = 63.1157\n",
            "epoch92, loss = 626.5105, weight = 63.4193\n",
            "epoch93, loss = 617.3630, weight = 63.7182\n",
            "epoch94, loss = 608.5018, weight = 64.0123\n",
            "epoch95, loss = 599.9173, weight = 64.3018\n",
            "epoch96, loss = 591.6015, weight = 64.5868\n",
            "epoch97, loss = 583.5452, weight = 64.8673\n",
            "epoch98, loss = 575.7408, weight = 65.1433\n",
            "epoch99, loss = 568.1802, weight = 65.4150\n",
            "epoch100, loss = 560.8559, weight = 65.6824\n",
            "epoch101, loss = 553.7601, weight = 65.9457\n",
            "epoch102, loss = 546.8859, weight = 66.2048\n",
            "epoch103, loss = 540.2264, weight = 66.4598\n",
            "epoch104, loss = 533.7745, weight = 66.7107\n",
            "epoch105, loss = 527.5240, weight = 66.9578\n",
            "epoch106, loss = 521.4686, weight = 67.2009\n",
            "epoch107, loss = 515.6021, weight = 67.4403\n",
            "epoch108, loss = 509.9186, weight = 67.6758\n",
            "epoch109, loss = 504.4122, weight = 67.9077\n",
            "epoch110, loss = 499.0776, weight = 68.1359\n",
            "epoch111, loss = 493.9093, weight = 68.3605\n",
            "epoch112, loss = 488.9020, weight = 68.5816\n",
            "epoch113, loss = 484.0509, weight = 68.7992\n",
            "epoch114, loss = 479.3508, weight = 69.0134\n",
            "epoch115, loss = 474.7971, weight = 69.2243\n",
            "epoch116, loss = 470.3852, weight = 69.4318\n",
            "epoch117, loss = 466.1108, weight = 69.6360\n",
            "epoch118, loss = 461.9694, weight = 69.8371\n",
            "epoch119, loss = 457.9570, weight = 70.0350\n",
            "epoch120, loss = 454.0695, weight = 70.2297\n",
            "epoch121, loss = 450.3029, weight = 70.4214\n",
            "epoch122, loss = 446.6536, weight = 70.6101\n",
            "epoch123, loss = 443.1179, weight = 70.7959\n",
            "epoch124, loss = 439.6921, weight = 70.9787\n",
            "epoch125, loss = 436.3728, weight = 71.1586\n",
            "epoch126, loss = 433.1568, weight = 71.3358\n",
            "epoch127, loss = 430.0406, weight = 71.5101\n",
            "epoch128, loss = 427.0216, weight = 71.6817\n",
            "epoch129, loss = 424.0963, weight = 71.8506\n",
            "epoch130, loss = 421.2619, weight = 72.0169\n",
            "epoch131, loss = 418.5155, weight = 72.1805\n",
            "epoch132, loss = 415.8545, weight = 72.3416\n",
            "epoch133, loss = 413.2761, weight = 72.5002\n",
            "epoch134, loss = 410.7778, weight = 72.6562\n",
            "epoch135, loss = 408.3572, weight = 72.8098\n",
            "epoch136, loss = 406.0117, weight = 72.9610\n",
            "epoch137, loss = 403.7391, weight = 73.1098\n",
            "epoch138, loss = 401.5371, weight = 73.2563\n",
            "epoch139, loss = 399.4034, weight = 73.4005\n",
            "epoch140, loss = 397.3358, weight = 73.5425\n",
            "epoch141, loss = 395.3325, weight = 73.6822\n",
            "epoch142, loss = 393.3912, weight = 73.8197\n",
            "epoch143, loss = 391.5102, weight = 73.9551\n",
            "epoch144, loss = 389.6876, weight = 74.0883\n",
            "epoch145, loss = 387.9215, weight = 74.2195\n",
            "epoch146, loss = 386.2101, weight = 74.3485\n",
            "epoch147, loss = 384.5519, weight = 74.4756\n",
            "epoch148, loss = 382.9451, weight = 74.6007\n",
            "epoch149, loss = 381.3881, weight = 74.7238\n",
            "epoch150, loss = 379.8793, weight = 74.8450\n",
            "epoch151, loss = 378.4173, weight = 74.9643\n",
            "epoch152, loss = 377.0006, weight = 75.0817\n",
            "epoch153, loss = 375.6278, weight = 75.1973\n",
            "epoch154, loss = 374.2975, weight = 75.3110\n",
            "epoch155, loss = 373.0084, weight = 75.4230\n",
            "epoch156, loss = 371.7593, weight = 75.5332\n",
            "epoch157, loss = 370.5489, weight = 75.6418\n",
            "epoch158, loss = 369.3759, weight = 75.7486\n",
            "epoch159, loss = 368.2393, weight = 75.8537\n",
            "epoch160, loss = 367.1378, weight = 75.9572\n",
            "epoch161, loss = 366.0704, weight = 76.0590\n",
            "epoch162, loss = 365.0361, weight = 76.1593\n",
            "epoch163, loss = 364.0339, weight = 76.2580\n",
            "epoch164, loss = 363.0627, weight = 76.3551\n",
            "epoch165, loss = 362.1215, weight = 76.4508\n",
            "epoch166, loss = 361.2094, weight = 76.5449\n",
            "epoch167, loss = 360.3255, weight = 76.6376\n",
            "epoch168, loss = 359.4690, weight = 76.7288\n",
            "epoch169, loss = 358.6389, weight = 76.8186\n",
            "epoch170, loss = 357.8346, weight = 76.9069\n",
            "epoch171, loss = 357.0551, weight = 76.9939\n",
            "epoch172, loss = 356.2997, weight = 77.0796\n",
            "epoch173, loss = 355.5677, weight = 77.1638\n",
            "epoch174, loss = 354.8583, weight = 77.2468\n",
            "epoch175, loss = 354.1709, weight = 77.3285\n",
            "epoch176, loss = 353.5047, weight = 77.4089\n",
            "epoch177, loss = 352.8590, weight = 77.4880\n",
            "epoch178, loss = 352.2334, weight = 77.5659\n",
            "epoch179, loss = 351.6271, weight = 77.6426\n",
            "epoch180, loss = 351.0394, weight = 77.7181\n",
            "epoch181, loss = 350.4700, weight = 77.7924\n",
            "epoch182, loss = 349.9181, weight = 77.8655\n",
            "epoch183, loss = 349.3833, weight = 77.9375\n",
            "epoch184, loss = 348.8650, weight = 78.0084\n",
            "epoch185, loss = 348.3627, weight = 78.0782\n",
            "epoch186, loss = 347.8760, weight = 78.1469\n",
            "epoch187, loss = 347.4043, weight = 78.2144\n",
            "epoch188, loss = 346.9471, weight = 78.2810\n",
            "epoch189, loss = 346.5041, weight = 78.3465\n",
            "epoch190, loss = 346.0747, weight = 78.4110\n",
            "epoch191, loss = 345.6586, weight = 78.4744\n",
            "epoch192, loss = 345.2553, weight = 78.5369\n",
            "epoch193, loss = 344.8645, weight = 78.5984\n",
            "epoch194, loss = 344.4857, weight = 78.6590\n",
            "epoch195, loss = 344.1187, weight = 78.7185\n",
            "epoch196, loss = 343.7630, weight = 78.7772\n",
            "epoch197, loss = 343.4181, weight = 78.8350\n",
            "epoch198, loss = 343.0840, weight = 78.8918\n",
            "epoch199, loss = 342.7602, weight = 78.9478\n",
            "epoch200, loss = 342.4464, weight = 79.0028\n",
            "epoch201, loss = 342.1422, weight = 79.0571\n",
            "epoch202, loss = 341.8474, weight = 79.1104\n",
            "epoch203, loss = 341.5618, weight = 79.1630\n",
            "epoch204, loss = 341.2849, weight = 79.2147\n",
            "epoch205, loss = 341.0165, weight = 79.2656\n",
            "epoch206, loss = 340.7565, weight = 79.3157\n",
            "epoch207, loss = 340.5045, weight = 79.3651\n",
            "epoch208, loss = 340.2602, weight = 79.4136\n",
            "epoch209, loss = 340.0235, weight = 79.4614\n",
            "epoch210, loss = 339.7941, weight = 79.5085\n",
            "epoch211, loss = 339.5717, weight = 79.5548\n",
            "epoch212, loss = 339.3562, weight = 79.6004\n",
            "epoch213, loss = 339.1473, weight = 79.6453\n",
            "epoch214, loss = 338.9449, weight = 79.6895\n",
            "epoch215, loss = 338.7486, weight = 79.7330\n",
            "epoch216, loss = 338.5585, weight = 79.7758\n",
            "epoch217, loss = 338.3742, weight = 79.8180\n",
            "epoch218, loss = 338.1956, weight = 79.8595\n",
            "epoch219, loss = 338.0225, weight = 79.9003\n",
            "epoch220, loss = 337.8547, weight = 79.9405\n",
            "epoch221, loss = 337.6921, weight = 79.9801\n",
            "epoch222, loss = 337.5345, weight = 80.0191\n",
            "epoch223, loss = 337.3818, weight = 80.0574\n",
            "epoch224, loss = 337.2337, weight = 80.0952\n",
            "epoch225, loss = 337.0902, weight = 80.1324\n",
            "epoch226, loss = 336.9512, weight = 80.1689\n",
            "epoch227, loss = 336.8164, weight = 80.2050\n",
            "epoch228, loss = 336.6857, weight = 80.2404\n",
            "epoch229, loss = 336.5591, weight = 80.2753\n",
            "epoch230, loss = 336.4364, weight = 80.3097\n",
            "epoch231, loss = 336.3175, weight = 80.3435\n",
            "epoch232, loss = 336.2022, weight = 80.3768\n",
            "epoch233, loss = 336.0905, weight = 80.4096\n",
            "epoch234, loss = 335.9822, weight = 80.4418\n",
            "epoch235, loss = 335.8772, weight = 80.4736\n",
            "epoch236, loss = 335.7755, weight = 80.5049\n",
            "epoch237, loss = 335.6769, weight = 80.5357\n",
            "epoch238, loss = 335.5813, weight = 80.5660\n",
            "epoch239, loss = 335.4887, weight = 80.5958\n",
            "epoch240, loss = 335.3990, weight = 80.6252\n",
            "epoch241, loss = 335.3120, weight = 80.6541\n",
            "epoch242, loss = 335.2276, weight = 80.6825\n",
            "epoch243, loss = 335.1459, weight = 80.7105\n",
            "epoch244, loss = 335.0667, weight = 80.7381\n",
            "epoch245, loss = 334.9899, weight = 80.7653\n",
            "epoch246, loss = 334.9155, weight = 80.7920\n",
            "epoch247, loss = 334.8433, weight = 80.8183\n",
            "epoch248, loss = 334.7734, weight = 80.8442\n",
            "epoch249, loss = 334.7056, weight = 80.8697\n",
            "epoch250, loss = 334.6400, weight = 80.8948\n",
            "epoch251, loss = 334.5763, weight = 80.9195\n",
            "epoch252, loss = 334.5146, weight = 80.9438\n",
            "epoch253, loss = 334.4548, weight = 80.9678\n",
            "epoch254, loss = 334.3969, weight = 80.9913\n",
            "epoch255, loss = 334.3407, weight = 81.0145\n",
            "epoch256, loss = 334.2862, weight = 81.0374\n",
            "epoch257, loss = 334.2334, weight = 81.0598\n",
            "epoch258, loss = 334.1823, weight = 81.0820\n",
            "epoch259, loss = 334.1327, weight = 81.1038\n",
            "epoch260, loss = 334.0847, weight = 81.1252\n",
            "epoch261, loss = 334.0381, weight = 81.1463\n",
            "epoch262, loss = 333.9929, weight = 81.1671\n",
            "epoch263, loss = 333.9491, weight = 81.1876\n",
            "epoch264, loss = 333.9067, weight = 81.2077\n",
            "epoch265, loss = 333.8655, weight = 81.2276\n",
            "epoch266, loss = 333.8257, weight = 81.2471\n",
            "epoch267, loss = 333.7871, weight = 81.2663\n",
            "epoch268, loss = 333.7496, weight = 81.2852\n",
            "epoch269, loss = 333.7133, weight = 81.3039\n",
            "epoch270, loss = 333.6782, weight = 81.3222\n",
            "epoch271, loss = 333.6441, weight = 81.3402\n",
            "epoch272, loss = 333.6111, weight = 81.3580\n",
            "epoch273, loss = 333.5790, weight = 81.3755\n",
            "epoch274, loss = 333.5480, weight = 81.3927\n",
            "epoch275, loss = 333.5179, weight = 81.4097\n",
            "epoch276, loss = 333.4887, weight = 81.4264\n",
            "epoch277, loss = 333.4604, weight = 81.4428\n",
            "epoch278, loss = 333.4330, weight = 81.4590\n",
            "epoch279, loss = 333.4065, weight = 81.4749\n",
            "epoch280, loss = 333.3807, weight = 81.4906\n",
            "epoch281, loss = 333.3558, weight = 81.5060\n",
            "epoch282, loss = 333.3316, weight = 81.5212\n",
            "epoch283, loss = 333.3081, weight = 81.5361\n",
            "epoch284, loss = 333.2854, weight = 81.5509\n",
            "epoch285, loss = 333.2634, weight = 81.5654\n",
            "epoch286, loss = 333.2420, weight = 81.5796\n",
            "epoch287, loss = 333.2213, weight = 81.5937\n",
            "epoch288, loss = 333.2013, weight = 81.6075\n",
            "epoch289, loss = 333.1818, weight = 81.6211\n",
            "epoch290, loss = 333.1630, weight = 81.6345\n",
            "epoch291, loss = 333.1447, weight = 81.6477\n",
            "epoch292, loss = 333.1270, weight = 81.6607\n",
            "epoch293, loss = 333.1098, weight = 81.6735\n",
            "epoch294, loss = 333.0932, weight = 81.6861\n",
            "epoch295, loss = 333.0771, weight = 81.6984\n",
            "epoch296, loss = 333.0614, weight = 81.7106\n",
            "epoch297, loss = 333.0463, weight = 81.7227\n",
            "epoch298, loss = 333.0316, weight = 81.7345\n",
            "epoch299, loss = 333.0174, weight = 81.7461\n",
            "epoch300, loss = 333.0036, weight = 81.7576\n",
            "epoch301, loss = 332.9902, weight = 81.7688\n",
            "epoch302, loss = 332.9772, weight = 81.7799\n",
            "epoch303, loss = 332.9647, weight = 81.7909\n",
            "epoch304, loss = 332.9525, weight = 81.8016\n",
            "epoch305, loss = 332.9407, weight = 81.8122\n",
            "epoch306, loss = 332.9292, weight = 81.8227\n",
            "epoch307, loss = 332.9181, weight = 81.8329\n",
            "epoch308, loss = 332.9074, weight = 81.8430\n",
            "epoch309, loss = 332.8970, weight = 81.8530\n",
            "epoch310, loss = 332.8868, weight = 81.8628\n",
            "epoch311, loss = 332.8771, weight = 81.8724\n",
            "epoch312, loss = 332.8676, weight = 81.8819\n",
            "epoch313, loss = 332.8584, weight = 81.8913\n",
            "epoch314, loss = 332.8495, weight = 81.9005\n",
            "epoch315, loss = 332.8408, weight = 81.9095\n",
            "epoch316, loss = 332.8324, weight = 81.9184\n",
            "epoch317, loss = 332.8243, weight = 81.9272\n",
            "epoch318, loss = 332.8164, weight = 81.9359\n",
            "epoch319, loss = 332.8088, weight = 81.9444\n",
            "epoch320, loss = 332.8014, weight = 81.9528\n",
            "epoch321, loss = 332.7942, weight = 81.9610\n",
            "epoch322, loss = 332.7873, weight = 81.9691\n",
            "epoch323, loss = 332.7805, weight = 81.9771\n",
            "epoch324, loss = 332.7740, weight = 81.9850\n",
            "epoch325, loss = 332.7677, weight = 81.9927\n",
            "epoch326, loss = 332.7615, weight = 82.0003\n",
            "epoch327, loss = 332.7556, weight = 82.0078\n",
            "epoch328, loss = 332.7498, weight = 82.0152\n",
            "epoch329, loss = 332.7443, weight = 82.0225\n",
            "epoch330, loss = 332.7389, weight = 82.0297\n",
            "epoch331, loss = 332.7336, weight = 82.0367\n",
            "epoch332, loss = 332.7285, weight = 82.0437\n",
            "epoch333, loss = 332.7236, weight = 82.0505\n",
            "epoch334, loss = 332.7188, weight = 82.0572\n",
            "epoch335, loss = 332.7141, weight = 82.0639\n",
            "epoch336, loss = 332.7096, weight = 82.0704\n",
            "epoch337, loss = 332.7053, weight = 82.0768\n",
            "epoch338, loss = 332.7011, weight = 82.0831\n",
            "epoch339, loss = 332.6970, weight = 82.0893\n",
            "epoch340, loss = 332.6931, weight = 82.0955\n",
            "epoch341, loss = 332.6891, weight = 82.1015\n",
            "epoch342, loss = 332.6854, weight = 82.1074\n",
            "epoch343, loss = 332.6818, weight = 82.1133\n",
            "epoch344, loss = 332.6783, weight = 82.1190\n",
            "epoch345, loss = 332.6749, weight = 82.1247\n",
            "epoch346, loss = 332.6716, weight = 82.1303\n",
            "epoch347, loss = 332.6685, weight = 82.1357\n",
            "epoch348, loss = 332.6654, weight = 82.1412\n",
            "epoch349, loss = 332.6624, weight = 82.1465\n",
            "epoch350, loss = 332.6595, weight = 82.1517\n",
            "epoch351, loss = 332.6567, weight = 82.1569\n",
            "epoch352, loss = 332.6539, weight = 82.1619\n",
            "epoch353, loss = 332.6512, weight = 82.1669\n",
            "epoch354, loss = 332.6487, weight = 82.1719\n",
            "epoch355, loss = 332.6462, weight = 82.1767\n",
            "epoch356, loss = 332.6438, weight = 82.1815\n",
            "epoch357, loss = 332.6415, weight = 82.1862\n",
            "epoch358, loss = 332.6392, weight = 82.1908\n",
            "epoch359, loss = 332.6370, weight = 82.1953\n",
            "epoch360, loss = 332.6349, weight = 82.1998\n",
            "epoch361, loss = 332.6328, weight = 82.2042\n",
            "epoch362, loss = 332.6308, weight = 82.2086\n",
            "epoch363, loss = 332.6289, weight = 82.2128\n",
            "epoch364, loss = 332.6270, weight = 82.2170\n",
            "epoch365, loss = 332.6252, weight = 82.2212\n",
            "epoch366, loss = 332.6234, weight = 82.2253\n",
            "epoch367, loss = 332.6217, weight = 82.2293\n",
            "epoch368, loss = 332.6201, weight = 82.2332\n",
            "epoch369, loss = 332.6184, weight = 82.2371\n",
            "epoch370, loss = 332.6169, weight = 82.2410\n",
            "epoch371, loss = 332.6154, weight = 82.2447\n",
            "epoch372, loss = 332.6139, weight = 82.2484\n",
            "epoch373, loss = 332.6125, weight = 82.2521\n",
            "epoch374, loss = 332.6111, weight = 82.2557\n",
            "epoch375, loss = 332.6098, weight = 82.2592\n",
            "epoch376, loss = 332.6085, weight = 82.2627\n",
            "epoch377, loss = 332.6072, weight = 82.2662\n",
            "epoch378, loss = 332.6060, weight = 82.2695\n",
            "epoch379, loss = 332.6049, weight = 82.2729\n",
            "epoch380, loss = 332.6037, weight = 82.2761\n",
            "epoch381, loss = 332.6025, weight = 82.2794\n",
            "epoch382, loss = 332.6015, weight = 82.2825\n",
            "epoch383, loss = 332.6005, weight = 82.2857\n",
            "epoch384, loss = 332.5995, weight = 82.2888\n",
            "epoch385, loss = 332.5985, weight = 82.2918\n",
            "epoch386, loss = 332.5975, weight = 82.2948\n",
            "epoch387, loss = 332.5966, weight = 82.2977\n",
            "epoch388, loss = 332.5957, weight = 82.3006\n",
            "epoch389, loss = 332.5949, weight = 82.3034\n",
            "epoch390, loss = 332.5940, weight = 82.3062\n",
            "epoch391, loss = 332.5932, weight = 82.3090\n",
            "epoch392, loss = 332.5924, weight = 82.3117\n",
            "epoch393, loss = 332.5917, weight = 82.3144\n",
            "epoch394, loss = 332.5909, weight = 82.3170\n",
            "epoch395, loss = 332.5902, weight = 82.3196\n",
            "epoch396, loss = 332.5895, weight = 82.3222\n",
            "epoch397, loss = 332.5888, weight = 82.3247\n",
            "epoch398, loss = 332.5883, weight = 82.3272\n",
            "epoch399, loss = 332.5876, weight = 82.3296\n",
            "epoch400, loss = 332.5869, weight = 82.3320\n",
            "epoch401, loss = 332.5863, weight = 82.3344\n",
            "epoch402, loss = 332.5858, weight = 82.3367\n",
            "epoch403, loss = 332.5852, weight = 82.3390\n",
            "epoch404, loss = 332.5847, weight = 82.3412\n",
            "epoch405, loss = 332.5842, weight = 82.3434\n",
            "epoch406, loss = 332.5837, weight = 82.3456\n",
            "epoch407, loss = 332.5832, weight = 82.3478\n",
            "epoch408, loss = 332.5826, weight = 82.3499\n",
            "epoch409, loss = 332.5822, weight = 82.3520\n",
            "epoch410, loss = 332.5818, weight = 82.3540\n",
            "epoch411, loss = 332.5813, weight = 82.3560\n",
            "epoch412, loss = 332.5809, weight = 82.3580\n",
            "epoch413, loss = 332.5805, weight = 82.3600\n",
            "epoch414, loss = 332.5801, weight = 82.3619\n",
            "epoch415, loss = 332.5797, weight = 82.3638\n",
            "epoch416, loss = 332.5794, weight = 82.3657\n",
            "epoch417, loss = 332.5790, weight = 82.3675\n",
            "epoch418, loss = 332.5786, weight = 82.3693\n",
            "epoch419, loss = 332.5783, weight = 82.3711\n",
            "epoch420, loss = 332.5780, weight = 82.3729\n",
            "epoch421, loss = 332.5777, weight = 82.3746\n",
            "epoch422, loss = 332.5773, weight = 82.3763\n",
            "epoch423, loss = 332.5770, weight = 82.3780\n",
            "epoch424, loss = 332.5768, weight = 82.3796\n",
            "epoch425, loss = 332.5765, weight = 82.3812\n",
            "epoch426, loss = 332.5762, weight = 82.3828\n",
            "epoch427, loss = 332.5759, weight = 82.3844\n",
            "epoch428, loss = 332.5757, weight = 82.3860\n",
            "epoch429, loss = 332.5754, weight = 82.3875\n",
            "epoch430, loss = 332.5752, weight = 82.3890\n",
            "epoch431, loss = 332.5750, weight = 82.3905\n",
            "epoch432, loss = 332.5747, weight = 82.3919\n",
            "epoch433, loss = 332.5745, weight = 82.3934\n",
            "epoch434, loss = 332.5743, weight = 82.3948\n",
            "epoch435, loss = 332.5741, weight = 82.3962\n",
            "epoch436, loss = 332.5739, weight = 82.3975\n",
            "epoch437, loss = 332.5737, weight = 82.3989\n",
            "epoch438, loss = 332.5735, weight = 82.4002\n",
            "epoch439, loss = 332.5733, weight = 82.4015\n",
            "epoch440, loss = 332.5732, weight = 82.4028\n",
            "epoch441, loss = 332.5730, weight = 82.4041\n",
            "epoch442, loss = 332.5728, weight = 82.4053\n",
            "epoch443, loss = 332.5727, weight = 82.4065\n",
            "epoch444, loss = 332.5724, weight = 82.4077\n",
            "epoch445, loss = 332.5724, weight = 82.4089\n",
            "epoch446, loss = 332.5722, weight = 82.4101\n",
            "epoch447, loss = 332.5721, weight = 82.4112\n",
            "epoch448, loss = 332.5719, weight = 82.4124\n",
            "epoch449, loss = 332.5718, weight = 82.4135\n",
            "epoch450, loss = 332.5717, weight = 82.4146\n",
            "epoch451, loss = 332.5715, weight = 82.4157\n",
            "epoch452, loss = 332.5714, weight = 82.4167\n",
            "epoch453, loss = 332.5713, weight = 82.4178\n",
            "epoch454, loss = 332.5712, weight = 82.4188\n",
            "epoch455, loss = 332.5711, weight = 82.4198\n",
            "epoch456, loss = 332.5710, weight = 82.4208\n",
            "epoch457, loss = 332.5708, weight = 82.4218\n",
            "epoch458, loss = 332.5708, weight = 82.4228\n",
            "epoch459, loss = 332.5707, weight = 82.4237\n",
            "epoch460, loss = 332.5706, weight = 82.4247\n",
            "epoch461, loss = 332.5704, weight = 82.4256\n",
            "epoch462, loss = 332.5704, weight = 82.4265\n",
            "epoch463, loss = 332.5703, weight = 82.4274\n",
            "epoch464, loss = 332.5702, weight = 82.4283\n",
            "epoch465, loss = 332.5702, weight = 82.4292\n",
            "epoch466, loss = 332.5700, weight = 82.4300\n",
            "epoch467, loss = 332.5699, weight = 82.4309\n",
            "epoch468, loss = 332.5699, weight = 82.4317\n",
            "epoch469, loss = 332.5698, weight = 82.4325\n",
            "epoch470, loss = 332.5698, weight = 82.4333\n",
            "epoch471, loss = 332.5697, weight = 82.4341\n",
            "epoch472, loss = 332.5696, weight = 82.4349\n",
            "epoch473, loss = 332.5696, weight = 82.4356\n",
            "epoch474, loss = 332.5695, weight = 82.4364\n",
            "epoch475, loss = 332.5695, weight = 82.4371\n",
            "epoch476, loss = 332.5694, weight = 82.4379\n",
            "epoch477, loss = 332.5693, weight = 82.4386\n",
            "epoch478, loss = 332.5692, weight = 82.4393\n",
            "epoch479, loss = 332.5692, weight = 82.4400\n",
            "epoch480, loss = 332.5692, weight = 82.4407\n",
            "epoch481, loss = 332.5692, weight = 82.4414\n",
            "epoch482, loss = 332.5691, weight = 82.4420\n",
            "epoch483, loss = 332.5690, weight = 82.4427\n",
            "epoch484, loss = 332.5690, weight = 82.4433\n",
            "epoch485, loss = 332.5689, weight = 82.4440\n",
            "epoch486, loss = 332.5689, weight = 82.4446\n",
            "epoch487, loss = 332.5688, weight = 82.4452\n",
            "epoch488, loss = 332.5688, weight = 82.4458\n",
            "epoch489, loss = 332.5688, weight = 82.4464\n",
            "epoch490, loss = 332.5688, weight = 82.4470\n",
            "epoch491, loss = 332.5687, weight = 82.4476\n",
            "epoch492, loss = 332.5687, weight = 82.4482\n",
            "epoch493, loss = 332.5687, weight = 82.4487\n",
            "epoch494, loss = 332.5686, weight = 82.4493\n",
            "epoch495, loss = 332.5686, weight = 82.4498\n",
            "epoch496, loss = 332.5685, weight = 82.4503\n",
            "epoch497, loss = 332.5685, weight = 82.4509\n",
            "epoch498, loss = 332.5685, weight = 82.4514\n",
            "epoch499, loss = 332.5685, weight = 82.4519\n",
            "epoch500, loss = 332.5685, weight = 82.4524\n",
            "epoch501, loss = 332.5684, weight = 82.4529\n",
            "epoch502, loss = 332.5684, weight = 82.4534\n",
            "epoch503, loss = 332.5684, weight = 82.4539\n",
            "epoch504, loss = 332.5683, weight = 82.4543\n",
            "epoch505, loss = 332.5683, weight = 82.4548\n",
            "epoch506, loss = 332.5683, weight = 82.4553\n",
            "epoch507, loss = 332.5682, weight = 82.4557\n",
            "epoch508, loss = 332.5682, weight = 82.4562\n",
            "epoch509, loss = 332.5682, weight = 82.4566\n",
            "epoch510, loss = 332.5682, weight = 82.4570\n",
            "epoch511, loss = 332.5682, weight = 82.4575\n",
            "epoch512, loss = 332.5681, weight = 82.4579\n",
            "epoch513, loss = 332.5681, weight = 82.4583\n",
            "epoch514, loss = 332.5681, weight = 82.4587\n",
            "epoch515, loss = 332.5681, weight = 82.4591\n",
            "epoch516, loss = 332.5681, weight = 82.4595\n",
            "epoch517, loss = 332.5681, weight = 82.4599\n",
            "epoch518, loss = 332.5681, weight = 82.4603\n",
            "epoch519, loss = 332.5681, weight = 82.4606\n",
            "epoch520, loss = 332.5680, weight = 82.4610\n",
            "epoch521, loss = 332.5681, weight = 82.4614\n",
            "epoch522, loss = 332.5680, weight = 82.4617\n",
            "epoch523, loss = 332.5680, weight = 82.4621\n",
            "epoch524, loss = 332.5680, weight = 82.4624\n",
            "epoch525, loss = 332.5680, weight = 82.4628\n",
            "epoch526, loss = 332.5680, weight = 82.4631\n",
            "epoch527, loss = 332.5679, weight = 82.4634\n",
            "epoch528, loss = 332.5679, weight = 82.4637\n",
            "epoch529, loss = 332.5679, weight = 82.4641\n",
            "epoch530, loss = 332.5679, weight = 82.4644\n",
            "epoch531, loss = 332.5679, weight = 82.4647\n",
            "epoch532, loss = 332.5679, weight = 82.4650\n",
            "epoch533, loss = 332.5679, weight = 82.4653\n",
            "epoch534, loss = 332.5678, weight = 82.4656\n",
            "epoch535, loss = 332.5678, weight = 82.4659\n",
            "epoch536, loss = 332.5678, weight = 82.4662\n",
            "epoch537, loss = 332.5678, weight = 82.4665\n",
            "epoch538, loss = 332.5678, weight = 82.4667\n",
            "epoch539, loss = 332.5678, weight = 82.4670\n",
            "epoch540, loss = 332.5678, weight = 82.4673\n",
            "epoch541, loss = 332.5678, weight = 82.4675\n",
            "epoch542, loss = 332.5678, weight = 82.4678\n",
            "epoch543, loss = 332.5678, weight = 82.4681\n",
            "epoch544, loss = 332.5678, weight = 82.4683\n",
            "epoch545, loss = 332.5678, weight = 82.4686\n",
            "epoch546, loss = 332.5678, weight = 82.4688\n",
            "epoch547, loss = 332.5677, weight = 82.4691\n",
            "epoch548, loss = 332.5678, weight = 82.4693\n",
            "epoch549, loss = 332.5678, weight = 82.4695\n",
            "epoch550, loss = 332.5677, weight = 82.4698\n",
            "epoch551, loss = 332.5677, weight = 82.4700\n",
            "epoch552, loss = 332.5677, weight = 82.4702\n",
            "epoch553, loss = 332.5677, weight = 82.4704\n",
            "epoch554, loss = 332.5677, weight = 82.4706\n",
            "epoch555, loss = 332.5677, weight = 82.4709\n",
            "epoch556, loss = 332.5677, weight = 82.4711\n",
            "epoch557, loss = 332.5677, weight = 82.4713\n",
            "epoch558, loss = 332.5677, weight = 82.4715\n",
            "epoch559, loss = 332.5677, weight = 82.4717\n",
            "epoch560, loss = 332.5677, weight = 82.4719\n",
            "epoch561, loss = 332.5677, weight = 82.4721\n",
            "epoch562, loss = 332.5677, weight = 82.4723\n",
            "epoch563, loss = 332.5677, weight = 82.4725\n",
            "epoch564, loss = 332.5677, weight = 82.4726\n",
            "epoch565, loss = 332.5677, weight = 82.4728\n",
            "epoch566, loss = 332.5677, weight = 82.4730\n",
            "epoch567, loss = 332.5677, weight = 82.4732\n",
            "epoch568, loss = 332.5677, weight = 82.4734\n",
            "epoch569, loss = 332.5677, weight = 82.4735\n",
            "epoch570, loss = 332.5677, weight = 82.4737\n",
            "epoch571, loss = 332.5677, weight = 82.4739\n",
            "epoch572, loss = 332.5677, weight = 82.4740\n",
            "epoch573, loss = 332.5677, weight = 82.4742\n",
            "epoch574, loss = 332.5677, weight = 82.4744\n",
            "epoch575, loss = 332.5677, weight = 82.4745\n",
            "epoch576, loss = 332.5677, weight = 82.4747\n",
            "epoch577, loss = 332.5677, weight = 82.4748\n",
            "epoch578, loss = 332.5676, weight = 82.4750\n",
            "epoch579, loss = 332.5677, weight = 82.4751\n",
            "epoch580, loss = 332.5676, weight = 82.4753\n",
            "epoch581, loss = 332.5676, weight = 82.4754\n",
            "epoch582, loss = 332.5676, weight = 82.4755\n",
            "epoch583, loss = 332.5676, weight = 82.4757\n",
            "epoch584, loss = 332.5676, weight = 82.4758\n",
            "epoch585, loss = 332.5677, weight = 82.4760\n",
            "epoch586, loss = 332.5676, weight = 82.4761\n",
            "epoch587, loss = 332.5677, weight = 82.4762\n",
            "epoch588, loss = 332.5676, weight = 82.4763\n",
            "epoch589, loss = 332.5676, weight = 82.4765\n",
            "epoch590, loss = 332.5676, weight = 82.4766\n",
            "epoch591, loss = 332.5676, weight = 82.4767\n",
            "epoch592, loss = 332.5676, weight = 82.4768\n",
            "epoch593, loss = 332.5676, weight = 82.4770\n",
            "epoch594, loss = 332.5676, weight = 82.4771\n",
            "epoch595, loss = 332.5676, weight = 82.4772\n",
            "epoch596, loss = 332.5676, weight = 82.4773\n",
            "epoch597, loss = 332.5676, weight = 82.4774\n",
            "epoch598, loss = 332.5676, weight = 82.4775\n",
            "epoch599, loss = 332.5676, weight = 82.4776\n",
            "epoch600, loss = 332.5676, weight = 82.4777\n",
            "epoch601, loss = 332.5676, weight = 82.4778\n",
            "epoch602, loss = 332.5676, weight = 82.4779\n",
            "epoch603, loss = 332.5676, weight = 82.4780\n",
            "epoch604, loss = 332.5676, weight = 82.4781\n",
            "epoch605, loss = 332.5676, weight = 82.4782\n",
            "epoch606, loss = 332.5676, weight = 82.4783\n",
            "epoch607, loss = 332.5676, weight = 82.4784\n",
            "epoch608, loss = 332.5676, weight = 82.4785\n",
            "epoch609, loss = 332.5676, weight = 82.4786\n",
            "epoch610, loss = 332.5676, weight = 82.4787\n",
            "epoch611, loss = 332.5676, weight = 82.4788\n",
            "epoch612, loss = 332.5676, weight = 82.4789\n",
            "epoch613, loss = 332.5676, weight = 82.4790\n",
            "epoch614, loss = 332.5676, weight = 82.4791\n",
            "epoch615, loss = 332.5676, weight = 82.4791\n",
            "epoch616, loss = 332.5676, weight = 82.4792\n",
            "epoch617, loss = 332.5676, weight = 82.4793\n",
            "epoch618, loss = 332.5676, weight = 82.4794\n",
            "epoch619, loss = 332.5676, weight = 82.4795\n",
            "epoch620, loss = 332.5676, weight = 82.4795\n",
            "epoch621, loss = 332.5676, weight = 82.4796\n",
            "epoch622, loss = 332.5676, weight = 82.4797\n",
            "epoch623, loss = 332.5676, weight = 82.4798\n",
            "epoch624, loss = 332.5676, weight = 82.4799\n",
            "epoch625, loss = 332.5676, weight = 82.4799\n",
            "epoch626, loss = 332.5676, weight = 82.4800\n",
            "epoch627, loss = 332.5676, weight = 82.4801\n",
            "epoch628, loss = 332.5676, weight = 82.4801\n",
            "epoch629, loss = 332.5676, weight = 82.4802\n",
            "epoch630, loss = 332.5676, weight = 82.4803\n",
            "epoch631, loss = 332.5676, weight = 82.4803\n",
            "epoch632, loss = 332.5676, weight = 82.4804\n",
            "epoch633, loss = 332.5676, weight = 82.4805\n",
            "epoch634, loss = 332.5676, weight = 82.4805\n",
            "epoch635, loss = 332.5676, weight = 82.4806\n",
            "epoch636, loss = 332.5676, weight = 82.4806\n",
            "epoch637, loss = 332.5675, weight = 82.4807\n",
            "epoch638, loss = 332.5676, weight = 82.4808\n",
            "epoch639, loss = 332.5676, weight = 82.4808\n",
            "epoch640, loss = 332.5675, weight = 82.4809\n",
            "epoch641, loss = 332.5675, weight = 82.4809\n",
            "epoch642, loss = 332.5676, weight = 82.4810\n",
            "epoch643, loss = 332.5676, weight = 82.4810\n",
            "epoch644, loss = 332.5676, weight = 82.4811\n",
            "epoch645, loss = 332.5676, weight = 82.4811\n",
            "epoch646, loss = 332.5676, weight = 82.4812\n",
            "epoch647, loss = 332.5676, weight = 82.4812\n",
            "epoch648, loss = 332.5676, weight = 82.4813\n",
            "epoch649, loss = 332.5675, weight = 82.4813\n",
            "epoch650, loss = 332.5675, weight = 82.4814\n",
            "epoch651, loss = 332.5676, weight = 82.4814\n",
            "epoch652, loss = 332.5676, weight = 82.4815\n",
            "epoch653, loss = 332.5676, weight = 82.4815\n",
            "epoch654, loss = 332.5676, weight = 82.4816\n",
            "epoch655, loss = 332.5676, weight = 82.4816\n",
            "epoch656, loss = 332.5676, weight = 82.4817\n",
            "epoch657, loss = 332.5676, weight = 82.4817\n",
            "epoch658, loss = 332.5675, weight = 82.4818\n",
            "epoch659, loss = 332.5676, weight = 82.4818\n",
            "epoch660, loss = 332.5675, weight = 82.4818\n",
            "epoch661, loss = 332.5676, weight = 82.4819\n",
            "epoch662, loss = 332.5676, weight = 82.4819\n",
            "epoch663, loss = 332.5676, weight = 82.4820\n",
            "epoch664, loss = 332.5676, weight = 82.4820\n",
            "epoch665, loss = 332.5676, weight = 82.4820\n",
            "epoch666, loss = 332.5676, weight = 82.4821\n",
            "epoch667, loss = 332.5676, weight = 82.4821\n",
            "epoch668, loss = 332.5675, weight = 82.4821\n",
            "epoch669, loss = 332.5675, weight = 82.4822\n",
            "epoch670, loss = 332.5675, weight = 82.4822\n",
            "epoch671, loss = 332.5676, weight = 82.4823\n",
            "epoch672, loss = 332.5676, weight = 82.4823\n",
            "epoch673, loss = 332.5676, weight = 82.4823\n",
            "epoch674, loss = 332.5676, weight = 82.4824\n",
            "epoch675, loss = 332.5675, weight = 82.4824\n",
            "epoch676, loss = 332.5676, weight = 82.4824\n",
            "epoch677, loss = 332.5675, weight = 82.4825\n",
            "epoch678, loss = 332.5676, weight = 82.4825\n",
            "epoch679, loss = 332.5676, weight = 82.4825\n",
            "epoch680, loss = 332.5676, weight = 82.4825\n",
            "epoch681, loss = 332.5676, weight = 82.4826\n",
            "epoch682, loss = 332.5675, weight = 82.4826\n",
            "epoch683, loss = 332.5675, weight = 82.4826\n",
            "epoch684, loss = 332.5676, weight = 82.4827\n",
            "epoch685, loss = 332.5676, weight = 82.4827\n",
            "epoch686, loss = 332.5675, weight = 82.4827\n",
            "epoch687, loss = 332.5676, weight = 82.4828\n",
            "epoch688, loss = 332.5676, weight = 82.4828\n",
            "epoch689, loss = 332.5675, weight = 82.4828\n",
            "epoch690, loss = 332.5676, weight = 82.4828\n",
            "epoch691, loss = 332.5676, weight = 82.4829\n",
            "epoch692, loss = 332.5676, weight = 82.4829\n",
            "epoch693, loss = 332.5676, weight = 82.4829\n",
            "epoch694, loss = 332.5675, weight = 82.4829\n",
            "epoch695, loss = 332.5676, weight = 82.4829\n",
            "epoch696, loss = 332.5675, weight = 82.4830\n",
            "epoch697, loss = 332.5676, weight = 82.4830\n",
            "epoch698, loss = 332.5676, weight = 82.4830\n",
            "epoch699, loss = 332.5675, weight = 82.4830\n",
            "epoch700, loss = 332.5675, weight = 82.4831\n",
            "epoch701, loss = 332.5676, weight = 82.4831\n",
            "epoch702, loss = 332.5676, weight = 82.4831\n",
            "epoch703, loss = 332.5675, weight = 82.4831\n",
            "epoch704, loss = 332.5676, weight = 82.4832\n",
            "epoch705, loss = 332.5676, weight = 82.4832\n",
            "epoch706, loss = 332.5676, weight = 82.4832\n",
            "epoch707, loss = 332.5675, weight = 82.4832\n",
            "epoch708, loss = 332.5675, weight = 82.4832\n",
            "epoch709, loss = 332.5676, weight = 82.4833\n",
            "epoch710, loss = 332.5676, weight = 82.4833\n",
            "epoch711, loss = 332.5675, weight = 82.4833\n",
            "epoch712, loss = 332.5676, weight = 82.4833\n",
            "epoch713, loss = 332.5676, weight = 82.4833\n",
            "epoch714, loss = 332.5676, weight = 82.4833\n",
            "epoch715, loss = 332.5676, weight = 82.4834\n",
            "epoch716, loss = 332.5676, weight = 82.4834\n",
            "epoch717, loss = 332.5676, weight = 82.4834\n",
            "epoch718, loss = 332.5676, weight = 82.4834\n",
            "epoch719, loss = 332.5676, weight = 82.4834\n",
            "epoch720, loss = 332.5676, weight = 82.4834\n",
            "epoch721, loss = 332.5676, weight = 82.4835\n",
            "epoch722, loss = 332.5676, weight = 82.4835\n",
            "epoch723, loss = 332.5676, weight = 82.4835\n",
            "epoch724, loss = 332.5676, weight = 82.4835\n",
            "epoch725, loss = 332.5676, weight = 82.4835\n",
            "epoch726, loss = 332.5676, weight = 82.4835\n",
            "epoch727, loss = 332.5676, weight = 82.4835\n",
            "epoch728, loss = 332.5676, weight = 82.4836\n",
            "epoch729, loss = 332.5676, weight = 82.4836\n",
            "epoch730, loss = 332.5676, weight = 82.4836\n",
            "epoch731, loss = 332.5676, weight = 82.4836\n",
            "epoch732, loss = 332.5676, weight = 82.4836\n",
            "epoch733, loss = 332.5676, weight = 82.4836\n",
            "epoch734, loss = 332.5676, weight = 82.4837\n",
            "epoch735, loss = 332.5675, weight = 82.4837\n",
            "epoch736, loss = 332.5676, weight = 82.4837\n",
            "epoch737, loss = 332.5676, weight = 82.4837\n",
            "epoch738, loss = 332.5676, weight = 82.4837\n",
            "epoch739, loss = 332.5676, weight = 82.4837\n",
            "epoch740, loss = 332.5675, weight = 82.4837\n",
            "epoch741, loss = 332.5676, weight = 82.4838\n",
            "epoch742, loss = 332.5676, weight = 82.4838\n",
            "epoch743, loss = 332.5676, weight = 82.4838\n",
            "epoch744, loss = 332.5675, weight = 82.4838\n",
            "epoch745, loss = 332.5675, weight = 82.4838\n",
            "epoch746, loss = 332.5676, weight = 82.4838\n",
            "epoch747, loss = 332.5676, weight = 82.4838\n",
            "epoch748, loss = 332.5675, weight = 82.4838\n",
            "epoch749, loss = 332.5675, weight = 82.4838\n",
            "epoch750, loss = 332.5676, weight = 82.4838\n",
            "epoch751, loss = 332.5675, weight = 82.4838\n",
            "epoch752, loss = 332.5676, weight = 82.4838\n",
            "epoch753, loss = 332.5676, weight = 82.4838\n",
            "epoch754, loss = 332.5675, weight = 82.4839\n",
            "epoch755, loss = 332.5676, weight = 82.4839\n",
            "epoch756, loss = 332.5675, weight = 82.4839\n",
            "epoch757, loss = 332.5675, weight = 82.4839\n",
            "epoch758, loss = 332.5676, weight = 82.4839\n",
            "epoch759, loss = 332.5675, weight = 82.4839\n",
            "epoch760, loss = 332.5676, weight = 82.4839\n",
            "epoch761, loss = 332.5675, weight = 82.4839\n",
            "epoch762, loss = 332.5675, weight = 82.4839\n",
            "epoch763, loss = 332.5676, weight = 82.4839\n",
            "epoch764, loss = 332.5675, weight = 82.4839\n",
            "epoch765, loss = 332.5676, weight = 82.4839\n",
            "epoch766, loss = 332.5676, weight = 82.4839\n",
            "epoch767, loss = 332.5675, weight = 82.4840\n",
            "epoch768, loss = 332.5676, weight = 82.4840\n",
            "epoch769, loss = 332.5676, weight = 82.4840\n",
            "epoch770, loss = 332.5675, weight = 82.4840\n",
            "epoch771, loss = 332.5675, weight = 82.4840\n",
            "epoch772, loss = 332.5676, weight = 82.4840\n",
            "epoch773, loss = 332.5676, weight = 82.4840\n",
            "epoch774, loss = 332.5676, weight = 82.4840\n",
            "epoch775, loss = 332.5676, weight = 82.4840\n",
            "epoch776, loss = 332.5676, weight = 82.4840\n",
            "epoch777, loss = 332.5676, weight = 82.4840\n",
            "epoch778, loss = 332.5675, weight = 82.4840\n",
            "epoch779, loss = 332.5675, weight = 82.4840\n",
            "epoch780, loss = 332.5675, weight = 82.4841\n",
            "epoch781, loss = 332.5676, weight = 82.4841\n",
            "epoch782, loss = 332.5676, weight = 82.4841\n",
            "epoch783, loss = 332.5676, weight = 82.4841\n",
            "epoch784, loss = 332.5676, weight = 82.4841\n",
            "epoch785, loss = 332.5676, weight = 82.4841\n",
            "epoch786, loss = 332.5676, weight = 82.4841\n",
            "epoch787, loss = 332.5676, weight = 82.4841\n",
            "epoch788, loss = 332.5675, weight = 82.4841\n",
            "epoch789, loss = 332.5675, weight = 82.4841\n",
            "epoch790, loss = 332.5676, weight = 82.4841\n",
            "epoch791, loss = 332.5676, weight = 82.4841\n",
            "epoch792, loss = 332.5675, weight = 82.4841\n",
            "epoch793, loss = 332.5675, weight = 82.4842\n",
            "epoch794, loss = 332.5676, weight = 82.4842\n",
            "epoch795, loss = 332.5676, weight = 82.4842\n",
            "epoch796, loss = 332.5676, weight = 82.4842\n",
            "epoch797, loss = 332.5676, weight = 82.4842\n",
            "epoch798, loss = 332.5675, weight = 82.4842\n",
            "epoch799, loss = 332.5675, weight = 82.4842\n",
            "epoch800, loss = 332.5676, weight = 82.4842\n",
            "epoch801, loss = 332.5676, weight = 82.4842\n",
            "epoch802, loss = 332.5676, weight = 82.4842\n",
            "epoch803, loss = 332.5675, weight = 82.4842\n",
            "epoch804, loss = 332.5675, weight = 82.4842\n",
            "epoch805, loss = 332.5676, weight = 82.4842\n",
            "epoch806, loss = 332.5676, weight = 82.4843\n",
            "epoch807, loss = 332.5676, weight = 82.4843\n",
            "epoch808, loss = 332.5675, weight = 82.4843\n",
            "epoch809, loss = 332.5676, weight = 82.4843\n",
            "epoch810, loss = 332.5676, weight = 82.4843\n",
            "epoch811, loss = 332.5676, weight = 82.4843\n",
            "epoch812, loss = 332.5676, weight = 82.4843\n",
            "epoch813, loss = 332.5676, weight = 82.4843\n",
            "epoch814, loss = 332.5676, weight = 82.4843\n",
            "epoch815, loss = 332.5676, weight = 82.4843\n",
            "epoch816, loss = 332.5675, weight = 82.4843\n",
            "epoch817, loss = 332.5676, weight = 82.4843\n",
            "epoch818, loss = 332.5676, weight = 82.4843\n",
            "epoch819, loss = 332.5676, weight = 82.4843\n",
            "epoch820, loss = 332.5676, weight = 82.4843\n",
            "epoch821, loss = 332.5676, weight = 82.4843\n",
            "epoch822, loss = 332.5676, weight = 82.4843\n",
            "epoch823, loss = 332.5676, weight = 82.4843\n",
            "epoch824, loss = 332.5676, weight = 82.4843\n",
            "epoch825, loss = 332.5676, weight = 82.4843\n",
            "epoch826, loss = 332.5676, weight = 82.4843\n",
            "epoch827, loss = 332.5676, weight = 82.4843\n",
            "epoch828, loss = 332.5676, weight = 82.4843\n",
            "epoch829, loss = 332.5676, weight = 82.4843\n",
            "epoch830, loss = 332.5676, weight = 82.4843\n",
            "epoch831, loss = 332.5676, weight = 82.4843\n",
            "epoch832, loss = 332.5676, weight = 82.4843\n",
            "epoch833, loss = 332.5676, weight = 82.4843\n",
            "epoch834, loss = 332.5676, weight = 82.4843\n",
            "epoch835, loss = 332.5676, weight = 82.4843\n",
            "epoch836, loss = 332.5676, weight = 82.4843\n",
            "epoch837, loss = 332.5676, weight = 82.4843\n",
            "epoch838, loss = 332.5676, weight = 82.4843\n",
            "epoch839, loss = 332.5676, weight = 82.4843\n",
            "epoch840, loss = 332.5676, weight = 82.4843\n",
            "epoch841, loss = 332.5676, weight = 82.4843\n",
            "epoch842, loss = 332.5676, weight = 82.4843\n",
            "epoch843, loss = 332.5676, weight = 82.4843\n",
            "epoch844, loss = 332.5676, weight = 82.4843\n",
            "epoch845, loss = 332.5676, weight = 82.4843\n",
            "epoch846, loss = 332.5676, weight = 82.4843\n",
            "epoch847, loss = 332.5676, weight = 82.4843\n",
            "epoch848, loss = 332.5676, weight = 82.4843\n",
            "epoch849, loss = 332.5676, weight = 82.4843\n",
            "epoch850, loss = 332.5676, weight = 82.4843\n",
            "epoch851, loss = 332.5676, weight = 82.4843\n",
            "epoch852, loss = 332.5676, weight = 82.4843\n",
            "epoch853, loss = 332.5676, weight = 82.4843\n",
            "epoch854, loss = 332.5676, weight = 82.4843\n",
            "epoch855, loss = 332.5676, weight = 82.4843\n",
            "epoch856, loss = 332.5676, weight = 82.4843\n",
            "epoch857, loss = 332.5676, weight = 82.4843\n",
            "epoch858, loss = 332.5676, weight = 82.4843\n",
            "epoch859, loss = 332.5676, weight = 82.4843\n",
            "epoch860, loss = 332.5676, weight = 82.4843\n",
            "epoch861, loss = 332.5676, weight = 82.4843\n",
            "epoch862, loss = 332.5676, weight = 82.4843\n",
            "epoch863, loss = 332.5676, weight = 82.4843\n",
            "epoch864, loss = 332.5676, weight = 82.4843\n",
            "epoch865, loss = 332.5676, weight = 82.4843\n",
            "epoch866, loss = 332.5676, weight = 82.4843\n",
            "epoch867, loss = 332.5676, weight = 82.4843\n",
            "epoch868, loss = 332.5676, weight = 82.4843\n",
            "epoch869, loss = 332.5676, weight = 82.4843\n",
            "epoch870, loss = 332.5676, weight = 82.4843\n",
            "epoch871, loss = 332.5676, weight = 82.4843\n",
            "epoch872, loss = 332.5676, weight = 82.4843\n",
            "epoch873, loss = 332.5676, weight = 82.4843\n",
            "epoch874, loss = 332.5676, weight = 82.4843\n",
            "epoch875, loss = 332.5676, weight = 82.4843\n",
            "epoch876, loss = 332.5676, weight = 82.4843\n",
            "epoch877, loss = 332.5676, weight = 82.4843\n",
            "epoch878, loss = 332.5676, weight = 82.4843\n",
            "epoch879, loss = 332.5676, weight = 82.4843\n",
            "epoch880, loss = 332.5676, weight = 82.4843\n",
            "epoch881, loss = 332.5676, weight = 82.4843\n",
            "epoch882, loss = 332.5676, weight = 82.4843\n",
            "epoch883, loss = 332.5676, weight = 82.4843\n",
            "epoch884, loss = 332.5676, weight = 82.4843\n",
            "epoch885, loss = 332.5676, weight = 82.4843\n",
            "epoch886, loss = 332.5676, weight = 82.4843\n",
            "epoch887, loss = 332.5676, weight = 82.4843\n",
            "epoch888, loss = 332.5676, weight = 82.4843\n",
            "epoch889, loss = 332.5676, weight = 82.4843\n",
            "epoch890, loss = 332.5676, weight = 82.4843\n",
            "epoch891, loss = 332.5676, weight = 82.4843\n",
            "epoch892, loss = 332.5676, weight = 82.4843\n",
            "epoch893, loss = 332.5676, weight = 82.4843\n",
            "epoch894, loss = 332.5676, weight = 82.4843\n",
            "epoch895, loss = 332.5676, weight = 82.4843\n",
            "epoch896, loss = 332.5676, weight = 82.4843\n",
            "epoch897, loss = 332.5676, weight = 82.4843\n",
            "epoch898, loss = 332.5676, weight = 82.4843\n",
            "epoch899, loss = 332.5676, weight = 82.4843\n",
            "epoch900, loss = 332.5676, weight = 82.4843\n",
            "epoch901, loss = 332.5676, weight = 82.4843\n",
            "epoch902, loss = 332.5676, weight = 82.4843\n",
            "epoch903, loss = 332.5676, weight = 82.4843\n",
            "epoch904, loss = 332.5676, weight = 82.4843\n",
            "epoch905, loss = 332.5676, weight = 82.4843\n",
            "epoch906, loss = 332.5676, weight = 82.4843\n",
            "epoch907, loss = 332.5676, weight = 82.4843\n",
            "epoch908, loss = 332.5676, weight = 82.4843\n",
            "epoch909, loss = 332.5676, weight = 82.4843\n",
            "epoch910, loss = 332.5676, weight = 82.4843\n",
            "epoch911, loss = 332.5676, weight = 82.4843\n",
            "epoch912, loss = 332.5676, weight = 82.4843\n",
            "epoch913, loss = 332.5676, weight = 82.4843\n",
            "epoch914, loss = 332.5676, weight = 82.4843\n",
            "epoch915, loss = 332.5676, weight = 82.4843\n",
            "epoch916, loss = 332.5676, weight = 82.4843\n",
            "epoch917, loss = 332.5676, weight = 82.4843\n",
            "epoch918, loss = 332.5676, weight = 82.4843\n",
            "epoch919, loss = 332.5676, weight = 82.4843\n",
            "epoch920, loss = 332.5676, weight = 82.4843\n",
            "epoch921, loss = 332.5676, weight = 82.4843\n",
            "epoch922, loss = 332.5676, weight = 82.4843\n",
            "epoch923, loss = 332.5676, weight = 82.4843\n",
            "epoch924, loss = 332.5676, weight = 82.4843\n",
            "epoch925, loss = 332.5676, weight = 82.4843\n",
            "epoch926, loss = 332.5676, weight = 82.4843\n",
            "epoch927, loss = 332.5676, weight = 82.4843\n",
            "epoch928, loss = 332.5676, weight = 82.4843\n",
            "epoch929, loss = 332.5676, weight = 82.4843\n",
            "epoch930, loss = 332.5676, weight = 82.4843\n",
            "epoch931, loss = 332.5676, weight = 82.4843\n",
            "epoch932, loss = 332.5676, weight = 82.4843\n",
            "epoch933, loss = 332.5676, weight = 82.4843\n",
            "epoch934, loss = 332.5676, weight = 82.4843\n",
            "epoch935, loss = 332.5676, weight = 82.4843\n",
            "epoch936, loss = 332.5676, weight = 82.4843\n",
            "epoch937, loss = 332.5676, weight = 82.4843\n",
            "epoch938, loss = 332.5676, weight = 82.4843\n",
            "epoch939, loss = 332.5676, weight = 82.4843\n",
            "epoch940, loss = 332.5676, weight = 82.4843\n",
            "epoch941, loss = 332.5676, weight = 82.4843\n",
            "epoch942, loss = 332.5676, weight = 82.4843\n",
            "epoch943, loss = 332.5676, weight = 82.4843\n",
            "epoch944, loss = 332.5676, weight = 82.4843\n",
            "epoch945, loss = 332.5676, weight = 82.4843\n",
            "epoch946, loss = 332.5676, weight = 82.4843\n",
            "epoch947, loss = 332.5676, weight = 82.4843\n",
            "epoch948, loss = 332.5676, weight = 82.4843\n",
            "epoch949, loss = 332.5676, weight = 82.4843\n",
            "epoch950, loss = 332.5676, weight = 82.4843\n",
            "epoch951, loss = 332.5676, weight = 82.4843\n",
            "epoch952, loss = 332.5676, weight = 82.4843\n",
            "epoch953, loss = 332.5676, weight = 82.4843\n",
            "epoch954, loss = 332.5676, weight = 82.4843\n",
            "epoch955, loss = 332.5676, weight = 82.4843\n",
            "epoch956, loss = 332.5676, weight = 82.4843\n",
            "epoch957, loss = 332.5676, weight = 82.4843\n",
            "epoch958, loss = 332.5676, weight = 82.4843\n",
            "epoch959, loss = 332.5676, weight = 82.4843\n",
            "epoch960, loss = 332.5676, weight = 82.4843\n",
            "epoch961, loss = 332.5676, weight = 82.4843\n",
            "epoch962, loss = 332.5676, weight = 82.4843\n",
            "epoch963, loss = 332.5676, weight = 82.4843\n",
            "epoch964, loss = 332.5676, weight = 82.4843\n",
            "epoch965, loss = 332.5676, weight = 82.4843\n",
            "epoch966, loss = 332.5676, weight = 82.4843\n",
            "epoch967, loss = 332.5676, weight = 82.4843\n",
            "epoch968, loss = 332.5676, weight = 82.4843\n",
            "epoch969, loss = 332.5676, weight = 82.4843\n",
            "epoch970, loss = 332.5676, weight = 82.4843\n",
            "epoch971, loss = 332.5676, weight = 82.4843\n",
            "epoch972, loss = 332.5676, weight = 82.4843\n",
            "epoch973, loss = 332.5676, weight = 82.4843\n",
            "epoch974, loss = 332.5676, weight = 82.4843\n",
            "epoch975, loss = 332.5676, weight = 82.4843\n",
            "epoch976, loss = 332.5676, weight = 82.4843\n",
            "epoch977, loss = 332.5676, weight = 82.4843\n",
            "epoch978, loss = 332.5676, weight = 82.4843\n",
            "epoch979, loss = 332.5676, weight = 82.4843\n",
            "epoch980, loss = 332.5676, weight = 82.4843\n",
            "epoch981, loss = 332.5676, weight = 82.4843\n",
            "epoch982, loss = 332.5676, weight = 82.4843\n",
            "epoch983, loss = 332.5676, weight = 82.4843\n",
            "epoch984, loss = 332.5676, weight = 82.4843\n",
            "epoch985, loss = 332.5676, weight = 82.4843\n",
            "epoch986, loss = 332.5676, weight = 82.4843\n",
            "epoch987, loss = 332.5676, weight = 82.4843\n",
            "epoch988, loss = 332.5676, weight = 82.4843\n",
            "epoch989, loss = 332.5676, weight = 82.4843\n",
            "epoch990, loss = 332.5676, weight = 82.4843\n",
            "epoch991, loss = 332.5676, weight = 82.4843\n",
            "epoch992, loss = 332.5676, weight = 82.4843\n",
            "epoch993, loss = 332.5676, weight = 82.4843\n",
            "epoch994, loss = 332.5676, weight = 82.4843\n",
            "epoch995, loss = 332.5676, weight = 82.4843\n",
            "epoch996, loss = 332.5676, weight = 82.4843\n",
            "epoch997, loss = 332.5676, weight = 82.4843\n",
            "epoch998, loss = 332.5676, weight = 82.4843\n",
            "epoch999, loss = 332.5676, weight = 82.4843\n",
            "epoch1000, loss = 332.5676, weight = 82.4843\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8-Logistic Regression**"
      ],
      "metadata": {
        "id": "z87igcJ_spG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Design model (input, output size, forward pass)\n",
        "# 2) Construct loss and optimiser\n",
        "# 3) Training loop\n",
        "#   - forward pass: compute prediction and loss\n",
        "#   - backward pass: gradients\n",
        "#   - update weights\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler # class used to scale out features\n",
        "# from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "#0) Prepare the data\n",
        "# it is a binary classification problem where we can predict cancer based on the existing features\n",
        "# this will return a sklearn Bunch class. Bunch is a subclass of Dict\n",
        "bc = datasets.load_breast_cancer()\n",
        "# print(bc)\n",
        "#' print(type(bc))\n",
        "X, y = bc.data, bc.target\n",
        "# X.shape is tellgn me\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples, n_features)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.2, random_state=1234)\n",
        "\n",
        "# scale\n",
        "# Standard scaler will make our features to have 0 mean and unit variance, this is recommended to do when you work with logistic regression\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "# we are not repeating the fit step whcih determines the standard deviation and the mean. they are stored in StandardScaler from the function called before\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# transporm to tensor\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "# reshape our y tensor so that as now I just have one row and we want to make it a column vector, so we want to put each value in one row with just one column\n",
        "y_train = y_train.view(y_train.shape[0], 1)\n",
        "y_test= y_test.view(y_test.shape[0], 1)\n",
        "\n",
        "# 1) model\n",
        "# our model is a comoination of weights an d bias, then in the logistc regression we appl a sigmoid function at the end\n",
        "# f = wx +b, sigmoid at the end\n",
        "# we will create out own model\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, n_input_features):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        # we build our model, the first layer is a linear model which only has one value (one class labed) at the end as output\n",
        "        # we will have at the end 30 input features and 1 output features\n",
        "        self.linear = nn.Linear(n_input_features, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_predicted = torch.sigmoid(self.linear(x))\n",
        "        return y_predicted\n",
        "\n",
        "\n",
        "model = LogisticRegression(n_features)\n",
        "\n",
        "# 2) loss and optiizer\n",
        "criterion = nn.BCELoss()\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "#3)\n",
        "num_epochs = 50000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # forward pass and loss\n",
        "    y_predicted = model((X_train))\n",
        "    loss = criterion(y_predicted, y_train)\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    #update the weights - pytorch will do all the calculation for us\n",
        "    optimizer.step()\n",
        "\n",
        "    # zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (epoch+1)%10 ==0:\n",
        "        print(f'epoch{epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "\n",
        "# let's evaluate th emodel\n",
        "# the evluation shoudl not be part of or computational graph where wewnt ot track th ehosotry\n",
        "with torch.no_grad(): # if we don't use no_grad the model this will part of the computatipnal grapgh, it will track the gradient\n",
        "    y_predicted = model(X_test)\n",
        "    print(type(y_predicted))\n",
        "    # let's convert to class labels 0 or 1\n",
        "    # the sigmoid function will return a value between 0 and 1\n",
        "    # if it is larger than 0.5 is class 1 otherwise is class 0\n",
        "    y_predicted_cls = y_predicted.round()\n",
        "    print(type(y_predicted_cls.eq(y_test)))\n",
        "    print(type(y_predicted_cls.eq(y_test).sum()))\n",
        "    acc = y_predicted_cls.eq(y_test).sum()/float(y_test.shape[0]) # y_test.shape[0] will return the number of samples\n",
        "    print('type acc is: ', type(acc))\n",
        "    # acc is a tensor with just one value\n",
        "    print(f'accuracy = {acc:.2f}') # OR  print(f'accuracy = {acc.item():.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FztYhKfcssqL",
        "outputId": "e4abc21d-83b4-432b-d351-0a2474392d4c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "epoch60, loss = 0.3013\n",
            "epoch70, loss = 0.2806\n",
            "epoch80, loss = 0.2637\n",
            "epoch90, loss = 0.2496\n",
            "epoch100, loss = 0.2376\n",
            "epoch110, loss = 0.2272\n",
            "epoch120, loss = 0.2182\n",
            "epoch130, loss = 0.2102\n",
            "epoch140, loss = 0.2030\n",
            "epoch150, loss = 0.1966\n",
            "epoch160, loss = 0.1908\n",
            "epoch170, loss = 0.1855\n",
            "epoch180, loss = 0.1806\n",
            "epoch190, loss = 0.1762\n",
            "epoch200, loss = 0.1720\n",
            "epoch210, loss = 0.1682\n",
            "epoch220, loss = 0.1647\n",
            "epoch230, loss = 0.1613\n",
            "epoch240, loss = 0.1582\n",
            "epoch250, loss = 0.1553\n",
            "epoch260, loss = 0.1526\n",
            "epoch270, loss = 0.1500\n",
            "epoch280, loss = 0.1475\n",
            "epoch290, loss = 0.1452\n",
            "epoch300, loss = 0.1430\n",
            "epoch310, loss = 0.1409\n",
            "epoch320, loss = 0.1389\n",
            "epoch330, loss = 0.1370\n",
            "epoch340, loss = 0.1352\n",
            "epoch350, loss = 0.1335\n",
            "epoch360, loss = 0.1318\n",
            "epoch370, loss = 0.1302\n",
            "epoch380, loss = 0.1287\n",
            "epoch390, loss = 0.1273\n",
            "epoch400, loss = 0.1259\n",
            "epoch410, loss = 0.1245\n",
            "epoch420, loss = 0.1232\n",
            "epoch430, loss = 0.1220\n",
            "epoch440, loss = 0.1208\n",
            "epoch450, loss = 0.1196\n",
            "epoch460, loss = 0.1185\n",
            "epoch470, loss = 0.1174\n",
            "epoch480, loss = 0.1163\n",
            "epoch490, loss = 0.1153\n",
            "epoch500, loss = 0.1143\n",
            "epoch510, loss = 0.1134\n",
            "epoch520, loss = 0.1125\n",
            "epoch530, loss = 0.1116\n",
            "epoch540, loss = 0.1107\n",
            "epoch550, loss = 0.1099\n",
            "epoch560, loss = 0.1090\n",
            "epoch570, loss = 0.1082\n",
            "epoch580, loss = 0.1075\n",
            "epoch590, loss = 0.1067\n",
            "epoch600, loss = 0.1060\n",
            "epoch610, loss = 0.1052\n",
            "epoch620, loss = 0.1045\n",
            "epoch630, loss = 0.1039\n",
            "epoch640, loss = 0.1032\n",
            "epoch650, loss = 0.1025\n",
            "epoch660, loss = 0.1019\n",
            "epoch670, loss = 0.1013\n",
            "epoch680, loss = 0.1007\n",
            "epoch690, loss = 0.1001\n",
            "epoch700, loss = 0.0995\n",
            "epoch710, loss = 0.0990\n",
            "epoch720, loss = 0.0984\n",
            "epoch730, loss = 0.0979\n",
            "epoch740, loss = 0.0973\n",
            "epoch750, loss = 0.0968\n",
            "epoch760, loss = 0.0963\n",
            "epoch770, loss = 0.0958\n",
            "epoch780, loss = 0.0953\n",
            "epoch790, loss = 0.0949\n",
            "epoch800, loss = 0.0944\n",
            "epoch810, loss = 0.0939\n",
            "epoch820, loss = 0.0935\n",
            "epoch830, loss = 0.0930\n",
            "epoch840, loss = 0.0926\n",
            "epoch850, loss = 0.0922\n",
            "epoch860, loss = 0.0918\n",
            "epoch870, loss = 0.0914\n",
            "epoch880, loss = 0.0910\n",
            "epoch890, loss = 0.0906\n",
            "epoch900, loss = 0.0902\n",
            "epoch910, loss = 0.0898\n",
            "epoch920, loss = 0.0894\n",
            "epoch930, loss = 0.0891\n",
            "epoch940, loss = 0.0887\n",
            "epoch950, loss = 0.0883\n",
            "epoch960, loss = 0.0880\n",
            "epoch970, loss = 0.0876\n",
            "epoch980, loss = 0.0873\n",
            "epoch990, loss = 0.0870\n",
            "epoch1000, loss = 0.0866\n",
            "epoch1010, loss = 0.0863\n",
            "epoch1020, loss = 0.0860\n",
            "epoch1030, loss = 0.0857\n",
            "epoch1040, loss = 0.0854\n",
            "epoch1050, loss = 0.0851\n",
            "epoch1060, loss = 0.0848\n",
            "epoch1070, loss = 0.0845\n",
            "epoch1080, loss = 0.0842\n",
            "epoch1090, loss = 0.0839\n",
            "epoch1100, loss = 0.0836\n",
            "epoch1110, loss = 0.0833\n",
            "epoch1120, loss = 0.0831\n",
            "epoch1130, loss = 0.0828\n",
            "epoch1140, loss = 0.0825\n",
            "epoch1150, loss = 0.0822\n",
            "epoch1160, loss = 0.0820\n",
            "epoch1170, loss = 0.0817\n",
            "epoch1180, loss = 0.0815\n",
            "epoch1190, loss = 0.0812\n",
            "epoch1200, loss = 0.0810\n",
            "epoch1210, loss = 0.0807\n",
            "epoch1220, loss = 0.0805\n",
            "epoch1230, loss = 0.0803\n",
            "epoch1240, loss = 0.0800\n",
            "epoch1250, loss = 0.0798\n",
            "epoch1260, loss = 0.0796\n",
            "epoch1270, loss = 0.0793\n",
            "epoch1280, loss = 0.0791\n",
            "epoch1290, loss = 0.0789\n",
            "epoch1300, loss = 0.0787\n",
            "epoch1310, loss = 0.0784\n",
            "epoch1320, loss = 0.0782\n",
            "epoch1330, loss = 0.0780\n",
            "epoch1340, loss = 0.0778\n",
            "epoch1350, loss = 0.0776\n",
            "epoch1360, loss = 0.0774\n",
            "epoch1370, loss = 0.0772\n",
            "epoch1380, loss = 0.0770\n",
            "epoch1390, loss = 0.0768\n",
            "epoch1400, loss = 0.0766\n",
            "epoch1410, loss = 0.0764\n",
            "epoch1420, loss = 0.0762\n",
            "epoch1430, loss = 0.0760\n",
            "epoch1440, loss = 0.0758\n",
            "epoch1450, loss = 0.0757\n",
            "epoch1460, loss = 0.0755\n",
            "epoch1470, loss = 0.0753\n",
            "epoch1480, loss = 0.0751\n",
            "epoch1490, loss = 0.0749\n",
            "epoch1500, loss = 0.0748\n",
            "epoch1510, loss = 0.0746\n",
            "epoch1520, loss = 0.0744\n",
            "epoch1530, loss = 0.0742\n",
            "epoch1540, loss = 0.0741\n",
            "epoch1550, loss = 0.0739\n",
            "epoch1560, loss = 0.0737\n",
            "epoch1570, loss = 0.0736\n",
            "epoch1580, loss = 0.0734\n",
            "epoch1590, loss = 0.0732\n",
            "epoch1600, loss = 0.0731\n",
            "epoch1610, loss = 0.0729\n",
            "epoch1620, loss = 0.0728\n",
            "epoch1630, loss = 0.0726\n",
            "epoch1640, loss = 0.0725\n",
            "epoch1650, loss = 0.0723\n",
            "epoch1660, loss = 0.0722\n",
            "epoch1670, loss = 0.0720\n",
            "epoch1680, loss = 0.0719\n",
            "epoch1690, loss = 0.0717\n",
            "epoch1700, loss = 0.0716\n",
            "epoch1710, loss = 0.0714\n",
            "epoch1720, loss = 0.0713\n",
            "epoch1730, loss = 0.0711\n",
            "epoch1740, loss = 0.0710\n",
            "epoch1750, loss = 0.0709\n",
            "epoch1760, loss = 0.0707\n",
            "epoch1770, loss = 0.0706\n",
            "epoch1780, loss = 0.0704\n",
            "epoch1790, loss = 0.0703\n",
            "epoch1800, loss = 0.0702\n",
            "epoch1810, loss = 0.0700\n",
            "epoch1820, loss = 0.0699\n",
            "epoch1830, loss = 0.0698\n",
            "epoch1840, loss = 0.0696\n",
            "epoch1850, loss = 0.0695\n",
            "epoch1860, loss = 0.0694\n",
            "epoch1870, loss = 0.0693\n",
            "epoch1880, loss = 0.0691\n",
            "epoch1890, loss = 0.0690\n",
            "epoch1900, loss = 0.0689\n",
            "epoch1910, loss = 0.0688\n",
            "epoch1920, loss = 0.0686\n",
            "epoch1930, loss = 0.0685\n",
            "epoch1940, loss = 0.0684\n",
            "epoch1950, loss = 0.0683\n",
            "epoch1960, loss = 0.0682\n",
            "epoch1970, loss = 0.0680\n",
            "epoch1980, loss = 0.0679\n",
            "epoch1990, loss = 0.0678\n",
            "epoch2000, loss = 0.0677\n",
            "epoch2010, loss = 0.0676\n",
            "epoch2020, loss = 0.0675\n",
            "epoch2030, loss = 0.0674\n",
            "epoch2040, loss = 0.0672\n",
            "epoch2050, loss = 0.0671\n",
            "epoch2060, loss = 0.0670\n",
            "epoch2070, loss = 0.0669\n",
            "epoch2080, loss = 0.0668\n",
            "epoch2090, loss = 0.0667\n",
            "epoch2100, loss = 0.0666\n",
            "epoch2110, loss = 0.0665\n",
            "epoch2120, loss = 0.0664\n",
            "epoch2130, loss = 0.0663\n",
            "epoch2140, loss = 0.0662\n",
            "epoch2150, loss = 0.0661\n",
            "epoch2160, loss = 0.0660\n",
            "epoch2170, loss = 0.0659\n",
            "epoch2180, loss = 0.0658\n",
            "epoch2190, loss = 0.0657\n",
            "epoch2200, loss = 0.0656\n",
            "epoch2210, loss = 0.0655\n",
            "epoch2220, loss = 0.0654\n",
            "epoch2230, loss = 0.0653\n",
            "epoch2240, loss = 0.0652\n",
            "epoch2250, loss = 0.0651\n",
            "epoch2260, loss = 0.0650\n",
            "epoch2270, loss = 0.0649\n",
            "epoch2280, loss = 0.0648\n",
            "epoch2290, loss = 0.0647\n",
            "epoch2300, loss = 0.0646\n",
            "epoch2310, loss = 0.0645\n",
            "epoch2320, loss = 0.0644\n",
            "epoch2330, loss = 0.0643\n",
            "epoch2340, loss = 0.0642\n",
            "epoch2350, loss = 0.0641\n",
            "epoch2360, loss = 0.0640\n",
            "epoch2370, loss = 0.0640\n",
            "epoch2380, loss = 0.0639\n",
            "epoch2390, loss = 0.0638\n",
            "epoch2400, loss = 0.0637\n",
            "epoch2410, loss = 0.0636\n",
            "epoch2420, loss = 0.0635\n",
            "epoch2430, loss = 0.0634\n",
            "epoch2440, loss = 0.0633\n",
            "epoch2450, loss = 0.0633\n",
            "epoch2460, loss = 0.0632\n",
            "epoch2470, loss = 0.0631\n",
            "epoch2480, loss = 0.0630\n",
            "epoch2490, loss = 0.0629\n",
            "epoch2500, loss = 0.0628\n",
            "epoch2510, loss = 0.0628\n",
            "epoch2520, loss = 0.0627\n",
            "epoch2530, loss = 0.0626\n",
            "epoch2540, loss = 0.0625\n",
            "epoch2550, loss = 0.0624\n",
            "epoch2560, loss = 0.0623\n",
            "epoch2570, loss = 0.0623\n",
            "epoch2580, loss = 0.0622\n",
            "epoch2590, loss = 0.0621\n",
            "epoch2600, loss = 0.0620\n",
            "epoch2610, loss = 0.0620\n",
            "epoch2620, loss = 0.0619\n",
            "epoch2630, loss = 0.0618\n",
            "epoch2640, loss = 0.0617\n",
            "epoch2650, loss = 0.0616\n",
            "epoch2660, loss = 0.0616\n",
            "epoch2670, loss = 0.0615\n",
            "epoch2680, loss = 0.0614\n",
            "epoch2690, loss = 0.0613\n",
            "epoch2700, loss = 0.0613\n",
            "epoch2710, loss = 0.0612\n",
            "epoch2720, loss = 0.0611\n",
            "epoch2730, loss = 0.0610\n",
            "epoch2740, loss = 0.0610\n",
            "epoch2750, loss = 0.0609\n",
            "epoch2760, loss = 0.0608\n",
            "epoch2770, loss = 0.0608\n",
            "epoch2780, loss = 0.0607\n",
            "epoch2790, loss = 0.0606\n",
            "epoch2800, loss = 0.0605\n",
            "epoch2810, loss = 0.0605\n",
            "epoch2820, loss = 0.0604\n",
            "epoch2830, loss = 0.0603\n",
            "epoch2840, loss = 0.0603\n",
            "epoch2850, loss = 0.0602\n",
            "epoch2860, loss = 0.0601\n",
            "epoch2870, loss = 0.0601\n",
            "epoch2880, loss = 0.0600\n",
            "epoch2890, loss = 0.0599\n",
            "epoch2900, loss = 0.0599\n",
            "epoch2910, loss = 0.0598\n",
            "epoch2920, loss = 0.0597\n",
            "epoch2930, loss = 0.0597\n",
            "epoch2940, loss = 0.0596\n",
            "epoch2950, loss = 0.0595\n",
            "epoch2960, loss = 0.0595\n",
            "epoch2970, loss = 0.0594\n",
            "epoch2980, loss = 0.0593\n",
            "epoch2990, loss = 0.0593\n",
            "epoch3000, loss = 0.0592\n",
            "epoch3010, loss = 0.0591\n",
            "epoch3020, loss = 0.0591\n",
            "epoch3030, loss = 0.0590\n",
            "epoch3040, loss = 0.0589\n",
            "epoch3050, loss = 0.0589\n",
            "epoch3060, loss = 0.0588\n",
            "epoch3070, loss = 0.0588\n",
            "epoch3080, loss = 0.0587\n",
            "epoch3090, loss = 0.0586\n",
            "epoch3100, loss = 0.0586\n",
            "epoch3110, loss = 0.0585\n",
            "epoch3120, loss = 0.0585\n",
            "epoch3130, loss = 0.0584\n",
            "epoch3140, loss = 0.0583\n",
            "epoch3150, loss = 0.0583\n",
            "epoch3160, loss = 0.0582\n",
            "epoch3170, loss = 0.0582\n",
            "epoch3180, loss = 0.0581\n",
            "epoch3190, loss = 0.0580\n",
            "epoch3200, loss = 0.0580\n",
            "epoch3210, loss = 0.0579\n",
            "epoch3220, loss = 0.0579\n",
            "epoch3230, loss = 0.0578\n",
            "epoch3240, loss = 0.0577\n",
            "epoch3250, loss = 0.0577\n",
            "epoch3260, loss = 0.0576\n",
            "epoch3270, loss = 0.0576\n",
            "epoch3280, loss = 0.0575\n",
            "epoch3290, loss = 0.0575\n",
            "epoch3300, loss = 0.0574\n",
            "epoch3310, loss = 0.0574\n",
            "epoch3320, loss = 0.0573\n",
            "epoch3330, loss = 0.0572\n",
            "epoch3340, loss = 0.0572\n",
            "epoch3350, loss = 0.0571\n",
            "epoch3360, loss = 0.0571\n",
            "epoch3370, loss = 0.0570\n",
            "epoch3380, loss = 0.0570\n",
            "epoch3390, loss = 0.0569\n",
            "epoch3400, loss = 0.0569\n",
            "epoch3410, loss = 0.0568\n",
            "epoch3420, loss = 0.0568\n",
            "epoch3430, loss = 0.0567\n",
            "epoch3440, loss = 0.0566\n",
            "epoch3450, loss = 0.0566\n",
            "epoch3460, loss = 0.0565\n",
            "epoch3470, loss = 0.0565\n",
            "epoch3480, loss = 0.0564\n",
            "epoch3490, loss = 0.0564\n",
            "epoch3500, loss = 0.0563\n",
            "epoch3510, loss = 0.0563\n",
            "epoch3520, loss = 0.0562\n",
            "epoch3530, loss = 0.0562\n",
            "epoch3540, loss = 0.0561\n",
            "epoch3550, loss = 0.0561\n",
            "epoch3560, loss = 0.0560\n",
            "epoch3570, loss = 0.0560\n",
            "epoch3580, loss = 0.0559\n",
            "epoch3590, loss = 0.0559\n",
            "epoch3600, loss = 0.0558\n",
            "epoch3610, loss = 0.0558\n",
            "epoch3620, loss = 0.0557\n",
            "epoch3630, loss = 0.0557\n",
            "epoch3640, loss = 0.0556\n",
            "epoch3650, loss = 0.0556\n",
            "epoch3660, loss = 0.0555\n",
            "epoch3670, loss = 0.0555\n",
            "epoch3680, loss = 0.0554\n",
            "epoch3690, loss = 0.0554\n",
            "epoch3700, loss = 0.0553\n",
            "epoch3710, loss = 0.0553\n",
            "epoch3720, loss = 0.0552\n",
            "epoch3730, loss = 0.0552\n",
            "epoch3740, loss = 0.0552\n",
            "epoch3750, loss = 0.0551\n",
            "epoch3760, loss = 0.0551\n",
            "epoch3770, loss = 0.0550\n",
            "epoch3780, loss = 0.0550\n",
            "epoch3790, loss = 0.0549\n",
            "epoch3800, loss = 0.0549\n",
            "epoch3810, loss = 0.0548\n",
            "epoch3820, loss = 0.0548\n",
            "epoch3830, loss = 0.0547\n",
            "epoch3840, loss = 0.0547\n",
            "epoch3850, loss = 0.0546\n",
            "epoch3860, loss = 0.0546\n",
            "epoch3870, loss = 0.0546\n",
            "epoch3880, loss = 0.0545\n",
            "epoch3890, loss = 0.0545\n",
            "epoch3900, loss = 0.0544\n",
            "epoch3910, loss = 0.0544\n",
            "epoch3920, loss = 0.0543\n",
            "epoch3930, loss = 0.0543\n",
            "epoch3940, loss = 0.0542\n",
            "epoch3950, loss = 0.0542\n",
            "epoch3960, loss = 0.0542\n",
            "epoch3970, loss = 0.0541\n",
            "epoch3980, loss = 0.0541\n",
            "epoch3990, loss = 0.0540\n",
            "epoch4000, loss = 0.0540\n",
            "epoch4010, loss = 0.0539\n",
            "epoch4020, loss = 0.0539\n",
            "epoch4030, loss = 0.0539\n",
            "epoch4040, loss = 0.0538\n",
            "epoch4050, loss = 0.0538\n",
            "epoch4060, loss = 0.0537\n",
            "epoch4070, loss = 0.0537\n",
            "epoch4080, loss = 0.0536\n",
            "epoch4090, loss = 0.0536\n",
            "epoch4100, loss = 0.0536\n",
            "epoch4110, loss = 0.0535\n",
            "epoch4120, loss = 0.0535\n",
            "epoch4130, loss = 0.0534\n",
            "epoch4140, loss = 0.0534\n",
            "epoch4150, loss = 0.0534\n",
            "epoch4160, loss = 0.0533\n",
            "epoch4170, loss = 0.0533\n",
            "epoch4180, loss = 0.0532\n",
            "epoch4190, loss = 0.0532\n",
            "epoch4200, loss = 0.0532\n",
            "epoch4210, loss = 0.0531\n",
            "epoch4220, loss = 0.0531\n",
            "epoch4230, loss = 0.0530\n",
            "epoch4240, loss = 0.0530\n",
            "epoch4250, loss = 0.0530\n",
            "epoch4260, loss = 0.0529\n",
            "epoch4270, loss = 0.0529\n",
            "epoch4280, loss = 0.0528\n",
            "epoch4290, loss = 0.0528\n",
            "epoch4300, loss = 0.0528\n",
            "epoch4310, loss = 0.0527\n",
            "epoch4320, loss = 0.0527\n",
            "epoch4330, loss = 0.0526\n",
            "epoch4340, loss = 0.0526\n",
            "epoch4350, loss = 0.0526\n",
            "epoch4360, loss = 0.0525\n",
            "epoch4370, loss = 0.0525\n",
            "epoch4380, loss = 0.0525\n",
            "epoch4390, loss = 0.0524\n",
            "epoch4400, loss = 0.0524\n",
            "epoch4410, loss = 0.0523\n",
            "epoch4420, loss = 0.0523\n",
            "epoch4430, loss = 0.0523\n",
            "epoch4440, loss = 0.0522\n",
            "epoch4450, loss = 0.0522\n",
            "epoch4460, loss = 0.0522\n",
            "epoch4470, loss = 0.0521\n",
            "epoch4480, loss = 0.0521\n",
            "epoch4490, loss = 0.0520\n",
            "epoch4500, loss = 0.0520\n",
            "epoch4510, loss = 0.0520\n",
            "epoch4520, loss = 0.0519\n",
            "epoch4530, loss = 0.0519\n",
            "epoch4540, loss = 0.0519\n",
            "epoch4550, loss = 0.0518\n",
            "epoch4560, loss = 0.0518\n",
            "epoch4570, loss = 0.0518\n",
            "epoch4580, loss = 0.0517\n",
            "epoch4590, loss = 0.0517\n",
            "epoch4600, loss = 0.0516\n",
            "epoch4610, loss = 0.0516\n",
            "epoch4620, loss = 0.0516\n",
            "epoch4630, loss = 0.0515\n",
            "epoch4640, loss = 0.0515\n",
            "epoch4650, loss = 0.0515\n",
            "epoch4660, loss = 0.0514\n",
            "epoch4670, loss = 0.0514\n",
            "epoch4680, loss = 0.0514\n",
            "epoch4690, loss = 0.0513\n",
            "epoch4700, loss = 0.0513\n",
            "epoch4710, loss = 0.0513\n",
            "epoch4720, loss = 0.0512\n",
            "epoch4730, loss = 0.0512\n",
            "epoch4740, loss = 0.0512\n",
            "epoch4750, loss = 0.0511\n",
            "epoch4760, loss = 0.0511\n",
            "epoch4770, loss = 0.0511\n",
            "epoch4780, loss = 0.0510\n",
            "epoch4790, loss = 0.0510\n",
            "epoch4800, loss = 0.0510\n",
            "epoch4810, loss = 0.0509\n",
            "epoch4820, loss = 0.0509\n",
            "epoch4830, loss = 0.0509\n",
            "epoch4840, loss = 0.0508\n",
            "epoch4850, loss = 0.0508\n",
            "epoch4860, loss = 0.0508\n",
            "epoch4870, loss = 0.0507\n",
            "epoch4880, loss = 0.0507\n",
            "epoch4890, loss = 0.0507\n",
            "epoch4900, loss = 0.0506\n",
            "epoch4910, loss = 0.0506\n",
            "epoch4920, loss = 0.0506\n",
            "epoch4930, loss = 0.0505\n",
            "epoch4940, loss = 0.0505\n",
            "epoch4950, loss = 0.0505\n",
            "epoch4960, loss = 0.0504\n",
            "epoch4970, loss = 0.0504\n",
            "epoch4980, loss = 0.0504\n",
            "epoch4990, loss = 0.0503\n",
            "epoch5000, loss = 0.0503\n",
            "epoch5010, loss = 0.0503\n",
            "epoch5020, loss = 0.0502\n",
            "epoch5030, loss = 0.0502\n",
            "epoch5040, loss = 0.0502\n",
            "epoch5050, loss = 0.0502\n",
            "epoch5060, loss = 0.0501\n",
            "epoch5070, loss = 0.0501\n",
            "epoch5080, loss = 0.0501\n",
            "epoch5090, loss = 0.0500\n",
            "epoch5100, loss = 0.0500\n",
            "epoch5110, loss = 0.0500\n",
            "epoch5120, loss = 0.0499\n",
            "epoch5130, loss = 0.0499\n",
            "epoch5140, loss = 0.0499\n",
            "epoch5150, loss = 0.0498\n",
            "epoch5160, loss = 0.0498\n",
            "epoch5170, loss = 0.0498\n",
            "epoch5180, loss = 0.0498\n",
            "epoch5190, loss = 0.0497\n",
            "epoch5200, loss = 0.0497\n",
            "epoch5210, loss = 0.0497\n",
            "epoch5220, loss = 0.0496\n",
            "epoch5230, loss = 0.0496\n",
            "epoch5240, loss = 0.0496\n",
            "epoch5250, loss = 0.0495\n",
            "epoch5260, loss = 0.0495\n",
            "epoch5270, loss = 0.0495\n",
            "epoch5280, loss = 0.0495\n",
            "epoch5290, loss = 0.0494\n",
            "epoch5300, loss = 0.0494\n",
            "epoch5310, loss = 0.0494\n",
            "epoch5320, loss = 0.0493\n",
            "epoch5330, loss = 0.0493\n",
            "epoch5340, loss = 0.0493\n",
            "epoch5350, loss = 0.0493\n",
            "epoch5360, loss = 0.0492\n",
            "epoch5370, loss = 0.0492\n",
            "epoch5380, loss = 0.0492\n",
            "epoch5390, loss = 0.0491\n",
            "epoch5400, loss = 0.0491\n",
            "epoch5410, loss = 0.0491\n",
            "epoch5420, loss = 0.0491\n",
            "epoch5430, loss = 0.0490\n",
            "epoch5440, loss = 0.0490\n",
            "epoch5450, loss = 0.0490\n",
            "epoch5460, loss = 0.0489\n",
            "epoch5470, loss = 0.0489\n",
            "epoch5480, loss = 0.0489\n",
            "epoch5490, loss = 0.0489\n",
            "epoch5500, loss = 0.0488\n",
            "epoch5510, loss = 0.0488\n",
            "epoch5520, loss = 0.0488\n",
            "epoch5530, loss = 0.0488\n",
            "epoch5540, loss = 0.0487\n",
            "epoch5550, loss = 0.0487\n",
            "epoch5560, loss = 0.0487\n",
            "epoch5570, loss = 0.0486\n",
            "epoch5580, loss = 0.0486\n",
            "epoch5590, loss = 0.0486\n",
            "epoch5600, loss = 0.0486\n",
            "epoch5610, loss = 0.0485\n",
            "epoch5620, loss = 0.0485\n",
            "epoch5630, loss = 0.0485\n",
            "epoch5640, loss = 0.0485\n",
            "epoch5650, loss = 0.0484\n",
            "epoch5660, loss = 0.0484\n",
            "epoch5670, loss = 0.0484\n",
            "epoch5680, loss = 0.0483\n",
            "epoch5690, loss = 0.0483\n",
            "epoch5700, loss = 0.0483\n",
            "epoch5710, loss = 0.0483\n",
            "epoch5720, loss = 0.0482\n",
            "epoch5730, loss = 0.0482\n",
            "epoch5740, loss = 0.0482\n",
            "epoch5750, loss = 0.0482\n",
            "epoch5760, loss = 0.0481\n",
            "epoch5770, loss = 0.0481\n",
            "epoch5780, loss = 0.0481\n",
            "epoch5790, loss = 0.0481\n",
            "epoch5800, loss = 0.0480\n",
            "epoch5810, loss = 0.0480\n",
            "epoch5820, loss = 0.0480\n",
            "epoch5830, loss = 0.0480\n",
            "epoch5840, loss = 0.0479\n",
            "epoch5850, loss = 0.0479\n",
            "epoch5860, loss = 0.0479\n",
            "epoch5870, loss = 0.0479\n",
            "epoch5880, loss = 0.0478\n",
            "epoch5890, loss = 0.0478\n",
            "epoch5900, loss = 0.0478\n",
            "epoch5910, loss = 0.0478\n",
            "epoch5920, loss = 0.0477\n",
            "epoch5930, loss = 0.0477\n",
            "epoch5940, loss = 0.0477\n",
            "epoch5950, loss = 0.0477\n",
            "epoch5960, loss = 0.0476\n",
            "epoch5970, loss = 0.0476\n",
            "epoch5980, loss = 0.0476\n",
            "epoch5990, loss = 0.0476\n",
            "epoch6000, loss = 0.0475\n",
            "epoch6010, loss = 0.0475\n",
            "epoch6020, loss = 0.0475\n",
            "epoch6030, loss = 0.0475\n",
            "epoch6040, loss = 0.0474\n",
            "epoch6050, loss = 0.0474\n",
            "epoch6060, loss = 0.0474\n",
            "epoch6070, loss = 0.0474\n",
            "epoch6080, loss = 0.0473\n",
            "epoch6090, loss = 0.0473\n",
            "epoch6100, loss = 0.0473\n",
            "epoch6110, loss = 0.0473\n",
            "epoch6120, loss = 0.0472\n",
            "epoch6130, loss = 0.0472\n",
            "epoch6140, loss = 0.0472\n",
            "epoch6150, loss = 0.0472\n",
            "epoch6160, loss = 0.0471\n",
            "epoch6170, loss = 0.0471\n",
            "epoch6180, loss = 0.0471\n",
            "epoch6190, loss = 0.0471\n",
            "epoch6200, loss = 0.0470\n",
            "epoch6210, loss = 0.0470\n",
            "epoch6220, loss = 0.0470\n",
            "epoch6230, loss = 0.0470\n",
            "epoch6240, loss = 0.0470\n",
            "epoch6250, loss = 0.0469\n",
            "epoch6260, loss = 0.0469\n",
            "epoch6270, loss = 0.0469\n",
            "epoch6280, loss = 0.0469\n",
            "epoch6290, loss = 0.0468\n",
            "epoch6300, loss = 0.0468\n",
            "epoch6310, loss = 0.0468\n",
            "epoch6320, loss = 0.0468\n",
            "epoch6330, loss = 0.0467\n",
            "epoch6340, loss = 0.0467\n",
            "epoch6350, loss = 0.0467\n",
            "epoch6360, loss = 0.0467\n",
            "epoch6370, loss = 0.0467\n",
            "epoch6380, loss = 0.0466\n",
            "epoch6390, loss = 0.0466\n",
            "epoch6400, loss = 0.0466\n",
            "epoch6410, loss = 0.0466\n",
            "epoch6420, loss = 0.0465\n",
            "epoch6430, loss = 0.0465\n",
            "epoch6440, loss = 0.0465\n",
            "epoch6450, loss = 0.0465\n",
            "epoch6460, loss = 0.0465\n",
            "epoch6470, loss = 0.0464\n",
            "epoch6480, loss = 0.0464\n",
            "epoch6490, loss = 0.0464\n",
            "epoch6500, loss = 0.0464\n",
            "epoch6510, loss = 0.0463\n",
            "epoch6520, loss = 0.0463\n",
            "epoch6530, loss = 0.0463\n",
            "epoch6540, loss = 0.0463\n",
            "epoch6550, loss = 0.0463\n",
            "epoch6560, loss = 0.0462\n",
            "epoch6570, loss = 0.0462\n",
            "epoch6580, loss = 0.0462\n",
            "epoch6590, loss = 0.0462\n",
            "epoch6600, loss = 0.0461\n",
            "epoch6610, loss = 0.0461\n",
            "epoch6620, loss = 0.0461\n",
            "epoch6630, loss = 0.0461\n",
            "epoch6640, loss = 0.0461\n",
            "epoch6650, loss = 0.0460\n",
            "epoch6660, loss = 0.0460\n",
            "epoch6670, loss = 0.0460\n",
            "epoch6680, loss = 0.0460\n",
            "epoch6690, loss = 0.0460\n",
            "epoch6700, loss = 0.0459\n",
            "epoch6710, loss = 0.0459\n",
            "epoch6720, loss = 0.0459\n",
            "epoch6730, loss = 0.0459\n",
            "epoch6740, loss = 0.0458\n",
            "epoch6750, loss = 0.0458\n",
            "epoch6760, loss = 0.0458\n",
            "epoch6770, loss = 0.0458\n",
            "epoch6780, loss = 0.0458\n",
            "epoch6790, loss = 0.0457\n",
            "epoch6800, loss = 0.0457\n",
            "epoch6810, loss = 0.0457\n",
            "epoch6820, loss = 0.0457\n",
            "epoch6830, loss = 0.0457\n",
            "epoch6840, loss = 0.0456\n",
            "epoch6850, loss = 0.0456\n",
            "epoch6860, loss = 0.0456\n",
            "epoch6870, loss = 0.0456\n",
            "epoch6880, loss = 0.0456\n",
            "epoch6890, loss = 0.0455\n",
            "epoch6900, loss = 0.0455\n",
            "epoch6910, loss = 0.0455\n",
            "epoch6920, loss = 0.0455\n",
            "epoch6930, loss = 0.0455\n",
            "epoch6940, loss = 0.0454\n",
            "epoch6950, loss = 0.0454\n",
            "epoch6960, loss = 0.0454\n",
            "epoch6970, loss = 0.0454\n",
            "epoch6980, loss = 0.0454\n",
            "epoch6990, loss = 0.0453\n",
            "epoch7000, loss = 0.0453\n",
            "epoch7010, loss = 0.0453\n",
            "epoch7020, loss = 0.0453\n",
            "epoch7030, loss = 0.0453\n",
            "epoch7040, loss = 0.0452\n",
            "epoch7050, loss = 0.0452\n",
            "epoch7060, loss = 0.0452\n",
            "epoch7070, loss = 0.0452\n",
            "epoch7080, loss = 0.0452\n",
            "epoch7090, loss = 0.0451\n",
            "epoch7100, loss = 0.0451\n",
            "epoch7110, loss = 0.0451\n",
            "epoch7120, loss = 0.0451\n",
            "epoch7130, loss = 0.0451\n",
            "epoch7140, loss = 0.0450\n",
            "epoch7150, loss = 0.0450\n",
            "epoch7160, loss = 0.0450\n",
            "epoch7170, loss = 0.0450\n",
            "epoch7180, loss = 0.0450\n",
            "epoch7190, loss = 0.0449\n",
            "epoch7200, loss = 0.0449\n",
            "epoch7210, loss = 0.0449\n",
            "epoch7220, loss = 0.0449\n",
            "epoch7230, loss = 0.0449\n",
            "epoch7240, loss = 0.0448\n",
            "epoch7250, loss = 0.0448\n",
            "epoch7260, loss = 0.0448\n",
            "epoch7270, loss = 0.0448\n",
            "epoch7280, loss = 0.0448\n",
            "epoch7290, loss = 0.0448\n",
            "epoch7300, loss = 0.0447\n",
            "epoch7310, loss = 0.0447\n",
            "epoch7320, loss = 0.0447\n",
            "epoch7330, loss = 0.0447\n",
            "epoch7340, loss = 0.0447\n",
            "epoch7350, loss = 0.0446\n",
            "epoch7360, loss = 0.0446\n",
            "epoch7370, loss = 0.0446\n",
            "epoch7380, loss = 0.0446\n",
            "epoch7390, loss = 0.0446\n",
            "epoch7400, loss = 0.0445\n",
            "epoch7410, loss = 0.0445\n",
            "epoch7420, loss = 0.0445\n",
            "epoch7430, loss = 0.0445\n",
            "epoch7440, loss = 0.0445\n",
            "epoch7450, loss = 0.0445\n",
            "epoch7460, loss = 0.0444\n",
            "epoch7470, loss = 0.0444\n",
            "epoch7480, loss = 0.0444\n",
            "epoch7490, loss = 0.0444\n",
            "epoch7500, loss = 0.0444\n",
            "epoch7510, loss = 0.0443\n",
            "epoch7520, loss = 0.0443\n",
            "epoch7530, loss = 0.0443\n",
            "epoch7540, loss = 0.0443\n",
            "epoch7550, loss = 0.0443\n",
            "epoch7560, loss = 0.0443\n",
            "epoch7570, loss = 0.0442\n",
            "epoch7580, loss = 0.0442\n",
            "epoch7590, loss = 0.0442\n",
            "epoch7600, loss = 0.0442\n",
            "epoch7610, loss = 0.0442\n",
            "epoch7620, loss = 0.0441\n",
            "epoch7630, loss = 0.0441\n",
            "epoch7640, loss = 0.0441\n",
            "epoch7650, loss = 0.0441\n",
            "epoch7660, loss = 0.0441\n",
            "epoch7670, loss = 0.0441\n",
            "epoch7680, loss = 0.0440\n",
            "epoch7690, loss = 0.0440\n",
            "epoch7700, loss = 0.0440\n",
            "epoch7710, loss = 0.0440\n",
            "epoch7720, loss = 0.0440\n",
            "epoch7730, loss = 0.0440\n",
            "epoch7740, loss = 0.0439\n",
            "epoch7750, loss = 0.0439\n",
            "epoch7760, loss = 0.0439\n",
            "epoch7770, loss = 0.0439\n",
            "epoch7780, loss = 0.0439\n",
            "epoch7790, loss = 0.0438\n",
            "epoch7800, loss = 0.0438\n",
            "epoch7810, loss = 0.0438\n",
            "epoch7820, loss = 0.0438\n",
            "epoch7830, loss = 0.0438\n",
            "epoch7840, loss = 0.0438\n",
            "epoch7850, loss = 0.0437\n",
            "epoch7860, loss = 0.0437\n",
            "epoch7870, loss = 0.0437\n",
            "epoch7880, loss = 0.0437\n",
            "epoch7890, loss = 0.0437\n",
            "epoch7900, loss = 0.0437\n",
            "epoch7910, loss = 0.0436\n",
            "epoch7920, loss = 0.0436\n",
            "epoch7930, loss = 0.0436\n",
            "epoch7940, loss = 0.0436\n",
            "epoch7950, loss = 0.0436\n",
            "epoch7960, loss = 0.0436\n",
            "epoch7970, loss = 0.0435\n",
            "epoch7980, loss = 0.0435\n",
            "epoch7990, loss = 0.0435\n",
            "epoch8000, loss = 0.0435\n",
            "epoch8010, loss = 0.0435\n",
            "epoch8020, loss = 0.0435\n",
            "epoch8030, loss = 0.0434\n",
            "epoch8040, loss = 0.0434\n",
            "epoch8050, loss = 0.0434\n",
            "epoch8060, loss = 0.0434\n",
            "epoch8070, loss = 0.0434\n",
            "epoch8080, loss = 0.0434\n",
            "epoch8090, loss = 0.0433\n",
            "epoch8100, loss = 0.0433\n",
            "epoch8110, loss = 0.0433\n",
            "epoch8120, loss = 0.0433\n",
            "epoch8130, loss = 0.0433\n",
            "epoch8140, loss = 0.0433\n",
            "epoch8150, loss = 0.0432\n",
            "epoch8160, loss = 0.0432\n",
            "epoch8170, loss = 0.0432\n",
            "epoch8180, loss = 0.0432\n",
            "epoch8190, loss = 0.0432\n",
            "epoch8200, loss = 0.0432\n",
            "epoch8210, loss = 0.0432\n",
            "epoch8220, loss = 0.0431\n",
            "epoch8230, loss = 0.0431\n",
            "epoch8240, loss = 0.0431\n",
            "epoch8250, loss = 0.0431\n",
            "epoch8260, loss = 0.0431\n",
            "epoch8270, loss = 0.0431\n",
            "epoch8280, loss = 0.0430\n",
            "epoch8290, loss = 0.0430\n",
            "epoch8300, loss = 0.0430\n",
            "epoch8310, loss = 0.0430\n",
            "epoch8320, loss = 0.0430\n",
            "epoch8330, loss = 0.0430\n",
            "epoch8340, loss = 0.0429\n",
            "epoch8350, loss = 0.0429\n",
            "epoch8360, loss = 0.0429\n",
            "epoch8370, loss = 0.0429\n",
            "epoch8380, loss = 0.0429\n",
            "epoch8390, loss = 0.0429\n",
            "epoch8400, loss = 0.0429\n",
            "epoch8410, loss = 0.0428\n",
            "epoch8420, loss = 0.0428\n",
            "epoch8430, loss = 0.0428\n",
            "epoch8440, loss = 0.0428\n",
            "epoch8450, loss = 0.0428\n",
            "epoch8460, loss = 0.0428\n",
            "epoch8470, loss = 0.0427\n",
            "epoch8480, loss = 0.0427\n",
            "epoch8490, loss = 0.0427\n",
            "epoch8500, loss = 0.0427\n",
            "epoch8510, loss = 0.0427\n",
            "epoch8520, loss = 0.0427\n",
            "epoch8530, loss = 0.0427\n",
            "epoch8540, loss = 0.0426\n",
            "epoch8550, loss = 0.0426\n",
            "epoch8560, loss = 0.0426\n",
            "epoch8570, loss = 0.0426\n",
            "epoch8580, loss = 0.0426\n",
            "epoch8590, loss = 0.0426\n",
            "epoch8600, loss = 0.0425\n",
            "epoch8610, loss = 0.0425\n",
            "epoch8620, loss = 0.0425\n",
            "epoch8630, loss = 0.0425\n",
            "epoch8640, loss = 0.0425\n",
            "epoch8650, loss = 0.0425\n",
            "epoch8660, loss = 0.0425\n",
            "epoch8670, loss = 0.0424\n",
            "epoch8680, loss = 0.0424\n",
            "epoch8690, loss = 0.0424\n",
            "epoch8700, loss = 0.0424\n",
            "epoch8710, loss = 0.0424\n",
            "epoch8720, loss = 0.0424\n",
            "epoch8730, loss = 0.0424\n",
            "epoch8740, loss = 0.0423\n",
            "epoch8750, loss = 0.0423\n",
            "epoch8760, loss = 0.0423\n",
            "epoch8770, loss = 0.0423\n",
            "epoch8780, loss = 0.0423\n",
            "epoch8790, loss = 0.0423\n",
            "epoch8800, loss = 0.0422\n",
            "epoch8810, loss = 0.0422\n",
            "epoch8820, loss = 0.0422\n",
            "epoch8830, loss = 0.0422\n",
            "epoch8840, loss = 0.0422\n",
            "epoch8850, loss = 0.0422\n",
            "epoch8860, loss = 0.0422\n",
            "epoch8870, loss = 0.0421\n",
            "epoch8880, loss = 0.0421\n",
            "epoch8890, loss = 0.0421\n",
            "epoch8900, loss = 0.0421\n",
            "epoch8910, loss = 0.0421\n",
            "epoch8920, loss = 0.0421\n",
            "epoch8930, loss = 0.0421\n",
            "epoch8940, loss = 0.0420\n",
            "epoch8950, loss = 0.0420\n",
            "epoch8960, loss = 0.0420\n",
            "epoch8970, loss = 0.0420\n",
            "epoch8980, loss = 0.0420\n",
            "epoch8990, loss = 0.0420\n",
            "epoch9000, loss = 0.0420\n",
            "epoch9010, loss = 0.0419\n",
            "epoch9020, loss = 0.0419\n",
            "epoch9030, loss = 0.0419\n",
            "epoch9040, loss = 0.0419\n",
            "epoch9050, loss = 0.0419\n",
            "epoch9060, loss = 0.0419\n",
            "epoch9070, loss = 0.0419\n",
            "epoch9080, loss = 0.0418\n",
            "epoch9090, loss = 0.0418\n",
            "epoch9100, loss = 0.0418\n",
            "epoch9110, loss = 0.0418\n",
            "epoch9120, loss = 0.0418\n",
            "epoch9130, loss = 0.0418\n",
            "epoch9140, loss = 0.0418\n",
            "epoch9150, loss = 0.0418\n",
            "epoch9160, loss = 0.0417\n",
            "epoch9170, loss = 0.0417\n",
            "epoch9180, loss = 0.0417\n",
            "epoch9190, loss = 0.0417\n",
            "epoch9200, loss = 0.0417\n",
            "epoch9210, loss = 0.0417\n",
            "epoch9220, loss = 0.0417\n",
            "epoch9230, loss = 0.0416\n",
            "epoch9240, loss = 0.0416\n",
            "epoch9250, loss = 0.0416\n",
            "epoch9260, loss = 0.0416\n",
            "epoch9270, loss = 0.0416\n",
            "epoch9280, loss = 0.0416\n",
            "epoch9290, loss = 0.0416\n",
            "epoch9300, loss = 0.0415\n",
            "epoch9310, loss = 0.0415\n",
            "epoch9320, loss = 0.0415\n",
            "epoch9330, loss = 0.0415\n",
            "epoch9340, loss = 0.0415\n",
            "epoch9350, loss = 0.0415\n",
            "epoch9360, loss = 0.0415\n",
            "epoch9370, loss = 0.0415\n",
            "epoch9380, loss = 0.0414\n",
            "epoch9390, loss = 0.0414\n",
            "epoch9400, loss = 0.0414\n",
            "epoch9410, loss = 0.0414\n",
            "epoch9420, loss = 0.0414\n",
            "epoch9430, loss = 0.0414\n",
            "epoch9440, loss = 0.0414\n",
            "epoch9450, loss = 0.0413\n",
            "epoch9460, loss = 0.0413\n",
            "epoch9470, loss = 0.0413\n",
            "epoch9480, loss = 0.0413\n",
            "epoch9490, loss = 0.0413\n",
            "epoch9500, loss = 0.0413\n",
            "epoch9510, loss = 0.0413\n",
            "epoch9520, loss = 0.0413\n",
            "epoch9530, loss = 0.0412\n",
            "epoch9540, loss = 0.0412\n",
            "epoch9550, loss = 0.0412\n",
            "epoch9560, loss = 0.0412\n",
            "epoch9570, loss = 0.0412\n",
            "epoch9580, loss = 0.0412\n",
            "epoch9590, loss = 0.0412\n",
            "epoch9600, loss = 0.0411\n",
            "epoch9610, loss = 0.0411\n",
            "epoch9620, loss = 0.0411\n",
            "epoch9630, loss = 0.0411\n",
            "epoch9640, loss = 0.0411\n",
            "epoch9650, loss = 0.0411\n",
            "epoch9660, loss = 0.0411\n",
            "epoch9670, loss = 0.0411\n",
            "epoch9680, loss = 0.0410\n",
            "epoch9690, loss = 0.0410\n",
            "epoch9700, loss = 0.0410\n",
            "epoch9710, loss = 0.0410\n",
            "epoch9720, loss = 0.0410\n",
            "epoch9730, loss = 0.0410\n",
            "epoch9740, loss = 0.0410\n",
            "epoch9750, loss = 0.0410\n",
            "epoch9760, loss = 0.0409\n",
            "epoch9770, loss = 0.0409\n",
            "epoch9780, loss = 0.0409\n",
            "epoch9790, loss = 0.0409\n",
            "epoch9800, loss = 0.0409\n",
            "epoch9810, loss = 0.0409\n",
            "epoch9820, loss = 0.0409\n",
            "epoch9830, loss = 0.0409\n",
            "epoch9840, loss = 0.0408\n",
            "epoch9850, loss = 0.0408\n",
            "epoch9860, loss = 0.0408\n",
            "epoch9870, loss = 0.0408\n",
            "epoch9880, loss = 0.0408\n",
            "epoch9890, loss = 0.0408\n",
            "epoch9900, loss = 0.0408\n",
            "epoch9910, loss = 0.0408\n",
            "epoch9920, loss = 0.0407\n",
            "epoch9930, loss = 0.0407\n",
            "epoch9940, loss = 0.0407\n",
            "epoch9950, loss = 0.0407\n",
            "epoch9960, loss = 0.0407\n",
            "epoch9970, loss = 0.0407\n",
            "epoch9980, loss = 0.0407\n",
            "epoch9990, loss = 0.0407\n",
            "epoch10000, loss = 0.0406\n",
            "epoch10010, loss = 0.0406\n",
            "epoch10020, loss = 0.0406\n",
            "epoch10030, loss = 0.0406\n",
            "epoch10040, loss = 0.0406\n",
            "epoch10050, loss = 0.0406\n",
            "epoch10060, loss = 0.0406\n",
            "epoch10070, loss = 0.0406\n",
            "epoch10080, loss = 0.0405\n",
            "epoch10090, loss = 0.0405\n",
            "epoch10100, loss = 0.0405\n",
            "epoch10110, loss = 0.0405\n",
            "epoch10120, loss = 0.0405\n",
            "epoch10130, loss = 0.0405\n",
            "epoch10140, loss = 0.0405\n",
            "epoch10150, loss = 0.0405\n",
            "epoch10160, loss = 0.0405\n",
            "epoch10170, loss = 0.0404\n",
            "epoch10180, loss = 0.0404\n",
            "epoch10190, loss = 0.0404\n",
            "epoch10200, loss = 0.0404\n",
            "epoch10210, loss = 0.0404\n",
            "epoch10220, loss = 0.0404\n",
            "epoch10230, loss = 0.0404\n",
            "epoch10240, loss = 0.0404\n",
            "epoch10250, loss = 0.0403\n",
            "epoch10260, loss = 0.0403\n",
            "epoch10270, loss = 0.0403\n",
            "epoch10280, loss = 0.0403\n",
            "epoch10290, loss = 0.0403\n",
            "epoch10300, loss = 0.0403\n",
            "epoch10310, loss = 0.0403\n",
            "epoch10320, loss = 0.0403\n",
            "epoch10330, loss = 0.0403\n",
            "epoch10340, loss = 0.0402\n",
            "epoch10350, loss = 0.0402\n",
            "epoch10360, loss = 0.0402\n",
            "epoch10370, loss = 0.0402\n",
            "epoch10380, loss = 0.0402\n",
            "epoch10390, loss = 0.0402\n",
            "epoch10400, loss = 0.0402\n",
            "epoch10410, loss = 0.0402\n",
            "epoch10420, loss = 0.0401\n",
            "epoch10430, loss = 0.0401\n",
            "epoch10440, loss = 0.0401\n",
            "epoch10450, loss = 0.0401\n",
            "epoch10460, loss = 0.0401\n",
            "epoch10470, loss = 0.0401\n",
            "epoch10480, loss = 0.0401\n",
            "epoch10490, loss = 0.0401\n",
            "epoch10500, loss = 0.0401\n",
            "epoch10510, loss = 0.0400\n",
            "epoch10520, loss = 0.0400\n",
            "epoch10530, loss = 0.0400\n",
            "epoch10540, loss = 0.0400\n",
            "epoch10550, loss = 0.0400\n",
            "epoch10560, loss = 0.0400\n",
            "epoch10570, loss = 0.0400\n",
            "epoch10580, loss = 0.0400\n",
            "epoch10590, loss = 0.0400\n",
            "epoch10600, loss = 0.0399\n",
            "epoch10610, loss = 0.0399\n",
            "epoch10620, loss = 0.0399\n",
            "epoch10630, loss = 0.0399\n",
            "epoch10640, loss = 0.0399\n",
            "epoch10650, loss = 0.0399\n",
            "epoch10660, loss = 0.0399\n",
            "epoch10670, loss = 0.0399\n",
            "epoch10680, loss = 0.0399\n",
            "epoch10690, loss = 0.0398\n",
            "epoch10700, loss = 0.0398\n",
            "epoch10710, loss = 0.0398\n",
            "epoch10720, loss = 0.0398\n",
            "epoch10730, loss = 0.0398\n",
            "epoch10740, loss = 0.0398\n",
            "epoch10750, loss = 0.0398\n",
            "epoch10760, loss = 0.0398\n",
            "epoch10770, loss = 0.0398\n",
            "epoch10780, loss = 0.0397\n",
            "epoch10790, loss = 0.0397\n",
            "epoch10800, loss = 0.0397\n",
            "epoch10810, loss = 0.0397\n",
            "epoch10820, loss = 0.0397\n",
            "epoch10830, loss = 0.0397\n",
            "epoch10840, loss = 0.0397\n",
            "epoch10850, loss = 0.0397\n",
            "epoch10860, loss = 0.0397\n",
            "epoch10870, loss = 0.0396\n",
            "epoch10880, loss = 0.0396\n",
            "epoch10890, loss = 0.0396\n",
            "epoch10900, loss = 0.0396\n",
            "epoch10910, loss = 0.0396\n",
            "epoch10920, loss = 0.0396\n",
            "epoch10930, loss = 0.0396\n",
            "epoch10940, loss = 0.0396\n",
            "epoch10950, loss = 0.0396\n",
            "epoch10960, loss = 0.0395\n",
            "epoch10970, loss = 0.0395\n",
            "epoch10980, loss = 0.0395\n",
            "epoch10990, loss = 0.0395\n",
            "epoch11000, loss = 0.0395\n",
            "epoch11010, loss = 0.0395\n",
            "epoch11020, loss = 0.0395\n",
            "epoch11030, loss = 0.0395\n",
            "epoch11040, loss = 0.0395\n",
            "epoch11050, loss = 0.0394\n",
            "epoch11060, loss = 0.0394\n",
            "epoch11070, loss = 0.0394\n",
            "epoch11080, loss = 0.0394\n",
            "epoch11090, loss = 0.0394\n",
            "epoch11100, loss = 0.0394\n",
            "epoch11110, loss = 0.0394\n",
            "epoch11120, loss = 0.0394\n",
            "epoch11130, loss = 0.0394\n",
            "epoch11140, loss = 0.0394\n",
            "epoch11150, loss = 0.0393\n",
            "epoch11160, loss = 0.0393\n",
            "epoch11170, loss = 0.0393\n",
            "epoch11180, loss = 0.0393\n",
            "epoch11190, loss = 0.0393\n",
            "epoch11200, loss = 0.0393\n",
            "epoch11210, loss = 0.0393\n",
            "epoch11220, loss = 0.0393\n",
            "epoch11230, loss = 0.0393\n",
            "epoch11240, loss = 0.0392\n",
            "epoch11250, loss = 0.0392\n",
            "epoch11260, loss = 0.0392\n",
            "epoch11270, loss = 0.0392\n",
            "epoch11280, loss = 0.0392\n",
            "epoch11290, loss = 0.0392\n",
            "epoch11300, loss = 0.0392\n",
            "epoch11310, loss = 0.0392\n",
            "epoch11320, loss = 0.0392\n",
            "epoch11330, loss = 0.0392\n",
            "epoch11340, loss = 0.0391\n",
            "epoch11350, loss = 0.0391\n",
            "epoch11360, loss = 0.0391\n",
            "epoch11370, loss = 0.0391\n",
            "epoch11380, loss = 0.0391\n",
            "epoch11390, loss = 0.0391\n",
            "epoch11400, loss = 0.0391\n",
            "epoch11410, loss = 0.0391\n",
            "epoch11420, loss = 0.0391\n",
            "epoch11430, loss = 0.0391\n",
            "epoch11440, loss = 0.0390\n",
            "epoch11450, loss = 0.0390\n",
            "epoch11460, loss = 0.0390\n",
            "epoch11470, loss = 0.0390\n",
            "epoch11480, loss = 0.0390\n",
            "epoch11490, loss = 0.0390\n",
            "epoch11500, loss = 0.0390\n",
            "epoch11510, loss = 0.0390\n",
            "epoch11520, loss = 0.0390\n",
            "epoch11530, loss = 0.0390\n",
            "epoch11540, loss = 0.0389\n",
            "epoch11550, loss = 0.0389\n",
            "epoch11560, loss = 0.0389\n",
            "epoch11570, loss = 0.0389\n",
            "epoch11580, loss = 0.0389\n",
            "epoch11590, loss = 0.0389\n",
            "epoch11600, loss = 0.0389\n",
            "epoch11610, loss = 0.0389\n",
            "epoch11620, loss = 0.0389\n",
            "epoch11630, loss = 0.0389\n",
            "epoch11640, loss = 0.0388\n",
            "epoch11650, loss = 0.0388\n",
            "epoch11660, loss = 0.0388\n",
            "epoch11670, loss = 0.0388\n",
            "epoch11680, loss = 0.0388\n",
            "epoch11690, loss = 0.0388\n",
            "epoch11700, loss = 0.0388\n",
            "epoch11710, loss = 0.0388\n",
            "epoch11720, loss = 0.0388\n",
            "epoch11730, loss = 0.0388\n",
            "epoch11740, loss = 0.0387\n",
            "epoch11750, loss = 0.0387\n",
            "epoch11760, loss = 0.0387\n",
            "epoch11770, loss = 0.0387\n",
            "epoch11780, loss = 0.0387\n",
            "epoch11790, loss = 0.0387\n",
            "epoch11800, loss = 0.0387\n",
            "epoch11810, loss = 0.0387\n",
            "epoch11820, loss = 0.0387\n",
            "epoch11830, loss = 0.0387\n",
            "epoch11840, loss = 0.0386\n",
            "epoch11850, loss = 0.0386\n",
            "epoch11860, loss = 0.0386\n",
            "epoch11870, loss = 0.0386\n",
            "epoch11880, loss = 0.0386\n",
            "epoch11890, loss = 0.0386\n",
            "epoch11900, loss = 0.0386\n",
            "epoch11910, loss = 0.0386\n",
            "epoch11920, loss = 0.0386\n",
            "epoch11930, loss = 0.0386\n",
            "epoch11940, loss = 0.0385\n",
            "epoch11950, loss = 0.0385\n",
            "epoch11960, loss = 0.0385\n",
            "epoch11970, loss = 0.0385\n",
            "epoch11980, loss = 0.0385\n",
            "epoch11990, loss = 0.0385\n",
            "epoch12000, loss = 0.0385\n",
            "epoch12010, loss = 0.0385\n",
            "epoch12020, loss = 0.0385\n",
            "epoch12030, loss = 0.0385\n",
            "epoch12040, loss = 0.0385\n",
            "epoch12050, loss = 0.0384\n",
            "epoch12060, loss = 0.0384\n",
            "epoch12070, loss = 0.0384\n",
            "epoch12080, loss = 0.0384\n",
            "epoch12090, loss = 0.0384\n",
            "epoch12100, loss = 0.0384\n",
            "epoch12110, loss = 0.0384\n",
            "epoch12120, loss = 0.0384\n",
            "epoch12130, loss = 0.0384\n",
            "epoch12140, loss = 0.0384\n",
            "epoch12150, loss = 0.0384\n",
            "epoch12160, loss = 0.0383\n",
            "epoch12170, loss = 0.0383\n",
            "epoch12180, loss = 0.0383\n",
            "epoch12190, loss = 0.0383\n",
            "epoch12200, loss = 0.0383\n",
            "epoch12210, loss = 0.0383\n",
            "epoch12220, loss = 0.0383\n",
            "epoch12230, loss = 0.0383\n",
            "epoch12240, loss = 0.0383\n",
            "epoch12250, loss = 0.0383\n",
            "epoch12260, loss = 0.0382\n",
            "epoch12270, loss = 0.0382\n",
            "epoch12280, loss = 0.0382\n",
            "epoch12290, loss = 0.0382\n",
            "epoch12300, loss = 0.0382\n",
            "epoch12310, loss = 0.0382\n",
            "epoch12320, loss = 0.0382\n",
            "epoch12330, loss = 0.0382\n",
            "epoch12340, loss = 0.0382\n",
            "epoch12350, loss = 0.0382\n",
            "epoch12360, loss = 0.0382\n",
            "epoch12370, loss = 0.0381\n",
            "epoch12380, loss = 0.0381\n",
            "epoch12390, loss = 0.0381\n",
            "epoch12400, loss = 0.0381\n",
            "epoch12410, loss = 0.0381\n",
            "epoch12420, loss = 0.0381\n",
            "epoch12430, loss = 0.0381\n",
            "epoch12440, loss = 0.0381\n",
            "epoch12450, loss = 0.0381\n",
            "epoch12460, loss = 0.0381\n",
            "epoch12470, loss = 0.0381\n",
            "epoch12480, loss = 0.0380\n",
            "epoch12490, loss = 0.0380\n",
            "epoch12500, loss = 0.0380\n",
            "epoch12510, loss = 0.0380\n",
            "epoch12520, loss = 0.0380\n",
            "epoch12530, loss = 0.0380\n",
            "epoch12540, loss = 0.0380\n",
            "epoch12550, loss = 0.0380\n",
            "epoch12560, loss = 0.0380\n",
            "epoch12570, loss = 0.0380\n",
            "epoch12580, loss = 0.0380\n",
            "epoch12590, loss = 0.0379\n",
            "epoch12600, loss = 0.0379\n",
            "epoch12610, loss = 0.0379\n",
            "epoch12620, loss = 0.0379\n",
            "epoch12630, loss = 0.0379\n",
            "epoch12640, loss = 0.0379\n",
            "epoch12650, loss = 0.0379\n",
            "epoch12660, loss = 0.0379\n",
            "epoch12670, loss = 0.0379\n",
            "epoch12680, loss = 0.0379\n",
            "epoch12690, loss = 0.0379\n",
            "epoch12700, loss = 0.0379\n",
            "epoch12710, loss = 0.0378\n",
            "epoch12720, loss = 0.0378\n",
            "epoch12730, loss = 0.0378\n",
            "epoch12740, loss = 0.0378\n",
            "epoch12750, loss = 0.0378\n",
            "epoch12760, loss = 0.0378\n",
            "epoch12770, loss = 0.0378\n",
            "epoch12780, loss = 0.0378\n",
            "epoch12790, loss = 0.0378\n",
            "epoch12800, loss = 0.0378\n",
            "epoch12810, loss = 0.0378\n",
            "epoch12820, loss = 0.0377\n",
            "epoch12830, loss = 0.0377\n",
            "epoch12840, loss = 0.0377\n",
            "epoch12850, loss = 0.0377\n",
            "epoch12860, loss = 0.0377\n",
            "epoch12870, loss = 0.0377\n",
            "epoch12880, loss = 0.0377\n",
            "epoch12890, loss = 0.0377\n",
            "epoch12900, loss = 0.0377\n",
            "epoch12910, loss = 0.0377\n",
            "epoch12920, loss = 0.0377\n",
            "epoch12930, loss = 0.0377\n",
            "epoch12940, loss = 0.0376\n",
            "epoch12950, loss = 0.0376\n",
            "epoch12960, loss = 0.0376\n",
            "epoch12970, loss = 0.0376\n",
            "epoch12980, loss = 0.0376\n",
            "epoch12990, loss = 0.0376\n",
            "epoch13000, loss = 0.0376\n",
            "epoch13010, loss = 0.0376\n",
            "epoch13020, loss = 0.0376\n",
            "epoch13030, loss = 0.0376\n",
            "epoch13040, loss = 0.0376\n",
            "epoch13050, loss = 0.0376\n",
            "epoch13060, loss = 0.0375\n",
            "epoch13070, loss = 0.0375\n",
            "epoch13080, loss = 0.0375\n",
            "epoch13090, loss = 0.0375\n",
            "epoch13100, loss = 0.0375\n",
            "epoch13110, loss = 0.0375\n",
            "epoch13120, loss = 0.0375\n",
            "epoch13130, loss = 0.0375\n",
            "epoch13140, loss = 0.0375\n",
            "epoch13150, loss = 0.0375\n",
            "epoch13160, loss = 0.0375\n",
            "epoch13170, loss = 0.0375\n",
            "epoch13180, loss = 0.0374\n",
            "epoch13190, loss = 0.0374\n",
            "epoch13200, loss = 0.0374\n",
            "epoch13210, loss = 0.0374\n",
            "epoch13220, loss = 0.0374\n",
            "epoch13230, loss = 0.0374\n",
            "epoch13240, loss = 0.0374\n",
            "epoch13250, loss = 0.0374\n",
            "epoch13260, loss = 0.0374\n",
            "epoch13270, loss = 0.0374\n",
            "epoch13280, loss = 0.0374\n",
            "epoch13290, loss = 0.0374\n",
            "epoch13300, loss = 0.0373\n",
            "epoch13310, loss = 0.0373\n",
            "epoch13320, loss = 0.0373\n",
            "epoch13330, loss = 0.0373\n",
            "epoch13340, loss = 0.0373\n",
            "epoch13350, loss = 0.0373\n",
            "epoch13360, loss = 0.0373\n",
            "epoch13370, loss = 0.0373\n",
            "epoch13380, loss = 0.0373\n",
            "epoch13390, loss = 0.0373\n",
            "epoch13400, loss = 0.0373\n",
            "epoch13410, loss = 0.0373\n",
            "epoch13420, loss = 0.0372\n",
            "epoch13430, loss = 0.0372\n",
            "epoch13440, loss = 0.0372\n",
            "epoch13450, loss = 0.0372\n",
            "epoch13460, loss = 0.0372\n",
            "epoch13470, loss = 0.0372\n",
            "epoch13480, loss = 0.0372\n",
            "epoch13490, loss = 0.0372\n",
            "epoch13500, loss = 0.0372\n",
            "epoch13510, loss = 0.0372\n",
            "epoch13520, loss = 0.0372\n",
            "epoch13530, loss = 0.0372\n",
            "epoch13540, loss = 0.0371\n",
            "epoch13550, loss = 0.0371\n",
            "epoch13560, loss = 0.0371\n",
            "epoch13570, loss = 0.0371\n",
            "epoch13580, loss = 0.0371\n",
            "epoch13590, loss = 0.0371\n",
            "epoch13600, loss = 0.0371\n",
            "epoch13610, loss = 0.0371\n",
            "epoch13620, loss = 0.0371\n",
            "epoch13630, loss = 0.0371\n",
            "epoch13640, loss = 0.0371\n",
            "epoch13650, loss = 0.0371\n",
            "epoch13660, loss = 0.0371\n",
            "epoch13670, loss = 0.0370\n",
            "epoch13680, loss = 0.0370\n",
            "epoch13690, loss = 0.0370\n",
            "epoch13700, loss = 0.0370\n",
            "epoch13710, loss = 0.0370\n",
            "epoch13720, loss = 0.0370\n",
            "epoch13730, loss = 0.0370\n",
            "epoch13740, loss = 0.0370\n",
            "epoch13750, loss = 0.0370\n",
            "epoch13760, loss = 0.0370\n",
            "epoch13770, loss = 0.0370\n",
            "epoch13780, loss = 0.0370\n",
            "epoch13790, loss = 0.0369\n",
            "epoch13800, loss = 0.0369\n",
            "epoch13810, loss = 0.0369\n",
            "epoch13820, loss = 0.0369\n",
            "epoch13830, loss = 0.0369\n",
            "epoch13840, loss = 0.0369\n",
            "epoch13850, loss = 0.0369\n",
            "epoch13860, loss = 0.0369\n",
            "epoch13870, loss = 0.0369\n",
            "epoch13880, loss = 0.0369\n",
            "epoch13890, loss = 0.0369\n",
            "epoch13900, loss = 0.0369\n",
            "epoch13910, loss = 0.0369\n",
            "epoch13920, loss = 0.0368\n",
            "epoch13930, loss = 0.0368\n",
            "epoch13940, loss = 0.0368\n",
            "epoch13950, loss = 0.0368\n",
            "epoch13960, loss = 0.0368\n",
            "epoch13970, loss = 0.0368\n",
            "epoch13980, loss = 0.0368\n",
            "epoch13990, loss = 0.0368\n",
            "epoch14000, loss = 0.0368\n",
            "epoch14010, loss = 0.0368\n",
            "epoch14020, loss = 0.0368\n",
            "epoch14030, loss = 0.0368\n",
            "epoch14040, loss = 0.0368\n",
            "epoch14050, loss = 0.0367\n",
            "epoch14060, loss = 0.0367\n",
            "epoch14070, loss = 0.0367\n",
            "epoch14080, loss = 0.0367\n",
            "epoch14090, loss = 0.0367\n",
            "epoch14100, loss = 0.0367\n",
            "epoch14110, loss = 0.0367\n",
            "epoch14120, loss = 0.0367\n",
            "epoch14130, loss = 0.0367\n",
            "epoch14140, loss = 0.0367\n",
            "epoch14150, loss = 0.0367\n",
            "epoch14160, loss = 0.0367\n",
            "epoch14170, loss = 0.0367\n",
            "epoch14180, loss = 0.0366\n",
            "epoch14190, loss = 0.0366\n",
            "epoch14200, loss = 0.0366\n",
            "epoch14210, loss = 0.0366\n",
            "epoch14220, loss = 0.0366\n",
            "epoch14230, loss = 0.0366\n",
            "epoch14240, loss = 0.0366\n",
            "epoch14250, loss = 0.0366\n",
            "epoch14260, loss = 0.0366\n",
            "epoch14270, loss = 0.0366\n",
            "epoch14280, loss = 0.0366\n",
            "epoch14290, loss = 0.0366\n",
            "epoch14300, loss = 0.0366\n",
            "epoch14310, loss = 0.0366\n",
            "epoch14320, loss = 0.0365\n",
            "epoch14330, loss = 0.0365\n",
            "epoch14340, loss = 0.0365\n",
            "epoch14350, loss = 0.0365\n",
            "epoch14360, loss = 0.0365\n",
            "epoch14370, loss = 0.0365\n",
            "epoch14380, loss = 0.0365\n",
            "epoch14390, loss = 0.0365\n",
            "epoch14400, loss = 0.0365\n",
            "epoch14410, loss = 0.0365\n",
            "epoch14420, loss = 0.0365\n",
            "epoch14430, loss = 0.0365\n",
            "epoch14440, loss = 0.0365\n",
            "epoch14450, loss = 0.0364\n",
            "epoch14460, loss = 0.0364\n",
            "epoch14470, loss = 0.0364\n",
            "epoch14480, loss = 0.0364\n",
            "epoch14490, loss = 0.0364\n",
            "epoch14500, loss = 0.0364\n",
            "epoch14510, loss = 0.0364\n",
            "epoch14520, loss = 0.0364\n",
            "epoch14530, loss = 0.0364\n",
            "epoch14540, loss = 0.0364\n",
            "epoch14550, loss = 0.0364\n",
            "epoch14560, loss = 0.0364\n",
            "epoch14570, loss = 0.0364\n",
            "epoch14580, loss = 0.0364\n",
            "epoch14590, loss = 0.0363\n",
            "epoch14600, loss = 0.0363\n",
            "epoch14610, loss = 0.0363\n",
            "epoch14620, loss = 0.0363\n",
            "epoch14630, loss = 0.0363\n",
            "epoch14640, loss = 0.0363\n",
            "epoch14650, loss = 0.0363\n",
            "epoch14660, loss = 0.0363\n",
            "epoch14670, loss = 0.0363\n",
            "epoch14680, loss = 0.0363\n",
            "epoch14690, loss = 0.0363\n",
            "epoch14700, loss = 0.0363\n",
            "epoch14710, loss = 0.0363\n",
            "epoch14720, loss = 0.0363\n",
            "epoch14730, loss = 0.0362\n",
            "epoch14740, loss = 0.0362\n",
            "epoch14750, loss = 0.0362\n",
            "epoch14760, loss = 0.0362\n",
            "epoch14770, loss = 0.0362\n",
            "epoch14780, loss = 0.0362\n",
            "epoch14790, loss = 0.0362\n",
            "epoch14800, loss = 0.0362\n",
            "epoch14810, loss = 0.0362\n",
            "epoch14820, loss = 0.0362\n",
            "epoch14830, loss = 0.0362\n",
            "epoch14840, loss = 0.0362\n",
            "epoch14850, loss = 0.0362\n",
            "epoch14860, loss = 0.0362\n",
            "epoch14870, loss = 0.0361\n",
            "epoch14880, loss = 0.0361\n",
            "epoch14890, loss = 0.0361\n",
            "epoch14900, loss = 0.0361\n",
            "epoch14910, loss = 0.0361\n",
            "epoch14920, loss = 0.0361\n",
            "epoch14930, loss = 0.0361\n",
            "epoch14940, loss = 0.0361\n",
            "epoch14950, loss = 0.0361\n",
            "epoch14960, loss = 0.0361\n",
            "epoch14970, loss = 0.0361\n",
            "epoch14980, loss = 0.0361\n",
            "epoch14990, loss = 0.0361\n",
            "epoch15000, loss = 0.0361\n",
            "epoch15010, loss = 0.0360\n",
            "epoch15020, loss = 0.0360\n",
            "epoch15030, loss = 0.0360\n",
            "epoch15040, loss = 0.0360\n",
            "epoch15050, loss = 0.0360\n",
            "epoch15060, loss = 0.0360\n",
            "epoch15070, loss = 0.0360\n",
            "epoch15080, loss = 0.0360\n",
            "epoch15090, loss = 0.0360\n",
            "epoch15100, loss = 0.0360\n",
            "epoch15110, loss = 0.0360\n",
            "epoch15120, loss = 0.0360\n",
            "epoch15130, loss = 0.0360\n",
            "epoch15140, loss = 0.0360\n",
            "epoch15150, loss = 0.0360\n",
            "epoch15160, loss = 0.0359\n",
            "epoch15170, loss = 0.0359\n",
            "epoch15180, loss = 0.0359\n",
            "epoch15190, loss = 0.0359\n",
            "epoch15200, loss = 0.0359\n",
            "epoch15210, loss = 0.0359\n",
            "epoch15220, loss = 0.0359\n",
            "epoch15230, loss = 0.0359\n",
            "epoch15240, loss = 0.0359\n",
            "epoch15250, loss = 0.0359\n",
            "epoch15260, loss = 0.0359\n",
            "epoch15270, loss = 0.0359\n",
            "epoch15280, loss = 0.0359\n",
            "epoch15290, loss = 0.0359\n",
            "epoch15300, loss = 0.0359\n",
            "epoch15310, loss = 0.0358\n",
            "epoch15320, loss = 0.0358\n",
            "epoch15330, loss = 0.0358\n",
            "epoch15340, loss = 0.0358\n",
            "epoch15350, loss = 0.0358\n",
            "epoch15360, loss = 0.0358\n",
            "epoch15370, loss = 0.0358\n",
            "epoch15380, loss = 0.0358\n",
            "epoch15390, loss = 0.0358\n",
            "epoch15400, loss = 0.0358\n",
            "epoch15410, loss = 0.0358\n",
            "epoch15420, loss = 0.0358\n",
            "epoch15430, loss = 0.0358\n",
            "epoch15440, loss = 0.0358\n",
            "epoch15450, loss = 0.0357\n",
            "epoch15460, loss = 0.0357\n",
            "epoch15470, loss = 0.0357\n",
            "epoch15480, loss = 0.0357\n",
            "epoch15490, loss = 0.0357\n",
            "epoch15500, loss = 0.0357\n",
            "epoch15510, loss = 0.0357\n",
            "epoch15520, loss = 0.0357\n",
            "epoch15530, loss = 0.0357\n",
            "epoch15540, loss = 0.0357\n",
            "epoch15550, loss = 0.0357\n",
            "epoch15560, loss = 0.0357\n",
            "epoch15570, loss = 0.0357\n",
            "epoch15580, loss = 0.0357\n",
            "epoch15590, loss = 0.0357\n",
            "epoch15600, loss = 0.0357\n",
            "epoch15610, loss = 0.0356\n",
            "epoch15620, loss = 0.0356\n",
            "epoch15630, loss = 0.0356\n",
            "epoch15640, loss = 0.0356\n",
            "epoch15650, loss = 0.0356\n",
            "epoch15660, loss = 0.0356\n",
            "epoch15670, loss = 0.0356\n",
            "epoch15680, loss = 0.0356\n",
            "epoch15690, loss = 0.0356\n",
            "epoch15700, loss = 0.0356\n",
            "epoch15710, loss = 0.0356\n",
            "epoch15720, loss = 0.0356\n",
            "epoch15730, loss = 0.0356\n",
            "epoch15740, loss = 0.0356\n",
            "epoch15750, loss = 0.0356\n",
            "epoch15760, loss = 0.0355\n",
            "epoch15770, loss = 0.0355\n",
            "epoch15780, loss = 0.0355\n",
            "epoch15790, loss = 0.0355\n",
            "epoch15800, loss = 0.0355\n",
            "epoch15810, loss = 0.0355\n",
            "epoch15820, loss = 0.0355\n",
            "epoch15830, loss = 0.0355\n",
            "epoch15840, loss = 0.0355\n",
            "epoch15850, loss = 0.0355\n",
            "epoch15860, loss = 0.0355\n",
            "epoch15870, loss = 0.0355\n",
            "epoch15880, loss = 0.0355\n",
            "epoch15890, loss = 0.0355\n",
            "epoch15900, loss = 0.0355\n",
            "epoch15910, loss = 0.0354\n",
            "epoch15920, loss = 0.0354\n",
            "epoch15930, loss = 0.0354\n",
            "epoch15940, loss = 0.0354\n",
            "epoch15950, loss = 0.0354\n",
            "epoch15960, loss = 0.0354\n",
            "epoch15970, loss = 0.0354\n",
            "epoch15980, loss = 0.0354\n",
            "epoch15990, loss = 0.0354\n",
            "epoch16000, loss = 0.0354\n",
            "epoch16010, loss = 0.0354\n",
            "epoch16020, loss = 0.0354\n",
            "epoch16030, loss = 0.0354\n",
            "epoch16040, loss = 0.0354\n",
            "epoch16050, loss = 0.0354\n",
            "epoch16060, loss = 0.0354\n",
            "epoch16070, loss = 0.0353\n",
            "epoch16080, loss = 0.0353\n",
            "epoch16090, loss = 0.0353\n",
            "epoch16100, loss = 0.0353\n",
            "epoch16110, loss = 0.0353\n",
            "epoch16120, loss = 0.0353\n",
            "epoch16130, loss = 0.0353\n",
            "epoch16140, loss = 0.0353\n",
            "epoch16150, loss = 0.0353\n",
            "epoch16160, loss = 0.0353\n",
            "epoch16170, loss = 0.0353\n",
            "epoch16180, loss = 0.0353\n",
            "epoch16190, loss = 0.0353\n",
            "epoch16200, loss = 0.0353\n",
            "epoch16210, loss = 0.0353\n",
            "epoch16220, loss = 0.0353\n",
            "epoch16230, loss = 0.0352\n",
            "epoch16240, loss = 0.0352\n",
            "epoch16250, loss = 0.0352\n",
            "epoch16260, loss = 0.0352\n",
            "epoch16270, loss = 0.0352\n",
            "epoch16280, loss = 0.0352\n",
            "epoch16290, loss = 0.0352\n",
            "epoch16300, loss = 0.0352\n",
            "epoch16310, loss = 0.0352\n",
            "epoch16320, loss = 0.0352\n",
            "epoch16330, loss = 0.0352\n",
            "epoch16340, loss = 0.0352\n",
            "epoch16350, loss = 0.0352\n",
            "epoch16360, loss = 0.0352\n",
            "epoch16370, loss = 0.0352\n",
            "epoch16380, loss = 0.0352\n",
            "epoch16390, loss = 0.0351\n",
            "epoch16400, loss = 0.0351\n",
            "epoch16410, loss = 0.0351\n",
            "epoch16420, loss = 0.0351\n",
            "epoch16430, loss = 0.0351\n",
            "epoch16440, loss = 0.0351\n",
            "epoch16450, loss = 0.0351\n",
            "epoch16460, loss = 0.0351\n",
            "epoch16470, loss = 0.0351\n",
            "epoch16480, loss = 0.0351\n",
            "epoch16490, loss = 0.0351\n",
            "epoch16500, loss = 0.0351\n",
            "epoch16510, loss = 0.0351\n",
            "epoch16520, loss = 0.0351\n",
            "epoch16530, loss = 0.0351\n",
            "epoch16540, loss = 0.0351\n",
            "epoch16550, loss = 0.0351\n",
            "epoch16560, loss = 0.0350\n",
            "epoch16570, loss = 0.0350\n",
            "epoch16580, loss = 0.0350\n",
            "epoch16590, loss = 0.0350\n",
            "epoch16600, loss = 0.0350\n",
            "epoch16610, loss = 0.0350\n",
            "epoch16620, loss = 0.0350\n",
            "epoch16630, loss = 0.0350\n",
            "epoch16640, loss = 0.0350\n",
            "epoch16650, loss = 0.0350\n",
            "epoch16660, loss = 0.0350\n",
            "epoch16670, loss = 0.0350\n",
            "epoch16680, loss = 0.0350\n",
            "epoch16690, loss = 0.0350\n",
            "epoch16700, loss = 0.0350\n",
            "epoch16710, loss = 0.0350\n",
            "epoch16720, loss = 0.0349\n",
            "epoch16730, loss = 0.0349\n",
            "epoch16740, loss = 0.0349\n",
            "epoch16750, loss = 0.0349\n",
            "epoch16760, loss = 0.0349\n",
            "epoch16770, loss = 0.0349\n",
            "epoch16780, loss = 0.0349\n",
            "epoch16790, loss = 0.0349\n",
            "epoch16800, loss = 0.0349\n",
            "epoch16810, loss = 0.0349\n",
            "epoch16820, loss = 0.0349\n",
            "epoch16830, loss = 0.0349\n",
            "epoch16840, loss = 0.0349\n",
            "epoch16850, loss = 0.0349\n",
            "epoch16860, loss = 0.0349\n",
            "epoch16870, loss = 0.0349\n",
            "epoch16880, loss = 0.0349\n",
            "epoch16890, loss = 0.0348\n",
            "epoch16900, loss = 0.0348\n",
            "epoch16910, loss = 0.0348\n",
            "epoch16920, loss = 0.0348\n",
            "epoch16930, loss = 0.0348\n",
            "epoch16940, loss = 0.0348\n",
            "epoch16950, loss = 0.0348\n",
            "epoch16960, loss = 0.0348\n",
            "epoch16970, loss = 0.0348\n",
            "epoch16980, loss = 0.0348\n",
            "epoch16990, loss = 0.0348\n",
            "epoch17000, loss = 0.0348\n",
            "epoch17010, loss = 0.0348\n",
            "epoch17020, loss = 0.0348\n",
            "epoch17030, loss = 0.0348\n",
            "epoch17040, loss = 0.0348\n",
            "epoch17050, loss = 0.0348\n",
            "epoch17060, loss = 0.0347\n",
            "epoch17070, loss = 0.0347\n",
            "epoch17080, loss = 0.0347\n",
            "epoch17090, loss = 0.0347\n",
            "epoch17100, loss = 0.0347\n",
            "epoch17110, loss = 0.0347\n",
            "epoch17120, loss = 0.0347\n",
            "epoch17130, loss = 0.0347\n",
            "epoch17140, loss = 0.0347\n",
            "epoch17150, loss = 0.0347\n",
            "epoch17160, loss = 0.0347\n",
            "epoch17170, loss = 0.0347\n",
            "epoch17180, loss = 0.0347\n",
            "epoch17190, loss = 0.0347\n",
            "epoch17200, loss = 0.0347\n",
            "epoch17210, loss = 0.0347\n",
            "epoch17220, loss = 0.0347\n",
            "epoch17230, loss = 0.0346\n",
            "epoch17240, loss = 0.0346\n",
            "epoch17250, loss = 0.0346\n",
            "epoch17260, loss = 0.0346\n",
            "epoch17270, loss = 0.0346\n",
            "epoch17280, loss = 0.0346\n",
            "epoch17290, loss = 0.0346\n",
            "epoch17300, loss = 0.0346\n",
            "epoch17310, loss = 0.0346\n",
            "epoch17320, loss = 0.0346\n",
            "epoch17330, loss = 0.0346\n",
            "epoch17340, loss = 0.0346\n",
            "epoch17350, loss = 0.0346\n",
            "epoch17360, loss = 0.0346\n",
            "epoch17370, loss = 0.0346\n",
            "epoch17380, loss = 0.0346\n",
            "epoch17390, loss = 0.0346\n",
            "epoch17400, loss = 0.0346\n",
            "epoch17410, loss = 0.0345\n",
            "epoch17420, loss = 0.0345\n",
            "epoch17430, loss = 0.0345\n",
            "epoch17440, loss = 0.0345\n",
            "epoch17450, loss = 0.0345\n",
            "epoch17460, loss = 0.0345\n",
            "epoch17470, loss = 0.0345\n",
            "epoch17480, loss = 0.0345\n",
            "epoch17490, loss = 0.0345\n",
            "epoch17500, loss = 0.0345\n",
            "epoch17510, loss = 0.0345\n",
            "epoch17520, loss = 0.0345\n",
            "epoch17530, loss = 0.0345\n",
            "epoch17540, loss = 0.0345\n",
            "epoch17550, loss = 0.0345\n",
            "epoch17560, loss = 0.0345\n",
            "epoch17570, loss = 0.0345\n",
            "epoch17580, loss = 0.0345\n",
            "epoch17590, loss = 0.0344\n",
            "epoch17600, loss = 0.0344\n",
            "epoch17610, loss = 0.0344\n",
            "epoch17620, loss = 0.0344\n",
            "epoch17630, loss = 0.0344\n",
            "epoch17640, loss = 0.0344\n",
            "epoch17650, loss = 0.0344\n",
            "epoch17660, loss = 0.0344\n",
            "epoch17670, loss = 0.0344\n",
            "epoch17680, loss = 0.0344\n",
            "epoch17690, loss = 0.0344\n",
            "epoch17700, loss = 0.0344\n",
            "epoch17710, loss = 0.0344\n",
            "epoch17720, loss = 0.0344\n",
            "epoch17730, loss = 0.0344\n",
            "epoch17740, loss = 0.0344\n",
            "epoch17750, loss = 0.0344\n",
            "epoch17760, loss = 0.0344\n",
            "epoch17770, loss = 0.0343\n",
            "epoch17780, loss = 0.0343\n",
            "epoch17790, loss = 0.0343\n",
            "epoch17800, loss = 0.0343\n",
            "epoch17810, loss = 0.0343\n",
            "epoch17820, loss = 0.0343\n",
            "epoch17830, loss = 0.0343\n",
            "epoch17840, loss = 0.0343\n",
            "epoch17850, loss = 0.0343\n",
            "epoch17860, loss = 0.0343\n",
            "epoch17870, loss = 0.0343\n",
            "epoch17880, loss = 0.0343\n",
            "epoch17890, loss = 0.0343\n",
            "epoch17900, loss = 0.0343\n",
            "epoch17910, loss = 0.0343\n",
            "epoch17920, loss = 0.0343\n",
            "epoch17930, loss = 0.0343\n",
            "epoch17940, loss = 0.0343\n",
            "epoch17950, loss = 0.0342\n",
            "epoch17960, loss = 0.0342\n",
            "epoch17970, loss = 0.0342\n",
            "epoch17980, loss = 0.0342\n",
            "epoch17990, loss = 0.0342\n",
            "epoch18000, loss = 0.0342\n",
            "epoch18010, loss = 0.0342\n",
            "epoch18020, loss = 0.0342\n",
            "epoch18030, loss = 0.0342\n",
            "epoch18040, loss = 0.0342\n",
            "epoch18050, loss = 0.0342\n",
            "epoch18060, loss = 0.0342\n",
            "epoch18070, loss = 0.0342\n",
            "epoch18080, loss = 0.0342\n",
            "epoch18090, loss = 0.0342\n",
            "epoch18100, loss = 0.0342\n",
            "epoch18110, loss = 0.0342\n",
            "epoch18120, loss = 0.0342\n",
            "epoch18130, loss = 0.0342\n",
            "epoch18140, loss = 0.0341\n",
            "epoch18150, loss = 0.0341\n",
            "epoch18160, loss = 0.0341\n",
            "epoch18170, loss = 0.0341\n",
            "epoch18180, loss = 0.0341\n",
            "epoch18190, loss = 0.0341\n",
            "epoch18200, loss = 0.0341\n",
            "epoch18210, loss = 0.0341\n",
            "epoch18220, loss = 0.0341\n",
            "epoch18230, loss = 0.0341\n",
            "epoch18240, loss = 0.0341\n",
            "epoch18250, loss = 0.0341\n",
            "epoch18260, loss = 0.0341\n",
            "epoch18270, loss = 0.0341\n",
            "epoch18280, loss = 0.0341\n",
            "epoch18290, loss = 0.0341\n",
            "epoch18300, loss = 0.0341\n",
            "epoch18310, loss = 0.0341\n",
            "epoch18320, loss = 0.0341\n",
            "epoch18330, loss = 0.0340\n",
            "epoch18340, loss = 0.0340\n",
            "epoch18350, loss = 0.0340\n",
            "epoch18360, loss = 0.0340\n",
            "epoch18370, loss = 0.0340\n",
            "epoch18380, loss = 0.0340\n",
            "epoch18390, loss = 0.0340\n",
            "epoch18400, loss = 0.0340\n",
            "epoch18410, loss = 0.0340\n",
            "epoch18420, loss = 0.0340\n",
            "epoch18430, loss = 0.0340\n",
            "epoch18440, loss = 0.0340\n",
            "epoch18450, loss = 0.0340\n",
            "epoch18460, loss = 0.0340\n",
            "epoch18470, loss = 0.0340\n",
            "epoch18480, loss = 0.0340\n",
            "epoch18490, loss = 0.0340\n",
            "epoch18500, loss = 0.0340\n",
            "epoch18510, loss = 0.0340\n",
            "epoch18520, loss = 0.0339\n",
            "epoch18530, loss = 0.0339\n",
            "epoch18540, loss = 0.0339\n",
            "epoch18550, loss = 0.0339\n",
            "epoch18560, loss = 0.0339\n",
            "epoch18570, loss = 0.0339\n",
            "epoch18580, loss = 0.0339\n",
            "epoch18590, loss = 0.0339\n",
            "epoch18600, loss = 0.0339\n",
            "epoch18610, loss = 0.0339\n",
            "epoch18620, loss = 0.0339\n",
            "epoch18630, loss = 0.0339\n",
            "epoch18640, loss = 0.0339\n",
            "epoch18650, loss = 0.0339\n",
            "epoch18660, loss = 0.0339\n",
            "epoch18670, loss = 0.0339\n",
            "epoch18680, loss = 0.0339\n",
            "epoch18690, loss = 0.0339\n",
            "epoch18700, loss = 0.0339\n",
            "epoch18710, loss = 0.0339\n",
            "epoch18720, loss = 0.0338\n",
            "epoch18730, loss = 0.0338\n",
            "epoch18740, loss = 0.0338\n",
            "epoch18750, loss = 0.0338\n",
            "epoch18760, loss = 0.0338\n",
            "epoch18770, loss = 0.0338\n",
            "epoch18780, loss = 0.0338\n",
            "epoch18790, loss = 0.0338\n",
            "epoch18800, loss = 0.0338\n",
            "epoch18810, loss = 0.0338\n",
            "epoch18820, loss = 0.0338\n",
            "epoch18830, loss = 0.0338\n",
            "epoch18840, loss = 0.0338\n",
            "epoch18850, loss = 0.0338\n",
            "epoch18860, loss = 0.0338\n",
            "epoch18870, loss = 0.0338\n",
            "epoch18880, loss = 0.0338\n",
            "epoch18890, loss = 0.0338\n",
            "epoch18900, loss = 0.0338\n",
            "epoch18910, loss = 0.0337\n",
            "epoch18920, loss = 0.0337\n",
            "epoch18930, loss = 0.0337\n",
            "epoch18940, loss = 0.0337\n",
            "epoch18950, loss = 0.0337\n",
            "epoch18960, loss = 0.0337\n",
            "epoch18970, loss = 0.0337\n",
            "epoch18980, loss = 0.0337\n",
            "epoch18990, loss = 0.0337\n",
            "epoch19000, loss = 0.0337\n",
            "epoch19010, loss = 0.0337\n",
            "epoch19020, loss = 0.0337\n",
            "epoch19030, loss = 0.0337\n",
            "epoch19040, loss = 0.0337\n",
            "epoch19050, loss = 0.0337\n",
            "epoch19060, loss = 0.0337\n",
            "epoch19070, loss = 0.0337\n",
            "epoch19080, loss = 0.0337\n",
            "epoch19090, loss = 0.0337\n",
            "epoch19100, loss = 0.0337\n",
            "epoch19110, loss = 0.0336\n",
            "epoch19120, loss = 0.0336\n",
            "epoch19130, loss = 0.0336\n",
            "epoch19140, loss = 0.0336\n",
            "epoch19150, loss = 0.0336\n",
            "epoch19160, loss = 0.0336\n",
            "epoch19170, loss = 0.0336\n",
            "epoch19180, loss = 0.0336\n",
            "epoch19190, loss = 0.0336\n",
            "epoch19200, loss = 0.0336\n",
            "epoch19210, loss = 0.0336\n",
            "epoch19220, loss = 0.0336\n",
            "epoch19230, loss = 0.0336\n",
            "epoch19240, loss = 0.0336\n",
            "epoch19250, loss = 0.0336\n",
            "epoch19260, loss = 0.0336\n",
            "epoch19270, loss = 0.0336\n",
            "epoch19280, loss = 0.0336\n",
            "epoch19290, loss = 0.0336\n",
            "epoch19300, loss = 0.0336\n",
            "epoch19310, loss = 0.0336\n",
            "epoch19320, loss = 0.0335\n",
            "epoch19330, loss = 0.0335\n",
            "epoch19340, loss = 0.0335\n",
            "epoch19350, loss = 0.0335\n",
            "epoch19360, loss = 0.0335\n",
            "epoch19370, loss = 0.0335\n",
            "epoch19380, loss = 0.0335\n",
            "epoch19390, loss = 0.0335\n",
            "epoch19400, loss = 0.0335\n",
            "epoch19410, loss = 0.0335\n",
            "epoch19420, loss = 0.0335\n",
            "epoch19430, loss = 0.0335\n",
            "epoch19440, loss = 0.0335\n",
            "epoch19450, loss = 0.0335\n",
            "epoch19460, loss = 0.0335\n",
            "epoch19470, loss = 0.0335\n",
            "epoch19480, loss = 0.0335\n",
            "epoch19490, loss = 0.0335\n",
            "epoch19500, loss = 0.0335\n",
            "epoch19510, loss = 0.0335\n",
            "epoch19520, loss = 0.0334\n",
            "epoch19530, loss = 0.0334\n",
            "epoch19540, loss = 0.0334\n",
            "epoch19550, loss = 0.0334\n",
            "epoch19560, loss = 0.0334\n",
            "epoch19570, loss = 0.0334\n",
            "epoch19580, loss = 0.0334\n",
            "epoch19590, loss = 0.0334\n",
            "epoch19600, loss = 0.0334\n",
            "epoch19610, loss = 0.0334\n",
            "epoch19620, loss = 0.0334\n",
            "epoch19630, loss = 0.0334\n",
            "epoch19640, loss = 0.0334\n",
            "epoch19650, loss = 0.0334\n",
            "epoch19660, loss = 0.0334\n",
            "epoch19670, loss = 0.0334\n",
            "epoch19680, loss = 0.0334\n",
            "epoch19690, loss = 0.0334\n",
            "epoch19700, loss = 0.0334\n",
            "epoch19710, loss = 0.0334\n",
            "epoch19720, loss = 0.0334\n",
            "epoch19730, loss = 0.0333\n",
            "epoch19740, loss = 0.0333\n",
            "epoch19750, loss = 0.0333\n",
            "epoch19760, loss = 0.0333\n",
            "epoch19770, loss = 0.0333\n",
            "epoch19780, loss = 0.0333\n",
            "epoch19790, loss = 0.0333\n",
            "epoch19800, loss = 0.0333\n",
            "epoch19810, loss = 0.0333\n",
            "epoch19820, loss = 0.0333\n",
            "epoch19830, loss = 0.0333\n",
            "epoch19840, loss = 0.0333\n",
            "epoch19850, loss = 0.0333\n",
            "epoch19860, loss = 0.0333\n",
            "epoch19870, loss = 0.0333\n",
            "epoch19880, loss = 0.0333\n",
            "epoch19890, loss = 0.0333\n",
            "epoch19900, loss = 0.0333\n",
            "epoch19910, loss = 0.0333\n",
            "epoch19920, loss = 0.0333\n",
            "epoch19930, loss = 0.0333\n",
            "epoch19940, loss = 0.0333\n",
            "epoch19950, loss = 0.0332\n",
            "epoch19960, loss = 0.0332\n",
            "epoch19970, loss = 0.0332\n",
            "epoch19980, loss = 0.0332\n",
            "epoch19990, loss = 0.0332\n",
            "epoch20000, loss = 0.0332\n",
            "epoch20010, loss = 0.0332\n",
            "epoch20020, loss = 0.0332\n",
            "epoch20030, loss = 0.0332\n",
            "epoch20040, loss = 0.0332\n",
            "epoch20050, loss = 0.0332\n",
            "epoch20060, loss = 0.0332\n",
            "epoch20070, loss = 0.0332\n",
            "epoch20080, loss = 0.0332\n",
            "epoch20090, loss = 0.0332\n",
            "epoch20100, loss = 0.0332\n",
            "epoch20110, loss = 0.0332\n",
            "epoch20120, loss = 0.0332\n",
            "epoch20130, loss = 0.0332\n",
            "epoch20140, loss = 0.0332\n",
            "epoch20150, loss = 0.0332\n",
            "epoch20160, loss = 0.0331\n",
            "epoch20170, loss = 0.0331\n",
            "epoch20180, loss = 0.0331\n",
            "epoch20190, loss = 0.0331\n",
            "epoch20200, loss = 0.0331\n",
            "epoch20210, loss = 0.0331\n",
            "epoch20220, loss = 0.0331\n",
            "epoch20230, loss = 0.0331\n",
            "epoch20240, loss = 0.0331\n",
            "epoch20250, loss = 0.0331\n",
            "epoch20260, loss = 0.0331\n",
            "epoch20270, loss = 0.0331\n",
            "epoch20280, loss = 0.0331\n",
            "epoch20290, loss = 0.0331\n",
            "epoch20300, loss = 0.0331\n",
            "epoch20310, loss = 0.0331\n",
            "epoch20320, loss = 0.0331\n",
            "epoch20330, loss = 0.0331\n",
            "epoch20340, loss = 0.0331\n",
            "epoch20350, loss = 0.0331\n",
            "epoch20360, loss = 0.0331\n",
            "epoch20370, loss = 0.0331\n",
            "epoch20380, loss = 0.0330\n",
            "epoch20390, loss = 0.0330\n",
            "epoch20400, loss = 0.0330\n",
            "epoch20410, loss = 0.0330\n",
            "epoch20420, loss = 0.0330\n",
            "epoch20430, loss = 0.0330\n",
            "epoch20440, loss = 0.0330\n",
            "epoch20450, loss = 0.0330\n",
            "epoch20460, loss = 0.0330\n",
            "epoch20470, loss = 0.0330\n",
            "epoch20480, loss = 0.0330\n",
            "epoch20490, loss = 0.0330\n",
            "epoch20500, loss = 0.0330\n",
            "epoch20510, loss = 0.0330\n",
            "epoch20520, loss = 0.0330\n",
            "epoch20530, loss = 0.0330\n",
            "epoch20540, loss = 0.0330\n",
            "epoch20550, loss = 0.0330\n",
            "epoch20560, loss = 0.0330\n",
            "epoch20570, loss = 0.0330\n",
            "epoch20580, loss = 0.0330\n",
            "epoch20590, loss = 0.0330\n",
            "epoch20600, loss = 0.0329\n",
            "epoch20610, loss = 0.0329\n",
            "epoch20620, loss = 0.0329\n",
            "epoch20630, loss = 0.0329\n",
            "epoch20640, loss = 0.0329\n",
            "epoch20650, loss = 0.0329\n",
            "epoch20660, loss = 0.0329\n",
            "epoch20670, loss = 0.0329\n",
            "epoch20680, loss = 0.0329\n",
            "epoch20690, loss = 0.0329\n",
            "epoch20700, loss = 0.0329\n",
            "epoch20710, loss = 0.0329\n",
            "epoch20720, loss = 0.0329\n",
            "epoch20730, loss = 0.0329\n",
            "epoch20740, loss = 0.0329\n",
            "epoch20750, loss = 0.0329\n",
            "epoch20760, loss = 0.0329\n",
            "epoch20770, loss = 0.0329\n",
            "epoch20780, loss = 0.0329\n",
            "epoch20790, loss = 0.0329\n",
            "epoch20800, loss = 0.0329\n",
            "epoch20810, loss = 0.0329\n",
            "epoch20820, loss = 0.0329\n",
            "epoch20830, loss = 0.0328\n",
            "epoch20840, loss = 0.0328\n",
            "epoch20850, loss = 0.0328\n",
            "epoch20860, loss = 0.0328\n",
            "epoch20870, loss = 0.0328\n",
            "epoch20880, loss = 0.0328\n",
            "epoch20890, loss = 0.0328\n",
            "epoch20900, loss = 0.0328\n",
            "epoch20910, loss = 0.0328\n",
            "epoch20920, loss = 0.0328\n",
            "epoch20930, loss = 0.0328\n",
            "epoch20940, loss = 0.0328\n",
            "epoch20950, loss = 0.0328\n",
            "epoch20960, loss = 0.0328\n",
            "epoch20970, loss = 0.0328\n",
            "epoch20980, loss = 0.0328\n",
            "epoch20990, loss = 0.0328\n",
            "epoch21000, loss = 0.0328\n",
            "epoch21010, loss = 0.0328\n",
            "epoch21020, loss = 0.0328\n",
            "epoch21030, loss = 0.0328\n",
            "epoch21040, loss = 0.0328\n",
            "epoch21050, loss = 0.0328\n",
            "epoch21060, loss = 0.0327\n",
            "epoch21070, loss = 0.0327\n",
            "epoch21080, loss = 0.0327\n",
            "epoch21090, loss = 0.0327\n",
            "epoch21100, loss = 0.0327\n",
            "epoch21110, loss = 0.0327\n",
            "epoch21120, loss = 0.0327\n",
            "epoch21130, loss = 0.0327\n",
            "epoch21140, loss = 0.0327\n",
            "epoch21150, loss = 0.0327\n",
            "epoch21160, loss = 0.0327\n",
            "epoch21170, loss = 0.0327\n",
            "epoch21180, loss = 0.0327\n",
            "epoch21190, loss = 0.0327\n",
            "epoch21200, loss = 0.0327\n",
            "epoch21210, loss = 0.0327\n",
            "epoch21220, loss = 0.0327\n",
            "epoch21230, loss = 0.0327\n",
            "epoch21240, loss = 0.0327\n",
            "epoch21250, loss = 0.0327\n",
            "epoch21260, loss = 0.0327\n",
            "epoch21270, loss = 0.0327\n",
            "epoch21280, loss = 0.0327\n",
            "epoch21290, loss = 0.0327\n",
            "epoch21300, loss = 0.0326\n",
            "epoch21310, loss = 0.0326\n",
            "epoch21320, loss = 0.0326\n",
            "epoch21330, loss = 0.0326\n",
            "epoch21340, loss = 0.0326\n",
            "epoch21350, loss = 0.0326\n",
            "epoch21360, loss = 0.0326\n",
            "epoch21370, loss = 0.0326\n",
            "epoch21380, loss = 0.0326\n",
            "epoch21390, loss = 0.0326\n",
            "epoch21400, loss = 0.0326\n",
            "epoch21410, loss = 0.0326\n",
            "epoch21420, loss = 0.0326\n",
            "epoch21430, loss = 0.0326\n",
            "epoch21440, loss = 0.0326\n",
            "epoch21450, loss = 0.0326\n",
            "epoch21460, loss = 0.0326\n",
            "epoch21470, loss = 0.0326\n",
            "epoch21480, loss = 0.0326\n",
            "epoch21490, loss = 0.0326\n",
            "epoch21500, loss = 0.0326\n",
            "epoch21510, loss = 0.0326\n",
            "epoch21520, loss = 0.0326\n",
            "epoch21530, loss = 0.0325\n",
            "epoch21540, loss = 0.0325\n",
            "epoch21550, loss = 0.0325\n",
            "epoch21560, loss = 0.0325\n",
            "epoch21570, loss = 0.0325\n",
            "epoch21580, loss = 0.0325\n",
            "epoch21590, loss = 0.0325\n",
            "epoch21600, loss = 0.0325\n",
            "epoch21610, loss = 0.0325\n",
            "epoch21620, loss = 0.0325\n",
            "epoch21630, loss = 0.0325\n",
            "epoch21640, loss = 0.0325\n",
            "epoch21650, loss = 0.0325\n",
            "epoch21660, loss = 0.0325\n",
            "epoch21670, loss = 0.0325\n",
            "epoch21680, loss = 0.0325\n",
            "epoch21690, loss = 0.0325\n",
            "epoch21700, loss = 0.0325\n",
            "epoch21710, loss = 0.0325\n",
            "epoch21720, loss = 0.0325\n",
            "epoch21730, loss = 0.0325\n",
            "epoch21740, loss = 0.0325\n",
            "epoch21750, loss = 0.0325\n",
            "epoch21760, loss = 0.0325\n",
            "epoch21770, loss = 0.0324\n",
            "epoch21780, loss = 0.0324\n",
            "epoch21790, loss = 0.0324\n",
            "epoch21800, loss = 0.0324\n",
            "epoch21810, loss = 0.0324\n",
            "epoch21820, loss = 0.0324\n",
            "epoch21830, loss = 0.0324\n",
            "epoch21840, loss = 0.0324\n",
            "epoch21850, loss = 0.0324\n",
            "epoch21860, loss = 0.0324\n",
            "epoch21870, loss = 0.0324\n",
            "epoch21880, loss = 0.0324\n",
            "epoch21890, loss = 0.0324\n",
            "epoch21900, loss = 0.0324\n",
            "epoch21910, loss = 0.0324\n",
            "epoch21920, loss = 0.0324\n",
            "epoch21930, loss = 0.0324\n",
            "epoch21940, loss = 0.0324\n",
            "epoch21950, loss = 0.0324\n",
            "epoch21960, loss = 0.0324\n",
            "epoch21970, loss = 0.0324\n",
            "epoch21980, loss = 0.0324\n",
            "epoch21990, loss = 0.0324\n",
            "epoch22000, loss = 0.0324\n",
            "epoch22010, loss = 0.0324\n",
            "epoch22020, loss = 0.0323\n",
            "epoch22030, loss = 0.0323\n",
            "epoch22040, loss = 0.0323\n",
            "epoch22050, loss = 0.0323\n",
            "epoch22060, loss = 0.0323\n",
            "epoch22070, loss = 0.0323\n",
            "epoch22080, loss = 0.0323\n",
            "epoch22090, loss = 0.0323\n",
            "epoch22100, loss = 0.0323\n",
            "epoch22110, loss = 0.0323\n",
            "epoch22120, loss = 0.0323\n",
            "epoch22130, loss = 0.0323\n",
            "epoch22140, loss = 0.0323\n",
            "epoch22150, loss = 0.0323\n",
            "epoch22160, loss = 0.0323\n",
            "epoch22170, loss = 0.0323\n",
            "epoch22180, loss = 0.0323\n",
            "epoch22190, loss = 0.0323\n",
            "epoch22200, loss = 0.0323\n",
            "epoch22210, loss = 0.0323\n",
            "epoch22220, loss = 0.0323\n",
            "epoch22230, loss = 0.0323\n",
            "epoch22240, loss = 0.0323\n",
            "epoch22250, loss = 0.0323\n",
            "epoch22260, loss = 0.0323\n",
            "epoch22270, loss = 0.0322\n",
            "epoch22280, loss = 0.0322\n",
            "epoch22290, loss = 0.0322\n",
            "epoch22300, loss = 0.0322\n",
            "epoch22310, loss = 0.0322\n",
            "epoch22320, loss = 0.0322\n",
            "epoch22330, loss = 0.0322\n",
            "epoch22340, loss = 0.0322\n",
            "epoch22350, loss = 0.0322\n",
            "epoch22360, loss = 0.0322\n",
            "epoch22370, loss = 0.0322\n",
            "epoch22380, loss = 0.0322\n",
            "epoch22390, loss = 0.0322\n",
            "epoch22400, loss = 0.0322\n",
            "epoch22410, loss = 0.0322\n",
            "epoch22420, loss = 0.0322\n",
            "epoch22430, loss = 0.0322\n",
            "epoch22440, loss = 0.0322\n",
            "epoch22450, loss = 0.0322\n",
            "epoch22460, loss = 0.0322\n",
            "epoch22470, loss = 0.0322\n",
            "epoch22480, loss = 0.0322\n",
            "epoch22490, loss = 0.0322\n",
            "epoch22500, loss = 0.0322\n",
            "epoch22510, loss = 0.0322\n",
            "epoch22520, loss = 0.0321\n",
            "epoch22530, loss = 0.0321\n",
            "epoch22540, loss = 0.0321\n",
            "epoch22550, loss = 0.0321\n",
            "epoch22560, loss = 0.0321\n",
            "epoch22570, loss = 0.0321\n",
            "epoch22580, loss = 0.0321\n",
            "epoch22590, loss = 0.0321\n",
            "epoch22600, loss = 0.0321\n",
            "epoch22610, loss = 0.0321\n",
            "epoch22620, loss = 0.0321\n",
            "epoch22630, loss = 0.0321\n",
            "epoch22640, loss = 0.0321\n",
            "epoch22650, loss = 0.0321\n",
            "epoch22660, loss = 0.0321\n",
            "epoch22670, loss = 0.0321\n",
            "epoch22680, loss = 0.0321\n",
            "epoch22690, loss = 0.0321\n",
            "epoch22700, loss = 0.0321\n",
            "epoch22710, loss = 0.0321\n",
            "epoch22720, loss = 0.0321\n",
            "epoch22730, loss = 0.0321\n",
            "epoch22740, loss = 0.0321\n",
            "epoch22750, loss = 0.0321\n",
            "epoch22760, loss = 0.0321\n",
            "epoch22770, loss = 0.0321\n",
            "epoch22780, loss = 0.0320\n",
            "epoch22790, loss = 0.0320\n",
            "epoch22800, loss = 0.0320\n",
            "epoch22810, loss = 0.0320\n",
            "epoch22820, loss = 0.0320\n",
            "epoch22830, loss = 0.0320\n",
            "epoch22840, loss = 0.0320\n",
            "epoch22850, loss = 0.0320\n",
            "epoch22860, loss = 0.0320\n",
            "epoch22870, loss = 0.0320\n",
            "epoch22880, loss = 0.0320\n",
            "epoch22890, loss = 0.0320\n",
            "epoch22900, loss = 0.0320\n",
            "epoch22910, loss = 0.0320\n",
            "epoch22920, loss = 0.0320\n",
            "epoch22930, loss = 0.0320\n",
            "epoch22940, loss = 0.0320\n",
            "epoch22950, loss = 0.0320\n",
            "epoch22960, loss = 0.0320\n",
            "epoch22970, loss = 0.0320\n",
            "epoch22980, loss = 0.0320\n",
            "epoch22990, loss = 0.0320\n",
            "epoch23000, loss = 0.0320\n",
            "epoch23010, loss = 0.0320\n",
            "epoch23020, loss = 0.0320\n",
            "epoch23030, loss = 0.0320\n",
            "epoch23040, loss = 0.0319\n",
            "epoch23050, loss = 0.0319\n",
            "epoch23060, loss = 0.0319\n",
            "epoch23070, loss = 0.0319\n",
            "epoch23080, loss = 0.0319\n",
            "epoch23090, loss = 0.0319\n",
            "epoch23100, loss = 0.0319\n",
            "epoch23110, loss = 0.0319\n",
            "epoch23120, loss = 0.0319\n",
            "epoch23130, loss = 0.0319\n",
            "epoch23140, loss = 0.0319\n",
            "epoch23150, loss = 0.0319\n",
            "epoch23160, loss = 0.0319\n",
            "epoch23170, loss = 0.0319\n",
            "epoch23180, loss = 0.0319\n",
            "epoch23190, loss = 0.0319\n",
            "epoch23200, loss = 0.0319\n",
            "epoch23210, loss = 0.0319\n",
            "epoch23220, loss = 0.0319\n",
            "epoch23230, loss = 0.0319\n",
            "epoch23240, loss = 0.0319\n",
            "epoch23250, loss = 0.0319\n",
            "epoch23260, loss = 0.0319\n",
            "epoch23270, loss = 0.0319\n",
            "epoch23280, loss = 0.0319\n",
            "epoch23290, loss = 0.0319\n",
            "epoch23300, loss = 0.0318\n",
            "epoch23310, loss = 0.0318\n",
            "epoch23320, loss = 0.0318\n",
            "epoch23330, loss = 0.0318\n",
            "epoch23340, loss = 0.0318\n",
            "epoch23350, loss = 0.0318\n",
            "epoch23360, loss = 0.0318\n",
            "epoch23370, loss = 0.0318\n",
            "epoch23380, loss = 0.0318\n",
            "epoch23390, loss = 0.0318\n",
            "epoch23400, loss = 0.0318\n",
            "epoch23410, loss = 0.0318\n",
            "epoch23420, loss = 0.0318\n",
            "epoch23430, loss = 0.0318\n",
            "epoch23440, loss = 0.0318\n",
            "epoch23450, loss = 0.0318\n",
            "epoch23460, loss = 0.0318\n",
            "epoch23470, loss = 0.0318\n",
            "epoch23480, loss = 0.0318\n",
            "epoch23490, loss = 0.0318\n",
            "epoch23500, loss = 0.0318\n",
            "epoch23510, loss = 0.0318\n",
            "epoch23520, loss = 0.0318\n",
            "epoch23530, loss = 0.0318\n",
            "epoch23540, loss = 0.0318\n",
            "epoch23550, loss = 0.0318\n",
            "epoch23560, loss = 0.0318\n",
            "epoch23570, loss = 0.0317\n",
            "epoch23580, loss = 0.0317\n",
            "epoch23590, loss = 0.0317\n",
            "epoch23600, loss = 0.0317\n",
            "epoch23610, loss = 0.0317\n",
            "epoch23620, loss = 0.0317\n",
            "epoch23630, loss = 0.0317\n",
            "epoch23640, loss = 0.0317\n",
            "epoch23650, loss = 0.0317\n",
            "epoch23660, loss = 0.0317\n",
            "epoch23670, loss = 0.0317\n",
            "epoch23680, loss = 0.0317\n",
            "epoch23690, loss = 0.0317\n",
            "epoch23700, loss = 0.0317\n",
            "epoch23710, loss = 0.0317\n",
            "epoch23720, loss = 0.0317\n",
            "epoch23730, loss = 0.0317\n",
            "epoch23740, loss = 0.0317\n",
            "epoch23750, loss = 0.0317\n",
            "epoch23760, loss = 0.0317\n",
            "epoch23770, loss = 0.0317\n",
            "epoch23780, loss = 0.0317\n",
            "epoch23790, loss = 0.0317\n",
            "epoch23800, loss = 0.0317\n",
            "epoch23810, loss = 0.0317\n",
            "epoch23820, loss = 0.0317\n",
            "epoch23830, loss = 0.0317\n",
            "epoch23840, loss = 0.0317\n",
            "epoch23850, loss = 0.0316\n",
            "epoch23860, loss = 0.0316\n",
            "epoch23870, loss = 0.0316\n",
            "epoch23880, loss = 0.0316\n",
            "epoch23890, loss = 0.0316\n",
            "epoch23900, loss = 0.0316\n",
            "epoch23910, loss = 0.0316\n",
            "epoch23920, loss = 0.0316\n",
            "epoch23930, loss = 0.0316\n",
            "epoch23940, loss = 0.0316\n",
            "epoch23950, loss = 0.0316\n",
            "epoch23960, loss = 0.0316\n",
            "epoch23970, loss = 0.0316\n",
            "epoch23980, loss = 0.0316\n",
            "epoch23990, loss = 0.0316\n",
            "epoch24000, loss = 0.0316\n",
            "epoch24010, loss = 0.0316\n",
            "epoch24020, loss = 0.0316\n",
            "epoch24030, loss = 0.0316\n",
            "epoch24040, loss = 0.0316\n",
            "epoch24050, loss = 0.0316\n",
            "epoch24060, loss = 0.0316\n",
            "epoch24070, loss = 0.0316\n",
            "epoch24080, loss = 0.0316\n",
            "epoch24090, loss = 0.0316\n",
            "epoch24100, loss = 0.0316\n",
            "epoch24110, loss = 0.0316\n",
            "epoch24120, loss = 0.0316\n",
            "epoch24130, loss = 0.0315\n",
            "epoch24140, loss = 0.0315\n",
            "epoch24150, loss = 0.0315\n",
            "epoch24160, loss = 0.0315\n",
            "epoch24170, loss = 0.0315\n",
            "epoch24180, loss = 0.0315\n",
            "epoch24190, loss = 0.0315\n",
            "epoch24200, loss = 0.0315\n",
            "epoch24210, loss = 0.0315\n",
            "epoch24220, loss = 0.0315\n",
            "epoch24230, loss = 0.0315\n",
            "epoch24240, loss = 0.0315\n",
            "epoch24250, loss = 0.0315\n",
            "epoch24260, loss = 0.0315\n",
            "epoch24270, loss = 0.0315\n",
            "epoch24280, loss = 0.0315\n",
            "epoch24290, loss = 0.0315\n",
            "epoch24300, loss = 0.0315\n",
            "epoch24310, loss = 0.0315\n",
            "epoch24320, loss = 0.0315\n",
            "epoch24330, loss = 0.0315\n",
            "epoch24340, loss = 0.0315\n",
            "epoch24350, loss = 0.0315\n",
            "epoch24360, loss = 0.0315\n",
            "epoch24370, loss = 0.0315\n",
            "epoch24380, loss = 0.0315\n",
            "epoch24390, loss = 0.0315\n",
            "epoch24400, loss = 0.0315\n",
            "epoch24410, loss = 0.0314\n",
            "epoch24420, loss = 0.0314\n",
            "epoch24430, loss = 0.0314\n",
            "epoch24440, loss = 0.0314\n",
            "epoch24450, loss = 0.0314\n",
            "epoch24460, loss = 0.0314\n",
            "epoch24470, loss = 0.0314\n",
            "epoch24480, loss = 0.0314\n",
            "epoch24490, loss = 0.0314\n",
            "epoch24500, loss = 0.0314\n",
            "epoch24510, loss = 0.0314\n",
            "epoch24520, loss = 0.0314\n",
            "epoch24530, loss = 0.0314\n",
            "epoch24540, loss = 0.0314\n",
            "epoch24550, loss = 0.0314\n",
            "epoch24560, loss = 0.0314\n",
            "epoch24570, loss = 0.0314\n",
            "epoch24580, loss = 0.0314\n",
            "epoch24590, loss = 0.0314\n",
            "epoch24600, loss = 0.0314\n",
            "epoch24610, loss = 0.0314\n",
            "epoch24620, loss = 0.0314\n",
            "epoch24630, loss = 0.0314\n",
            "epoch24640, loss = 0.0314\n",
            "epoch24650, loss = 0.0314\n",
            "epoch24660, loss = 0.0314\n",
            "epoch24670, loss = 0.0314\n",
            "epoch24680, loss = 0.0314\n",
            "epoch24690, loss = 0.0314\n",
            "epoch24700, loss = 0.0313\n",
            "epoch24710, loss = 0.0313\n",
            "epoch24720, loss = 0.0313\n",
            "epoch24730, loss = 0.0313\n",
            "epoch24740, loss = 0.0313\n",
            "epoch24750, loss = 0.0313\n",
            "epoch24760, loss = 0.0313\n",
            "epoch24770, loss = 0.0313\n",
            "epoch24780, loss = 0.0313\n",
            "epoch24790, loss = 0.0313\n",
            "epoch24800, loss = 0.0313\n",
            "epoch24810, loss = 0.0313\n",
            "epoch24820, loss = 0.0313\n",
            "epoch24830, loss = 0.0313\n",
            "epoch24840, loss = 0.0313\n",
            "epoch24850, loss = 0.0313\n",
            "epoch24860, loss = 0.0313\n",
            "epoch24870, loss = 0.0313\n",
            "epoch24880, loss = 0.0313\n",
            "epoch24890, loss = 0.0313\n",
            "epoch24900, loss = 0.0313\n",
            "epoch24910, loss = 0.0313\n",
            "epoch24920, loss = 0.0313\n",
            "epoch24930, loss = 0.0313\n",
            "epoch24940, loss = 0.0313\n",
            "epoch24950, loss = 0.0313\n",
            "epoch24960, loss = 0.0313\n",
            "epoch24970, loss = 0.0313\n",
            "epoch24980, loss = 0.0313\n",
            "epoch24990, loss = 0.0312\n",
            "epoch25000, loss = 0.0312\n",
            "epoch25010, loss = 0.0312\n",
            "epoch25020, loss = 0.0312\n",
            "epoch25030, loss = 0.0312\n",
            "epoch25040, loss = 0.0312\n",
            "epoch25050, loss = 0.0312\n",
            "epoch25060, loss = 0.0312\n",
            "epoch25070, loss = 0.0312\n",
            "epoch25080, loss = 0.0312\n",
            "epoch25090, loss = 0.0312\n",
            "epoch25100, loss = 0.0312\n",
            "epoch25110, loss = 0.0312\n",
            "epoch25120, loss = 0.0312\n",
            "epoch25130, loss = 0.0312\n",
            "epoch25140, loss = 0.0312\n",
            "epoch25150, loss = 0.0312\n",
            "epoch25160, loss = 0.0312\n",
            "epoch25170, loss = 0.0312\n",
            "epoch25180, loss = 0.0312\n",
            "epoch25190, loss = 0.0312\n",
            "epoch25200, loss = 0.0312\n",
            "epoch25210, loss = 0.0312\n",
            "epoch25220, loss = 0.0312\n",
            "epoch25230, loss = 0.0312\n",
            "epoch25240, loss = 0.0312\n",
            "epoch25250, loss = 0.0312\n",
            "epoch25260, loss = 0.0312\n",
            "epoch25270, loss = 0.0312\n",
            "epoch25280, loss = 0.0312\n",
            "epoch25290, loss = 0.0311\n",
            "epoch25300, loss = 0.0311\n",
            "epoch25310, loss = 0.0311\n",
            "epoch25320, loss = 0.0311\n",
            "epoch25330, loss = 0.0311\n",
            "epoch25340, loss = 0.0311\n",
            "epoch25350, loss = 0.0311\n",
            "epoch25360, loss = 0.0311\n",
            "epoch25370, loss = 0.0311\n",
            "epoch25380, loss = 0.0311\n",
            "epoch25390, loss = 0.0311\n",
            "epoch25400, loss = 0.0311\n",
            "epoch25410, loss = 0.0311\n",
            "epoch25420, loss = 0.0311\n",
            "epoch25430, loss = 0.0311\n",
            "epoch25440, loss = 0.0311\n",
            "epoch25450, loss = 0.0311\n",
            "epoch25460, loss = 0.0311\n",
            "epoch25470, loss = 0.0311\n",
            "epoch25480, loss = 0.0311\n",
            "epoch25490, loss = 0.0311\n",
            "epoch25500, loss = 0.0311\n",
            "epoch25510, loss = 0.0311\n",
            "epoch25520, loss = 0.0311\n",
            "epoch25530, loss = 0.0311\n",
            "epoch25540, loss = 0.0311\n",
            "epoch25550, loss = 0.0311\n",
            "epoch25560, loss = 0.0311\n",
            "epoch25570, loss = 0.0311\n",
            "epoch25580, loss = 0.0311\n",
            "epoch25590, loss = 0.0310\n",
            "epoch25600, loss = 0.0310\n",
            "epoch25610, loss = 0.0310\n",
            "epoch25620, loss = 0.0310\n",
            "epoch25630, loss = 0.0310\n",
            "epoch25640, loss = 0.0310\n",
            "epoch25650, loss = 0.0310\n",
            "epoch25660, loss = 0.0310\n",
            "epoch25670, loss = 0.0310\n",
            "epoch25680, loss = 0.0310\n",
            "epoch25690, loss = 0.0310\n",
            "epoch25700, loss = 0.0310\n",
            "epoch25710, loss = 0.0310\n",
            "epoch25720, loss = 0.0310\n",
            "epoch25730, loss = 0.0310\n",
            "epoch25740, loss = 0.0310\n",
            "epoch25750, loss = 0.0310\n",
            "epoch25760, loss = 0.0310\n",
            "epoch25770, loss = 0.0310\n",
            "epoch25780, loss = 0.0310\n",
            "epoch25790, loss = 0.0310\n",
            "epoch25800, loss = 0.0310\n",
            "epoch25810, loss = 0.0310\n",
            "epoch25820, loss = 0.0310\n",
            "epoch25830, loss = 0.0310\n",
            "epoch25840, loss = 0.0310\n",
            "epoch25850, loss = 0.0310\n",
            "epoch25860, loss = 0.0310\n",
            "epoch25870, loss = 0.0310\n",
            "epoch25880, loss = 0.0310\n",
            "epoch25890, loss = 0.0310\n",
            "epoch25900, loss = 0.0309\n",
            "epoch25910, loss = 0.0309\n",
            "epoch25920, loss = 0.0309\n",
            "epoch25930, loss = 0.0309\n",
            "epoch25940, loss = 0.0309\n",
            "epoch25950, loss = 0.0309\n",
            "epoch25960, loss = 0.0309\n",
            "epoch25970, loss = 0.0309\n",
            "epoch25980, loss = 0.0309\n",
            "epoch25990, loss = 0.0309\n",
            "epoch26000, loss = 0.0309\n",
            "epoch26010, loss = 0.0309\n",
            "epoch26020, loss = 0.0309\n",
            "epoch26030, loss = 0.0309\n",
            "epoch26040, loss = 0.0309\n",
            "epoch26050, loss = 0.0309\n",
            "epoch26060, loss = 0.0309\n",
            "epoch26070, loss = 0.0309\n",
            "epoch26080, loss = 0.0309\n",
            "epoch26090, loss = 0.0309\n",
            "epoch26100, loss = 0.0309\n",
            "epoch26110, loss = 0.0309\n",
            "epoch26120, loss = 0.0309\n",
            "epoch26130, loss = 0.0309\n",
            "epoch26140, loss = 0.0309\n",
            "epoch26150, loss = 0.0309\n",
            "epoch26160, loss = 0.0309\n",
            "epoch26170, loss = 0.0309\n",
            "epoch26180, loss = 0.0309\n",
            "epoch26190, loss = 0.0309\n",
            "epoch26200, loss = 0.0309\n",
            "epoch26210, loss = 0.0308\n",
            "epoch26220, loss = 0.0308\n",
            "epoch26230, loss = 0.0308\n",
            "epoch26240, loss = 0.0308\n",
            "epoch26250, loss = 0.0308\n",
            "epoch26260, loss = 0.0308\n",
            "epoch26270, loss = 0.0308\n",
            "epoch26280, loss = 0.0308\n",
            "epoch26290, loss = 0.0308\n",
            "epoch26300, loss = 0.0308\n",
            "epoch26310, loss = 0.0308\n",
            "epoch26320, loss = 0.0308\n",
            "epoch26330, loss = 0.0308\n",
            "epoch26340, loss = 0.0308\n",
            "epoch26350, loss = 0.0308\n",
            "epoch26360, loss = 0.0308\n",
            "epoch26370, loss = 0.0308\n",
            "epoch26380, loss = 0.0308\n",
            "epoch26390, loss = 0.0308\n",
            "epoch26400, loss = 0.0308\n",
            "epoch26410, loss = 0.0308\n",
            "epoch26420, loss = 0.0308\n",
            "epoch26430, loss = 0.0308\n",
            "epoch26440, loss = 0.0308\n",
            "epoch26450, loss = 0.0308\n",
            "epoch26460, loss = 0.0308\n",
            "epoch26470, loss = 0.0308\n",
            "epoch26480, loss = 0.0308\n",
            "epoch26490, loss = 0.0308\n",
            "epoch26500, loss = 0.0308\n",
            "epoch26510, loss = 0.0308\n",
            "epoch26520, loss = 0.0308\n",
            "epoch26530, loss = 0.0307\n",
            "epoch26540, loss = 0.0307\n",
            "epoch26550, loss = 0.0307\n",
            "epoch26560, loss = 0.0307\n",
            "epoch26570, loss = 0.0307\n",
            "epoch26580, loss = 0.0307\n",
            "epoch26590, loss = 0.0307\n",
            "epoch26600, loss = 0.0307\n",
            "epoch26610, loss = 0.0307\n",
            "epoch26620, loss = 0.0307\n",
            "epoch26630, loss = 0.0307\n",
            "epoch26640, loss = 0.0307\n",
            "epoch26650, loss = 0.0307\n",
            "epoch26660, loss = 0.0307\n",
            "epoch26670, loss = 0.0307\n",
            "epoch26680, loss = 0.0307\n",
            "epoch26690, loss = 0.0307\n",
            "epoch26700, loss = 0.0307\n",
            "epoch26710, loss = 0.0307\n",
            "epoch26720, loss = 0.0307\n",
            "epoch26730, loss = 0.0307\n",
            "epoch26740, loss = 0.0307\n",
            "epoch26750, loss = 0.0307\n",
            "epoch26760, loss = 0.0307\n",
            "epoch26770, loss = 0.0307\n",
            "epoch26780, loss = 0.0307\n",
            "epoch26790, loss = 0.0307\n",
            "epoch26800, loss = 0.0307\n",
            "epoch26810, loss = 0.0307\n",
            "epoch26820, loss = 0.0307\n",
            "epoch26830, loss = 0.0307\n",
            "epoch26840, loss = 0.0307\n",
            "epoch26850, loss = 0.0307\n",
            "epoch26860, loss = 0.0306\n",
            "epoch26870, loss = 0.0306\n",
            "epoch26880, loss = 0.0306\n",
            "epoch26890, loss = 0.0306\n",
            "epoch26900, loss = 0.0306\n",
            "epoch26910, loss = 0.0306\n",
            "epoch26920, loss = 0.0306\n",
            "epoch26930, loss = 0.0306\n",
            "epoch26940, loss = 0.0306\n",
            "epoch26950, loss = 0.0306\n",
            "epoch26960, loss = 0.0306\n",
            "epoch26970, loss = 0.0306\n",
            "epoch26980, loss = 0.0306\n",
            "epoch26990, loss = 0.0306\n",
            "epoch27000, loss = 0.0306\n",
            "epoch27010, loss = 0.0306\n",
            "epoch27020, loss = 0.0306\n",
            "epoch27030, loss = 0.0306\n",
            "epoch27040, loss = 0.0306\n",
            "epoch27050, loss = 0.0306\n",
            "epoch27060, loss = 0.0306\n",
            "epoch27070, loss = 0.0306\n",
            "epoch27080, loss = 0.0306\n",
            "epoch27090, loss = 0.0306\n",
            "epoch27100, loss = 0.0306\n",
            "epoch27110, loss = 0.0306\n",
            "epoch27120, loss = 0.0306\n",
            "epoch27130, loss = 0.0306\n",
            "epoch27140, loss = 0.0306\n",
            "epoch27150, loss = 0.0306\n",
            "epoch27160, loss = 0.0306\n",
            "epoch27170, loss = 0.0306\n",
            "epoch27180, loss = 0.0306\n",
            "epoch27190, loss = 0.0305\n",
            "epoch27200, loss = 0.0305\n",
            "epoch27210, loss = 0.0305\n",
            "epoch27220, loss = 0.0305\n",
            "epoch27230, loss = 0.0305\n",
            "epoch27240, loss = 0.0305\n",
            "epoch27250, loss = 0.0305\n",
            "epoch27260, loss = 0.0305\n",
            "epoch27270, loss = 0.0305\n",
            "epoch27280, loss = 0.0305\n",
            "epoch27290, loss = 0.0305\n",
            "epoch27300, loss = 0.0305\n",
            "epoch27310, loss = 0.0305\n",
            "epoch27320, loss = 0.0305\n",
            "epoch27330, loss = 0.0305\n",
            "epoch27340, loss = 0.0305\n",
            "epoch27350, loss = 0.0305\n",
            "epoch27360, loss = 0.0305\n",
            "epoch27370, loss = 0.0305\n",
            "epoch27380, loss = 0.0305\n",
            "epoch27390, loss = 0.0305\n",
            "epoch27400, loss = 0.0305\n",
            "epoch27410, loss = 0.0305\n",
            "epoch27420, loss = 0.0305\n",
            "epoch27430, loss = 0.0305\n",
            "epoch27440, loss = 0.0305\n",
            "epoch27450, loss = 0.0305\n",
            "epoch27460, loss = 0.0305\n",
            "epoch27470, loss = 0.0305\n",
            "epoch27480, loss = 0.0305\n",
            "epoch27490, loss = 0.0305\n",
            "epoch27500, loss = 0.0305\n",
            "epoch27510, loss = 0.0305\n",
            "epoch27520, loss = 0.0304\n",
            "epoch27530, loss = 0.0304\n",
            "epoch27540, loss = 0.0304\n",
            "epoch27550, loss = 0.0304\n",
            "epoch27560, loss = 0.0304\n",
            "epoch27570, loss = 0.0304\n",
            "epoch27580, loss = 0.0304\n",
            "epoch27590, loss = 0.0304\n",
            "epoch27600, loss = 0.0304\n",
            "epoch27610, loss = 0.0304\n",
            "epoch27620, loss = 0.0304\n",
            "epoch27630, loss = 0.0304\n",
            "epoch27640, loss = 0.0304\n",
            "epoch27650, loss = 0.0304\n",
            "epoch27660, loss = 0.0304\n",
            "epoch27670, loss = 0.0304\n",
            "epoch27680, loss = 0.0304\n",
            "epoch27690, loss = 0.0304\n",
            "epoch27700, loss = 0.0304\n",
            "epoch27710, loss = 0.0304\n",
            "epoch27720, loss = 0.0304\n",
            "epoch27730, loss = 0.0304\n",
            "epoch27740, loss = 0.0304\n",
            "epoch27750, loss = 0.0304\n",
            "epoch27760, loss = 0.0304\n",
            "epoch27770, loss = 0.0304\n",
            "epoch27780, loss = 0.0304\n",
            "epoch27790, loss = 0.0304\n",
            "epoch27800, loss = 0.0304\n",
            "epoch27810, loss = 0.0304\n",
            "epoch27820, loss = 0.0304\n",
            "epoch27830, loss = 0.0304\n",
            "epoch27840, loss = 0.0304\n",
            "epoch27850, loss = 0.0304\n",
            "epoch27860, loss = 0.0304\n",
            "epoch27870, loss = 0.0303\n",
            "epoch27880, loss = 0.0303\n",
            "epoch27890, loss = 0.0303\n",
            "epoch27900, loss = 0.0303\n",
            "epoch27910, loss = 0.0303\n",
            "epoch27920, loss = 0.0303\n",
            "epoch27930, loss = 0.0303\n",
            "epoch27940, loss = 0.0303\n",
            "epoch27950, loss = 0.0303\n",
            "epoch27960, loss = 0.0303\n",
            "epoch27970, loss = 0.0303\n",
            "epoch27980, loss = 0.0303\n",
            "epoch27990, loss = 0.0303\n",
            "epoch28000, loss = 0.0303\n",
            "epoch28010, loss = 0.0303\n",
            "epoch28020, loss = 0.0303\n",
            "epoch28030, loss = 0.0303\n",
            "epoch28040, loss = 0.0303\n",
            "epoch28050, loss = 0.0303\n",
            "epoch28060, loss = 0.0303\n",
            "epoch28070, loss = 0.0303\n",
            "epoch28080, loss = 0.0303\n",
            "epoch28090, loss = 0.0303\n",
            "epoch28100, loss = 0.0303\n",
            "epoch28110, loss = 0.0303\n",
            "epoch28120, loss = 0.0303\n",
            "epoch28130, loss = 0.0303\n",
            "epoch28140, loss = 0.0303\n",
            "epoch28150, loss = 0.0303\n",
            "epoch28160, loss = 0.0303\n",
            "epoch28170, loss = 0.0303\n",
            "epoch28180, loss = 0.0303\n",
            "epoch28190, loss = 0.0303\n",
            "epoch28200, loss = 0.0303\n",
            "epoch28210, loss = 0.0302\n",
            "epoch28220, loss = 0.0302\n",
            "epoch28230, loss = 0.0302\n",
            "epoch28240, loss = 0.0302\n",
            "epoch28250, loss = 0.0302\n",
            "epoch28260, loss = 0.0302\n",
            "epoch28270, loss = 0.0302\n",
            "epoch28280, loss = 0.0302\n",
            "epoch28290, loss = 0.0302\n",
            "epoch28300, loss = 0.0302\n",
            "epoch28310, loss = 0.0302\n",
            "epoch28320, loss = 0.0302\n",
            "epoch28330, loss = 0.0302\n",
            "epoch28340, loss = 0.0302\n",
            "epoch28350, loss = 0.0302\n",
            "epoch28360, loss = 0.0302\n",
            "epoch28370, loss = 0.0302\n",
            "epoch28380, loss = 0.0302\n",
            "epoch28390, loss = 0.0302\n",
            "epoch28400, loss = 0.0302\n",
            "epoch28410, loss = 0.0302\n",
            "epoch28420, loss = 0.0302\n",
            "epoch28430, loss = 0.0302\n",
            "epoch28440, loss = 0.0302\n",
            "epoch28450, loss = 0.0302\n",
            "epoch28460, loss = 0.0302\n",
            "epoch28470, loss = 0.0302\n",
            "epoch28480, loss = 0.0302\n",
            "epoch28490, loss = 0.0302\n",
            "epoch28500, loss = 0.0302\n",
            "epoch28510, loss = 0.0302\n",
            "epoch28520, loss = 0.0302\n",
            "epoch28530, loss = 0.0302\n",
            "epoch28540, loss = 0.0302\n",
            "epoch28550, loss = 0.0302\n",
            "epoch28560, loss = 0.0302\n",
            "epoch28570, loss = 0.0301\n",
            "epoch28580, loss = 0.0301\n",
            "epoch28590, loss = 0.0301\n",
            "epoch28600, loss = 0.0301\n",
            "epoch28610, loss = 0.0301\n",
            "epoch28620, loss = 0.0301\n",
            "epoch28630, loss = 0.0301\n",
            "epoch28640, loss = 0.0301\n",
            "epoch28650, loss = 0.0301\n",
            "epoch28660, loss = 0.0301\n",
            "epoch28670, loss = 0.0301\n",
            "epoch28680, loss = 0.0301\n",
            "epoch28690, loss = 0.0301\n",
            "epoch28700, loss = 0.0301\n",
            "epoch28710, loss = 0.0301\n",
            "epoch28720, loss = 0.0301\n",
            "epoch28730, loss = 0.0301\n",
            "epoch28740, loss = 0.0301\n",
            "epoch28750, loss = 0.0301\n",
            "epoch28760, loss = 0.0301\n",
            "epoch28770, loss = 0.0301\n",
            "epoch28780, loss = 0.0301\n",
            "epoch28790, loss = 0.0301\n",
            "epoch28800, loss = 0.0301\n",
            "epoch28810, loss = 0.0301\n",
            "epoch28820, loss = 0.0301\n",
            "epoch28830, loss = 0.0301\n",
            "epoch28840, loss = 0.0301\n",
            "epoch28850, loss = 0.0301\n",
            "epoch28860, loss = 0.0301\n",
            "epoch28870, loss = 0.0301\n",
            "epoch28880, loss = 0.0301\n",
            "epoch28890, loss = 0.0301\n",
            "epoch28900, loss = 0.0301\n",
            "epoch28910, loss = 0.0301\n",
            "epoch28920, loss = 0.0301\n",
            "epoch28930, loss = 0.0300\n",
            "epoch28940, loss = 0.0300\n",
            "epoch28950, loss = 0.0300\n",
            "epoch28960, loss = 0.0300\n",
            "epoch28970, loss = 0.0300\n",
            "epoch28980, loss = 0.0300\n",
            "epoch28990, loss = 0.0300\n",
            "epoch29000, loss = 0.0300\n",
            "epoch29010, loss = 0.0300\n",
            "epoch29020, loss = 0.0300\n",
            "epoch29030, loss = 0.0300\n",
            "epoch29040, loss = 0.0300\n",
            "epoch29050, loss = 0.0300\n",
            "epoch29060, loss = 0.0300\n",
            "epoch29070, loss = 0.0300\n",
            "epoch29080, loss = 0.0300\n",
            "epoch29090, loss = 0.0300\n",
            "epoch29100, loss = 0.0300\n",
            "epoch29110, loss = 0.0300\n",
            "epoch29120, loss = 0.0300\n",
            "epoch29130, loss = 0.0300\n",
            "epoch29140, loss = 0.0300\n",
            "epoch29150, loss = 0.0300\n",
            "epoch29160, loss = 0.0300\n",
            "epoch29170, loss = 0.0300\n",
            "epoch29180, loss = 0.0300\n",
            "epoch29190, loss = 0.0300\n",
            "epoch29200, loss = 0.0300\n",
            "epoch29210, loss = 0.0300\n",
            "epoch29220, loss = 0.0300\n",
            "epoch29230, loss = 0.0300\n",
            "epoch29240, loss = 0.0300\n",
            "epoch29250, loss = 0.0300\n",
            "epoch29260, loss = 0.0300\n",
            "epoch29270, loss = 0.0300\n",
            "epoch29280, loss = 0.0300\n",
            "epoch29290, loss = 0.0300\n",
            "epoch29300, loss = 0.0299\n",
            "epoch29310, loss = 0.0299\n",
            "epoch29320, loss = 0.0299\n",
            "epoch29330, loss = 0.0299\n",
            "epoch29340, loss = 0.0299\n",
            "epoch29350, loss = 0.0299\n",
            "epoch29360, loss = 0.0299\n",
            "epoch29370, loss = 0.0299\n",
            "epoch29380, loss = 0.0299\n",
            "epoch29390, loss = 0.0299\n",
            "epoch29400, loss = 0.0299\n",
            "epoch29410, loss = 0.0299\n",
            "epoch29420, loss = 0.0299\n",
            "epoch29430, loss = 0.0299\n",
            "epoch29440, loss = 0.0299\n",
            "epoch29450, loss = 0.0299\n",
            "epoch29460, loss = 0.0299\n",
            "epoch29470, loss = 0.0299\n",
            "epoch29480, loss = 0.0299\n",
            "epoch29490, loss = 0.0299\n",
            "epoch29500, loss = 0.0299\n",
            "epoch29510, loss = 0.0299\n",
            "epoch29520, loss = 0.0299\n",
            "epoch29530, loss = 0.0299\n",
            "epoch29540, loss = 0.0299\n",
            "epoch29550, loss = 0.0299\n",
            "epoch29560, loss = 0.0299\n",
            "epoch29570, loss = 0.0299\n",
            "epoch29580, loss = 0.0299\n",
            "epoch29590, loss = 0.0299\n",
            "epoch29600, loss = 0.0299\n",
            "epoch29610, loss = 0.0299\n",
            "epoch29620, loss = 0.0299\n",
            "epoch29630, loss = 0.0299\n",
            "epoch29640, loss = 0.0299\n",
            "epoch29650, loss = 0.0299\n",
            "epoch29660, loss = 0.0299\n",
            "epoch29670, loss = 0.0298\n",
            "epoch29680, loss = 0.0298\n",
            "epoch29690, loss = 0.0298\n",
            "epoch29700, loss = 0.0298\n",
            "epoch29710, loss = 0.0298\n",
            "epoch29720, loss = 0.0298\n",
            "epoch29730, loss = 0.0298\n",
            "epoch29740, loss = 0.0298\n",
            "epoch29750, loss = 0.0298\n",
            "epoch29760, loss = 0.0298\n",
            "epoch29770, loss = 0.0298\n",
            "epoch29780, loss = 0.0298\n",
            "epoch29790, loss = 0.0298\n",
            "epoch29800, loss = 0.0298\n",
            "epoch29810, loss = 0.0298\n",
            "epoch29820, loss = 0.0298\n",
            "epoch29830, loss = 0.0298\n",
            "epoch29840, loss = 0.0298\n",
            "epoch29850, loss = 0.0298\n",
            "epoch29860, loss = 0.0298\n",
            "epoch29870, loss = 0.0298\n",
            "epoch29880, loss = 0.0298\n",
            "epoch29890, loss = 0.0298\n",
            "epoch29900, loss = 0.0298\n",
            "epoch29910, loss = 0.0298\n",
            "epoch29920, loss = 0.0298\n",
            "epoch29930, loss = 0.0298\n",
            "epoch29940, loss = 0.0298\n",
            "epoch29950, loss = 0.0298\n",
            "epoch29960, loss = 0.0298\n",
            "epoch29970, loss = 0.0298\n",
            "epoch29980, loss = 0.0298\n",
            "epoch29990, loss = 0.0298\n",
            "epoch30000, loss = 0.0298\n",
            "epoch30010, loss = 0.0298\n",
            "epoch30020, loss = 0.0298\n",
            "epoch30030, loss = 0.0298\n",
            "epoch30040, loss = 0.0298\n",
            "epoch30050, loss = 0.0297\n",
            "epoch30060, loss = 0.0297\n",
            "epoch30070, loss = 0.0297\n",
            "epoch30080, loss = 0.0297\n",
            "epoch30090, loss = 0.0297\n",
            "epoch30100, loss = 0.0297\n",
            "epoch30110, loss = 0.0297\n",
            "epoch30120, loss = 0.0297\n",
            "epoch30130, loss = 0.0297\n",
            "epoch30140, loss = 0.0297\n",
            "epoch30150, loss = 0.0297\n",
            "epoch30160, loss = 0.0297\n",
            "epoch30170, loss = 0.0297\n",
            "epoch30180, loss = 0.0297\n",
            "epoch30190, loss = 0.0297\n",
            "epoch30200, loss = 0.0297\n",
            "epoch30210, loss = 0.0297\n",
            "epoch30220, loss = 0.0297\n",
            "epoch30230, loss = 0.0297\n",
            "epoch30240, loss = 0.0297\n",
            "epoch30250, loss = 0.0297\n",
            "epoch30260, loss = 0.0297\n",
            "epoch30270, loss = 0.0297\n",
            "epoch30280, loss = 0.0297\n",
            "epoch30290, loss = 0.0297\n",
            "epoch30300, loss = 0.0297\n",
            "epoch30310, loss = 0.0297\n",
            "epoch30320, loss = 0.0297\n",
            "epoch30330, loss = 0.0297\n",
            "epoch30340, loss = 0.0297\n",
            "epoch30350, loss = 0.0297\n",
            "epoch30360, loss = 0.0297\n",
            "epoch30370, loss = 0.0297\n",
            "epoch30380, loss = 0.0297\n",
            "epoch30390, loss = 0.0297\n",
            "epoch30400, loss = 0.0297\n",
            "epoch30410, loss = 0.0297\n",
            "epoch30420, loss = 0.0297\n",
            "epoch30430, loss = 0.0297\n",
            "epoch30440, loss = 0.0296\n",
            "epoch30450, loss = 0.0296\n",
            "epoch30460, loss = 0.0296\n",
            "epoch30470, loss = 0.0296\n",
            "epoch30480, loss = 0.0296\n",
            "epoch30490, loss = 0.0296\n",
            "epoch30500, loss = 0.0296\n",
            "epoch30510, loss = 0.0296\n",
            "epoch30520, loss = 0.0296\n",
            "epoch30530, loss = 0.0296\n",
            "epoch30540, loss = 0.0296\n",
            "epoch30550, loss = 0.0296\n",
            "epoch30560, loss = 0.0296\n",
            "epoch30570, loss = 0.0296\n",
            "epoch30580, loss = 0.0296\n",
            "epoch30590, loss = 0.0296\n",
            "epoch30600, loss = 0.0296\n",
            "epoch30610, loss = 0.0296\n",
            "epoch30620, loss = 0.0296\n",
            "epoch30630, loss = 0.0296\n",
            "epoch30640, loss = 0.0296\n",
            "epoch30650, loss = 0.0296\n",
            "epoch30660, loss = 0.0296\n",
            "epoch30670, loss = 0.0296\n",
            "epoch30680, loss = 0.0296\n",
            "epoch30690, loss = 0.0296\n",
            "epoch30700, loss = 0.0296\n",
            "epoch30710, loss = 0.0296\n",
            "epoch30720, loss = 0.0296\n",
            "epoch30730, loss = 0.0296\n",
            "epoch30740, loss = 0.0296\n",
            "epoch30750, loss = 0.0296\n",
            "epoch30760, loss = 0.0296\n",
            "epoch30770, loss = 0.0296\n",
            "epoch30780, loss = 0.0296\n",
            "epoch30790, loss = 0.0296\n",
            "epoch30800, loss = 0.0296\n",
            "epoch30810, loss = 0.0296\n",
            "epoch30820, loss = 0.0296\n",
            "epoch30830, loss = 0.0296\n",
            "epoch30840, loss = 0.0295\n",
            "epoch30850, loss = 0.0295\n",
            "epoch30860, loss = 0.0295\n",
            "epoch30870, loss = 0.0295\n",
            "epoch30880, loss = 0.0295\n",
            "epoch30890, loss = 0.0295\n",
            "epoch30900, loss = 0.0295\n",
            "epoch30910, loss = 0.0295\n",
            "epoch30920, loss = 0.0295\n",
            "epoch30930, loss = 0.0295\n",
            "epoch30940, loss = 0.0295\n",
            "epoch30950, loss = 0.0295\n",
            "epoch30960, loss = 0.0295\n",
            "epoch30970, loss = 0.0295\n",
            "epoch30980, loss = 0.0295\n",
            "epoch30990, loss = 0.0295\n",
            "epoch31000, loss = 0.0295\n",
            "epoch31010, loss = 0.0295\n",
            "epoch31020, loss = 0.0295\n",
            "epoch31030, loss = 0.0295\n",
            "epoch31040, loss = 0.0295\n",
            "epoch31050, loss = 0.0295\n",
            "epoch31060, loss = 0.0295\n",
            "epoch31070, loss = 0.0295\n",
            "epoch31080, loss = 0.0295\n",
            "epoch31090, loss = 0.0295\n",
            "epoch31100, loss = 0.0295\n",
            "epoch31110, loss = 0.0295\n",
            "epoch31120, loss = 0.0295\n",
            "epoch31130, loss = 0.0295\n",
            "epoch31140, loss = 0.0295\n",
            "epoch31150, loss = 0.0295\n",
            "epoch31160, loss = 0.0295\n",
            "epoch31170, loss = 0.0295\n",
            "epoch31180, loss = 0.0295\n",
            "epoch31190, loss = 0.0295\n",
            "epoch31200, loss = 0.0295\n",
            "epoch31210, loss = 0.0295\n",
            "epoch31220, loss = 0.0295\n",
            "epoch31230, loss = 0.0295\n",
            "epoch31240, loss = 0.0294\n",
            "epoch31250, loss = 0.0294\n",
            "epoch31260, loss = 0.0294\n",
            "epoch31270, loss = 0.0294\n",
            "epoch31280, loss = 0.0294\n",
            "epoch31290, loss = 0.0294\n",
            "epoch31300, loss = 0.0294\n",
            "epoch31310, loss = 0.0294\n",
            "epoch31320, loss = 0.0294\n",
            "epoch31330, loss = 0.0294\n",
            "epoch31340, loss = 0.0294\n",
            "epoch31350, loss = 0.0294\n",
            "epoch31360, loss = 0.0294\n",
            "epoch31370, loss = 0.0294\n",
            "epoch31380, loss = 0.0294\n",
            "epoch31390, loss = 0.0294\n",
            "epoch31400, loss = 0.0294\n",
            "epoch31410, loss = 0.0294\n",
            "epoch31420, loss = 0.0294\n",
            "epoch31430, loss = 0.0294\n",
            "epoch31440, loss = 0.0294\n",
            "epoch31450, loss = 0.0294\n",
            "epoch31460, loss = 0.0294\n",
            "epoch31470, loss = 0.0294\n",
            "epoch31480, loss = 0.0294\n",
            "epoch31490, loss = 0.0294\n",
            "epoch31500, loss = 0.0294\n",
            "epoch31510, loss = 0.0294\n",
            "epoch31520, loss = 0.0294\n",
            "epoch31530, loss = 0.0294\n",
            "epoch31540, loss = 0.0294\n",
            "epoch31550, loss = 0.0294\n",
            "epoch31560, loss = 0.0294\n",
            "epoch31570, loss = 0.0294\n",
            "epoch31580, loss = 0.0294\n",
            "epoch31590, loss = 0.0294\n",
            "epoch31600, loss = 0.0294\n",
            "epoch31610, loss = 0.0294\n",
            "epoch31620, loss = 0.0294\n",
            "epoch31630, loss = 0.0294\n",
            "epoch31640, loss = 0.0294\n",
            "epoch31650, loss = 0.0293\n",
            "epoch31660, loss = 0.0293\n",
            "epoch31670, loss = 0.0293\n",
            "epoch31680, loss = 0.0293\n",
            "epoch31690, loss = 0.0293\n",
            "epoch31700, loss = 0.0293\n",
            "epoch31710, loss = 0.0293\n",
            "epoch31720, loss = 0.0293\n",
            "epoch31730, loss = 0.0293\n",
            "epoch31740, loss = 0.0293\n",
            "epoch31750, loss = 0.0293\n",
            "epoch31760, loss = 0.0293\n",
            "epoch31770, loss = 0.0293\n",
            "epoch31780, loss = 0.0293\n",
            "epoch31790, loss = 0.0293\n",
            "epoch31800, loss = 0.0293\n",
            "epoch31810, loss = 0.0293\n",
            "epoch31820, loss = 0.0293\n",
            "epoch31830, loss = 0.0293\n",
            "epoch31840, loss = 0.0293\n",
            "epoch31850, loss = 0.0293\n",
            "epoch31860, loss = 0.0293\n",
            "epoch31870, loss = 0.0293\n",
            "epoch31880, loss = 0.0293\n",
            "epoch31890, loss = 0.0293\n",
            "epoch31900, loss = 0.0293\n",
            "epoch31910, loss = 0.0293\n",
            "epoch31920, loss = 0.0293\n",
            "epoch31930, loss = 0.0293\n",
            "epoch31940, loss = 0.0293\n",
            "epoch31950, loss = 0.0293\n",
            "epoch31960, loss = 0.0293\n",
            "epoch31970, loss = 0.0293\n",
            "epoch31980, loss = 0.0293\n",
            "epoch31990, loss = 0.0293\n",
            "epoch32000, loss = 0.0293\n",
            "epoch32010, loss = 0.0293\n",
            "epoch32020, loss = 0.0293\n",
            "epoch32030, loss = 0.0293\n",
            "epoch32040, loss = 0.0293\n",
            "epoch32050, loss = 0.0293\n",
            "epoch32060, loss = 0.0293\n",
            "epoch32070, loss = 0.0292\n",
            "epoch32080, loss = 0.0292\n",
            "epoch32090, loss = 0.0292\n",
            "epoch32100, loss = 0.0292\n",
            "epoch32110, loss = 0.0292\n",
            "epoch32120, loss = 0.0292\n",
            "epoch32130, loss = 0.0292\n",
            "epoch32140, loss = 0.0292\n",
            "epoch32150, loss = 0.0292\n",
            "epoch32160, loss = 0.0292\n",
            "epoch32170, loss = 0.0292\n",
            "epoch32180, loss = 0.0292\n",
            "epoch32190, loss = 0.0292\n",
            "epoch32200, loss = 0.0292\n",
            "epoch32210, loss = 0.0292\n",
            "epoch32220, loss = 0.0292\n",
            "epoch32230, loss = 0.0292\n",
            "epoch32240, loss = 0.0292\n",
            "epoch32250, loss = 0.0292\n",
            "epoch32260, loss = 0.0292\n",
            "epoch32270, loss = 0.0292\n",
            "epoch32280, loss = 0.0292\n",
            "epoch32290, loss = 0.0292\n",
            "epoch32300, loss = 0.0292\n",
            "epoch32310, loss = 0.0292\n",
            "epoch32320, loss = 0.0292\n",
            "epoch32330, loss = 0.0292\n",
            "epoch32340, loss = 0.0292\n",
            "epoch32350, loss = 0.0292\n",
            "epoch32360, loss = 0.0292\n",
            "epoch32370, loss = 0.0292\n",
            "epoch32380, loss = 0.0292\n",
            "epoch32390, loss = 0.0292\n",
            "epoch32400, loss = 0.0292\n",
            "epoch32410, loss = 0.0292\n",
            "epoch32420, loss = 0.0292\n",
            "epoch32430, loss = 0.0292\n",
            "epoch32440, loss = 0.0292\n",
            "epoch32450, loss = 0.0292\n",
            "epoch32460, loss = 0.0292\n",
            "epoch32470, loss = 0.0292\n",
            "epoch32480, loss = 0.0292\n",
            "epoch32490, loss = 0.0291\n",
            "epoch32500, loss = 0.0291\n",
            "epoch32510, loss = 0.0291\n",
            "epoch32520, loss = 0.0291\n",
            "epoch32530, loss = 0.0291\n",
            "epoch32540, loss = 0.0291\n",
            "epoch32550, loss = 0.0291\n",
            "epoch32560, loss = 0.0291\n",
            "epoch32570, loss = 0.0291\n",
            "epoch32580, loss = 0.0291\n",
            "epoch32590, loss = 0.0291\n",
            "epoch32600, loss = 0.0291\n",
            "epoch32610, loss = 0.0291\n",
            "epoch32620, loss = 0.0291\n",
            "epoch32630, loss = 0.0291\n",
            "epoch32640, loss = 0.0291\n",
            "epoch32650, loss = 0.0291\n",
            "epoch32660, loss = 0.0291\n",
            "epoch32670, loss = 0.0291\n",
            "epoch32680, loss = 0.0291\n",
            "epoch32690, loss = 0.0291\n",
            "epoch32700, loss = 0.0291\n",
            "epoch32710, loss = 0.0291\n",
            "epoch32720, loss = 0.0291\n",
            "epoch32730, loss = 0.0291\n",
            "epoch32740, loss = 0.0291\n",
            "epoch32750, loss = 0.0291\n",
            "epoch32760, loss = 0.0291\n",
            "epoch32770, loss = 0.0291\n",
            "epoch32780, loss = 0.0291\n",
            "epoch32790, loss = 0.0291\n",
            "epoch32800, loss = 0.0291\n",
            "epoch32810, loss = 0.0291\n",
            "epoch32820, loss = 0.0291\n",
            "epoch32830, loss = 0.0291\n",
            "epoch32840, loss = 0.0291\n",
            "epoch32850, loss = 0.0291\n",
            "epoch32860, loss = 0.0291\n",
            "epoch32870, loss = 0.0291\n",
            "epoch32880, loss = 0.0291\n",
            "epoch32890, loss = 0.0291\n",
            "epoch32900, loss = 0.0291\n",
            "epoch32910, loss = 0.0291\n",
            "epoch32920, loss = 0.0290\n",
            "epoch32930, loss = 0.0290\n",
            "epoch32940, loss = 0.0290\n",
            "epoch32950, loss = 0.0290\n",
            "epoch32960, loss = 0.0290\n",
            "epoch32970, loss = 0.0290\n",
            "epoch32980, loss = 0.0290\n",
            "epoch32990, loss = 0.0290\n",
            "epoch33000, loss = 0.0290\n",
            "epoch33010, loss = 0.0290\n",
            "epoch33020, loss = 0.0290\n",
            "epoch33030, loss = 0.0290\n",
            "epoch33040, loss = 0.0290\n",
            "epoch33050, loss = 0.0290\n",
            "epoch33060, loss = 0.0290\n",
            "epoch33070, loss = 0.0290\n",
            "epoch33080, loss = 0.0290\n",
            "epoch33090, loss = 0.0290\n",
            "epoch33100, loss = 0.0290\n",
            "epoch33110, loss = 0.0290\n",
            "epoch33120, loss = 0.0290\n",
            "epoch33130, loss = 0.0290\n",
            "epoch33140, loss = 0.0290\n",
            "epoch33150, loss = 0.0290\n",
            "epoch33160, loss = 0.0290\n",
            "epoch33170, loss = 0.0290\n",
            "epoch33180, loss = 0.0290\n",
            "epoch33190, loss = 0.0290\n",
            "epoch33200, loss = 0.0290\n",
            "epoch33210, loss = 0.0290\n",
            "epoch33220, loss = 0.0290\n",
            "epoch33230, loss = 0.0290\n",
            "epoch33240, loss = 0.0290\n",
            "epoch33250, loss = 0.0290\n",
            "epoch33260, loss = 0.0290\n",
            "epoch33270, loss = 0.0290\n",
            "epoch33280, loss = 0.0290\n",
            "epoch33290, loss = 0.0290\n",
            "epoch33300, loss = 0.0290\n",
            "epoch33310, loss = 0.0290\n",
            "epoch33320, loss = 0.0290\n",
            "epoch33330, loss = 0.0290\n",
            "epoch33340, loss = 0.0290\n",
            "epoch33350, loss = 0.0290\n",
            "epoch33360, loss = 0.0290\n",
            "epoch33370, loss = 0.0289\n",
            "epoch33380, loss = 0.0289\n",
            "epoch33390, loss = 0.0289\n",
            "epoch33400, loss = 0.0289\n",
            "epoch33410, loss = 0.0289\n",
            "epoch33420, loss = 0.0289\n",
            "epoch33430, loss = 0.0289\n",
            "epoch33440, loss = 0.0289\n",
            "epoch33450, loss = 0.0289\n",
            "epoch33460, loss = 0.0289\n",
            "epoch33470, loss = 0.0289\n",
            "epoch33480, loss = 0.0289\n",
            "epoch33490, loss = 0.0289\n",
            "epoch33500, loss = 0.0289\n",
            "epoch33510, loss = 0.0289\n",
            "epoch33520, loss = 0.0289\n",
            "epoch33530, loss = 0.0289\n",
            "epoch33540, loss = 0.0289\n",
            "epoch33550, loss = 0.0289\n",
            "epoch33560, loss = 0.0289\n",
            "epoch33570, loss = 0.0289\n",
            "epoch33580, loss = 0.0289\n",
            "epoch33590, loss = 0.0289\n",
            "epoch33600, loss = 0.0289\n",
            "epoch33610, loss = 0.0289\n",
            "epoch33620, loss = 0.0289\n",
            "epoch33630, loss = 0.0289\n",
            "epoch33640, loss = 0.0289\n",
            "epoch33650, loss = 0.0289\n",
            "epoch33660, loss = 0.0289\n",
            "epoch33670, loss = 0.0289\n",
            "epoch33680, loss = 0.0289\n",
            "epoch33690, loss = 0.0289\n",
            "epoch33700, loss = 0.0289\n",
            "epoch33710, loss = 0.0289\n",
            "epoch33720, loss = 0.0289\n",
            "epoch33730, loss = 0.0289\n",
            "epoch33740, loss = 0.0289\n",
            "epoch33750, loss = 0.0289\n",
            "epoch33760, loss = 0.0289\n",
            "epoch33770, loss = 0.0289\n",
            "epoch33780, loss = 0.0289\n",
            "epoch33790, loss = 0.0289\n",
            "epoch33800, loss = 0.0289\n",
            "epoch33810, loss = 0.0289\n",
            "epoch33820, loss = 0.0288\n",
            "epoch33830, loss = 0.0288\n",
            "epoch33840, loss = 0.0288\n",
            "epoch33850, loss = 0.0288\n",
            "epoch33860, loss = 0.0288\n",
            "epoch33870, loss = 0.0288\n",
            "epoch33880, loss = 0.0288\n",
            "epoch33890, loss = 0.0288\n",
            "epoch33900, loss = 0.0288\n",
            "epoch33910, loss = 0.0288\n",
            "epoch33920, loss = 0.0288\n",
            "epoch33930, loss = 0.0288\n",
            "epoch33940, loss = 0.0288\n",
            "epoch33950, loss = 0.0288\n",
            "epoch33960, loss = 0.0288\n",
            "epoch33970, loss = 0.0288\n",
            "epoch33980, loss = 0.0288\n",
            "epoch33990, loss = 0.0288\n",
            "epoch34000, loss = 0.0288\n",
            "epoch34010, loss = 0.0288\n",
            "epoch34020, loss = 0.0288\n",
            "epoch34030, loss = 0.0288\n",
            "epoch34040, loss = 0.0288\n",
            "epoch34050, loss = 0.0288\n",
            "epoch34060, loss = 0.0288\n",
            "epoch34070, loss = 0.0288\n",
            "epoch34080, loss = 0.0288\n",
            "epoch34090, loss = 0.0288\n",
            "epoch34100, loss = 0.0288\n",
            "epoch34110, loss = 0.0288\n",
            "epoch34120, loss = 0.0288\n",
            "epoch34130, loss = 0.0288\n",
            "epoch34140, loss = 0.0288\n",
            "epoch34150, loss = 0.0288\n",
            "epoch34160, loss = 0.0288\n",
            "epoch34170, loss = 0.0288\n",
            "epoch34180, loss = 0.0288\n",
            "epoch34190, loss = 0.0288\n",
            "epoch34200, loss = 0.0288\n",
            "epoch34210, loss = 0.0288\n",
            "epoch34220, loss = 0.0288\n",
            "epoch34230, loss = 0.0288\n",
            "epoch34240, loss = 0.0288\n",
            "epoch34250, loss = 0.0288\n",
            "epoch34260, loss = 0.0288\n",
            "epoch34270, loss = 0.0288\n",
            "epoch34280, loss = 0.0287\n",
            "epoch34290, loss = 0.0287\n",
            "epoch34300, loss = 0.0287\n",
            "epoch34310, loss = 0.0287\n",
            "epoch34320, loss = 0.0287\n",
            "epoch34330, loss = 0.0287\n",
            "epoch34340, loss = 0.0287\n",
            "epoch34350, loss = 0.0287\n",
            "epoch34360, loss = 0.0287\n",
            "epoch34370, loss = 0.0287\n",
            "epoch34380, loss = 0.0287\n",
            "epoch34390, loss = 0.0287\n",
            "epoch34400, loss = 0.0287\n",
            "epoch34410, loss = 0.0287\n",
            "epoch34420, loss = 0.0287\n",
            "epoch34430, loss = 0.0287\n",
            "epoch34440, loss = 0.0287\n",
            "epoch34450, loss = 0.0287\n",
            "epoch34460, loss = 0.0287\n",
            "epoch34470, loss = 0.0287\n",
            "epoch34480, loss = 0.0287\n",
            "epoch34490, loss = 0.0287\n",
            "epoch34500, loss = 0.0287\n",
            "epoch34510, loss = 0.0287\n",
            "epoch34520, loss = 0.0287\n",
            "epoch34530, loss = 0.0287\n",
            "epoch34540, loss = 0.0287\n",
            "epoch34550, loss = 0.0287\n",
            "epoch34560, loss = 0.0287\n",
            "epoch34570, loss = 0.0287\n",
            "epoch34580, loss = 0.0287\n",
            "epoch34590, loss = 0.0287\n",
            "epoch34600, loss = 0.0287\n",
            "epoch34610, loss = 0.0287\n",
            "epoch34620, loss = 0.0287\n",
            "epoch34630, loss = 0.0287\n",
            "epoch34640, loss = 0.0287\n",
            "epoch34650, loss = 0.0287\n",
            "epoch34660, loss = 0.0287\n",
            "epoch34670, loss = 0.0287\n",
            "epoch34680, loss = 0.0287\n",
            "epoch34690, loss = 0.0287\n",
            "epoch34700, loss = 0.0287\n",
            "epoch34710, loss = 0.0287\n",
            "epoch34720, loss = 0.0287\n",
            "epoch34730, loss = 0.0287\n",
            "epoch34740, loss = 0.0286\n",
            "epoch34750, loss = 0.0286\n",
            "epoch34760, loss = 0.0286\n",
            "epoch34770, loss = 0.0286\n",
            "epoch34780, loss = 0.0286\n",
            "epoch34790, loss = 0.0286\n",
            "epoch34800, loss = 0.0286\n",
            "epoch34810, loss = 0.0286\n",
            "epoch34820, loss = 0.0286\n",
            "epoch34830, loss = 0.0286\n",
            "epoch34840, loss = 0.0286\n",
            "epoch34850, loss = 0.0286\n",
            "epoch34860, loss = 0.0286\n",
            "epoch34870, loss = 0.0286\n",
            "epoch34880, loss = 0.0286\n",
            "epoch34890, loss = 0.0286\n",
            "epoch34900, loss = 0.0286\n",
            "epoch34910, loss = 0.0286\n",
            "epoch34920, loss = 0.0286\n",
            "epoch34930, loss = 0.0286\n",
            "epoch34940, loss = 0.0286\n",
            "epoch34950, loss = 0.0286\n",
            "epoch34960, loss = 0.0286\n",
            "epoch34970, loss = 0.0286\n",
            "epoch34980, loss = 0.0286\n",
            "epoch34990, loss = 0.0286\n",
            "epoch35000, loss = 0.0286\n",
            "epoch35010, loss = 0.0286\n",
            "epoch35020, loss = 0.0286\n",
            "epoch35030, loss = 0.0286\n",
            "epoch35040, loss = 0.0286\n",
            "epoch35050, loss = 0.0286\n",
            "epoch35060, loss = 0.0286\n",
            "epoch35070, loss = 0.0286\n",
            "epoch35080, loss = 0.0286\n",
            "epoch35090, loss = 0.0286\n",
            "epoch35100, loss = 0.0286\n",
            "epoch35110, loss = 0.0286\n",
            "epoch35120, loss = 0.0286\n",
            "epoch35130, loss = 0.0286\n",
            "epoch35140, loss = 0.0286\n",
            "epoch35150, loss = 0.0286\n",
            "epoch35160, loss = 0.0286\n",
            "epoch35170, loss = 0.0286\n",
            "epoch35180, loss = 0.0286\n",
            "epoch35190, loss = 0.0286\n",
            "epoch35200, loss = 0.0286\n",
            "epoch35210, loss = 0.0286\n",
            "epoch35220, loss = 0.0285\n",
            "epoch35230, loss = 0.0285\n",
            "epoch35240, loss = 0.0285\n",
            "epoch35250, loss = 0.0285\n",
            "epoch35260, loss = 0.0285\n",
            "epoch35270, loss = 0.0285\n",
            "epoch35280, loss = 0.0285\n",
            "epoch35290, loss = 0.0285\n",
            "epoch35300, loss = 0.0285\n",
            "epoch35310, loss = 0.0285\n",
            "epoch35320, loss = 0.0285\n",
            "epoch35330, loss = 0.0285\n",
            "epoch35340, loss = 0.0285\n",
            "epoch35350, loss = 0.0285\n",
            "epoch35360, loss = 0.0285\n",
            "epoch35370, loss = 0.0285\n",
            "epoch35380, loss = 0.0285\n",
            "epoch35390, loss = 0.0285\n",
            "epoch35400, loss = 0.0285\n",
            "epoch35410, loss = 0.0285\n",
            "epoch35420, loss = 0.0285\n",
            "epoch35430, loss = 0.0285\n",
            "epoch35440, loss = 0.0285\n",
            "epoch35450, loss = 0.0285\n",
            "epoch35460, loss = 0.0285\n",
            "epoch35470, loss = 0.0285\n",
            "epoch35480, loss = 0.0285\n",
            "epoch35490, loss = 0.0285\n",
            "epoch35500, loss = 0.0285\n",
            "epoch35510, loss = 0.0285\n",
            "epoch35520, loss = 0.0285\n",
            "epoch35530, loss = 0.0285\n",
            "epoch35540, loss = 0.0285\n",
            "epoch35550, loss = 0.0285\n",
            "epoch35560, loss = 0.0285\n",
            "epoch35570, loss = 0.0285\n",
            "epoch35580, loss = 0.0285\n",
            "epoch35590, loss = 0.0285\n",
            "epoch35600, loss = 0.0285\n",
            "epoch35610, loss = 0.0285\n",
            "epoch35620, loss = 0.0285\n",
            "epoch35630, loss = 0.0285\n",
            "epoch35640, loss = 0.0285\n",
            "epoch35650, loss = 0.0285\n",
            "epoch35660, loss = 0.0285\n",
            "epoch35670, loss = 0.0285\n",
            "epoch35680, loss = 0.0285\n",
            "epoch35690, loss = 0.0285\n",
            "epoch35700, loss = 0.0285\n",
            "epoch35710, loss = 0.0284\n",
            "epoch35720, loss = 0.0284\n",
            "epoch35730, loss = 0.0284\n",
            "epoch35740, loss = 0.0284\n",
            "epoch35750, loss = 0.0284\n",
            "epoch35760, loss = 0.0284\n",
            "epoch35770, loss = 0.0284\n",
            "epoch35780, loss = 0.0284\n",
            "epoch35790, loss = 0.0284\n",
            "epoch35800, loss = 0.0284\n",
            "epoch35810, loss = 0.0284\n",
            "epoch35820, loss = 0.0284\n",
            "epoch35830, loss = 0.0284\n",
            "epoch35840, loss = 0.0284\n",
            "epoch35850, loss = 0.0284\n",
            "epoch35860, loss = 0.0284\n",
            "epoch35870, loss = 0.0284\n",
            "epoch35880, loss = 0.0284\n",
            "epoch35890, loss = 0.0284\n",
            "epoch35900, loss = 0.0284\n",
            "epoch35910, loss = 0.0284\n",
            "epoch35920, loss = 0.0284\n",
            "epoch35930, loss = 0.0284\n",
            "epoch35940, loss = 0.0284\n",
            "epoch35950, loss = 0.0284\n",
            "epoch35960, loss = 0.0284\n",
            "epoch35970, loss = 0.0284\n",
            "epoch35980, loss = 0.0284\n",
            "epoch35990, loss = 0.0284\n",
            "epoch36000, loss = 0.0284\n",
            "epoch36010, loss = 0.0284\n",
            "epoch36020, loss = 0.0284\n",
            "epoch36030, loss = 0.0284\n",
            "epoch36040, loss = 0.0284\n",
            "epoch36050, loss = 0.0284\n",
            "epoch36060, loss = 0.0284\n",
            "epoch36070, loss = 0.0284\n",
            "epoch36080, loss = 0.0284\n",
            "epoch36090, loss = 0.0284\n",
            "epoch36100, loss = 0.0284\n",
            "epoch36110, loss = 0.0284\n",
            "epoch36120, loss = 0.0284\n",
            "epoch36130, loss = 0.0284\n",
            "epoch36140, loss = 0.0284\n",
            "epoch36150, loss = 0.0284\n",
            "epoch36160, loss = 0.0284\n",
            "epoch36170, loss = 0.0284\n",
            "epoch36180, loss = 0.0284\n",
            "epoch36190, loss = 0.0284\n",
            "epoch36200, loss = 0.0283\n",
            "epoch36210, loss = 0.0283\n",
            "epoch36220, loss = 0.0283\n",
            "epoch36230, loss = 0.0283\n",
            "epoch36240, loss = 0.0283\n",
            "epoch36250, loss = 0.0283\n",
            "epoch36260, loss = 0.0283\n",
            "epoch36270, loss = 0.0283\n",
            "epoch36280, loss = 0.0283\n",
            "epoch36290, loss = 0.0283\n",
            "epoch36300, loss = 0.0283\n",
            "epoch36310, loss = 0.0283\n",
            "epoch36320, loss = 0.0283\n",
            "epoch36330, loss = 0.0283\n",
            "epoch36340, loss = 0.0283\n",
            "epoch36350, loss = 0.0283\n",
            "epoch36360, loss = 0.0283\n",
            "epoch36370, loss = 0.0283\n",
            "epoch36380, loss = 0.0283\n",
            "epoch36390, loss = 0.0283\n",
            "epoch36400, loss = 0.0283\n",
            "epoch36410, loss = 0.0283\n",
            "epoch36420, loss = 0.0283\n",
            "epoch36430, loss = 0.0283\n",
            "epoch36440, loss = 0.0283\n",
            "epoch36450, loss = 0.0283\n",
            "epoch36460, loss = 0.0283\n",
            "epoch36470, loss = 0.0283\n",
            "epoch36480, loss = 0.0283\n",
            "epoch36490, loss = 0.0283\n",
            "epoch36500, loss = 0.0283\n",
            "epoch36510, loss = 0.0283\n",
            "epoch36520, loss = 0.0283\n",
            "epoch36530, loss = 0.0283\n",
            "epoch36540, loss = 0.0283\n",
            "epoch36550, loss = 0.0283\n",
            "epoch36560, loss = 0.0283\n",
            "epoch36570, loss = 0.0283\n",
            "epoch36580, loss = 0.0283\n",
            "epoch36590, loss = 0.0283\n",
            "epoch36600, loss = 0.0283\n",
            "epoch36610, loss = 0.0283\n",
            "epoch36620, loss = 0.0283\n",
            "epoch36630, loss = 0.0283\n",
            "epoch36640, loss = 0.0283\n",
            "epoch36650, loss = 0.0283\n",
            "epoch36660, loss = 0.0283\n",
            "epoch36670, loss = 0.0283\n",
            "epoch36680, loss = 0.0283\n",
            "epoch36690, loss = 0.0283\n",
            "epoch36700, loss = 0.0283\n",
            "epoch36710, loss = 0.0282\n",
            "epoch36720, loss = 0.0282\n",
            "epoch36730, loss = 0.0282\n",
            "epoch36740, loss = 0.0282\n",
            "epoch36750, loss = 0.0282\n",
            "epoch36760, loss = 0.0282\n",
            "epoch36770, loss = 0.0282\n",
            "epoch36780, loss = 0.0282\n",
            "epoch36790, loss = 0.0282\n",
            "epoch36800, loss = 0.0282\n",
            "epoch36810, loss = 0.0282\n",
            "epoch36820, loss = 0.0282\n",
            "epoch36830, loss = 0.0282\n",
            "epoch36840, loss = 0.0282\n",
            "epoch36850, loss = 0.0282\n",
            "epoch36860, loss = 0.0282\n",
            "epoch36870, loss = 0.0282\n",
            "epoch36880, loss = 0.0282\n",
            "epoch36890, loss = 0.0282\n",
            "epoch36900, loss = 0.0282\n",
            "epoch36910, loss = 0.0282\n",
            "epoch36920, loss = 0.0282\n",
            "epoch36930, loss = 0.0282\n",
            "epoch36940, loss = 0.0282\n",
            "epoch36950, loss = 0.0282\n",
            "epoch36960, loss = 0.0282\n",
            "epoch36970, loss = 0.0282\n",
            "epoch36980, loss = 0.0282\n",
            "epoch36990, loss = 0.0282\n",
            "epoch37000, loss = 0.0282\n",
            "epoch37010, loss = 0.0282\n",
            "epoch37020, loss = 0.0282\n",
            "epoch37030, loss = 0.0282\n",
            "epoch37040, loss = 0.0282\n",
            "epoch37050, loss = 0.0282\n",
            "epoch37060, loss = 0.0282\n",
            "epoch37070, loss = 0.0282\n",
            "epoch37080, loss = 0.0282\n",
            "epoch37090, loss = 0.0282\n",
            "epoch37100, loss = 0.0282\n",
            "epoch37110, loss = 0.0282\n",
            "epoch37120, loss = 0.0282\n",
            "epoch37130, loss = 0.0282\n",
            "epoch37140, loss = 0.0282\n",
            "epoch37150, loss = 0.0282\n",
            "epoch37160, loss = 0.0282\n",
            "epoch37170, loss = 0.0282\n",
            "epoch37180, loss = 0.0282\n",
            "epoch37190, loss = 0.0282\n",
            "epoch37200, loss = 0.0282\n",
            "epoch37210, loss = 0.0282\n",
            "epoch37220, loss = 0.0281\n",
            "epoch37230, loss = 0.0281\n",
            "epoch37240, loss = 0.0281\n",
            "epoch37250, loss = 0.0281\n",
            "epoch37260, loss = 0.0281\n",
            "epoch37270, loss = 0.0281\n",
            "epoch37280, loss = 0.0281\n",
            "epoch37290, loss = 0.0281\n",
            "epoch37300, loss = 0.0281\n",
            "epoch37310, loss = 0.0281\n",
            "epoch37320, loss = 0.0281\n",
            "epoch37330, loss = 0.0281\n",
            "epoch37340, loss = 0.0281\n",
            "epoch37350, loss = 0.0281\n",
            "epoch37360, loss = 0.0281\n",
            "epoch37370, loss = 0.0281\n",
            "epoch37380, loss = 0.0281\n",
            "epoch37390, loss = 0.0281\n",
            "epoch37400, loss = 0.0281\n",
            "epoch37410, loss = 0.0281\n",
            "epoch37420, loss = 0.0281\n",
            "epoch37430, loss = 0.0281\n",
            "epoch37440, loss = 0.0281\n",
            "epoch37450, loss = 0.0281\n",
            "epoch37460, loss = 0.0281\n",
            "epoch37470, loss = 0.0281\n",
            "epoch37480, loss = 0.0281\n",
            "epoch37490, loss = 0.0281\n",
            "epoch37500, loss = 0.0281\n",
            "epoch37510, loss = 0.0281\n",
            "epoch37520, loss = 0.0281\n",
            "epoch37530, loss = 0.0281\n",
            "epoch37540, loss = 0.0281\n",
            "epoch37550, loss = 0.0281\n",
            "epoch37560, loss = 0.0281\n",
            "epoch37570, loss = 0.0281\n",
            "epoch37580, loss = 0.0281\n",
            "epoch37590, loss = 0.0281\n",
            "epoch37600, loss = 0.0281\n",
            "epoch37610, loss = 0.0281\n",
            "epoch37620, loss = 0.0281\n",
            "epoch37630, loss = 0.0281\n",
            "epoch37640, loss = 0.0281\n",
            "epoch37650, loss = 0.0281\n",
            "epoch37660, loss = 0.0281\n",
            "epoch37670, loss = 0.0281\n",
            "epoch37680, loss = 0.0281\n",
            "epoch37690, loss = 0.0281\n",
            "epoch37700, loss = 0.0281\n",
            "epoch37710, loss = 0.0281\n",
            "epoch37720, loss = 0.0281\n",
            "epoch37730, loss = 0.0281\n",
            "epoch37740, loss = 0.0281\n",
            "epoch37750, loss = 0.0280\n",
            "epoch37760, loss = 0.0280\n",
            "epoch37770, loss = 0.0280\n",
            "epoch37780, loss = 0.0280\n",
            "epoch37790, loss = 0.0280\n",
            "epoch37800, loss = 0.0280\n",
            "epoch37810, loss = 0.0280\n",
            "epoch37820, loss = 0.0280\n",
            "epoch37830, loss = 0.0280\n",
            "epoch37840, loss = 0.0280\n",
            "epoch37850, loss = 0.0280\n",
            "epoch37860, loss = 0.0280\n",
            "epoch37870, loss = 0.0280\n",
            "epoch37880, loss = 0.0280\n",
            "epoch37890, loss = 0.0280\n",
            "epoch37900, loss = 0.0280\n",
            "epoch37910, loss = 0.0280\n",
            "epoch37920, loss = 0.0280\n",
            "epoch37930, loss = 0.0280\n",
            "epoch37940, loss = 0.0280\n",
            "epoch37950, loss = 0.0280\n",
            "epoch37960, loss = 0.0280\n",
            "epoch37970, loss = 0.0280\n",
            "epoch37980, loss = 0.0280\n",
            "epoch37990, loss = 0.0280\n",
            "epoch38000, loss = 0.0280\n",
            "epoch38010, loss = 0.0280\n",
            "epoch38020, loss = 0.0280\n",
            "epoch38030, loss = 0.0280\n",
            "epoch38040, loss = 0.0280\n",
            "epoch38050, loss = 0.0280\n",
            "epoch38060, loss = 0.0280\n",
            "epoch38070, loss = 0.0280\n",
            "epoch38080, loss = 0.0280\n",
            "epoch38090, loss = 0.0280\n",
            "epoch38100, loss = 0.0280\n",
            "epoch38110, loss = 0.0280\n",
            "epoch38120, loss = 0.0280\n",
            "epoch38130, loss = 0.0280\n",
            "epoch38140, loss = 0.0280\n",
            "epoch38150, loss = 0.0280\n",
            "epoch38160, loss = 0.0280\n",
            "epoch38170, loss = 0.0280\n",
            "epoch38180, loss = 0.0280\n",
            "epoch38190, loss = 0.0280\n",
            "epoch38200, loss = 0.0280\n",
            "epoch38210, loss = 0.0280\n",
            "epoch38220, loss = 0.0280\n",
            "epoch38230, loss = 0.0280\n",
            "epoch38240, loss = 0.0280\n",
            "epoch38250, loss = 0.0280\n",
            "epoch38260, loss = 0.0280\n",
            "epoch38270, loss = 0.0280\n",
            "epoch38280, loss = 0.0280\n",
            "epoch38290, loss = 0.0279\n",
            "epoch38300, loss = 0.0279\n",
            "epoch38310, loss = 0.0279\n",
            "epoch38320, loss = 0.0279\n",
            "epoch38330, loss = 0.0279\n",
            "epoch38340, loss = 0.0279\n",
            "epoch38350, loss = 0.0279\n",
            "epoch38360, loss = 0.0279\n",
            "epoch38370, loss = 0.0279\n",
            "epoch38380, loss = 0.0279\n",
            "epoch38390, loss = 0.0279\n",
            "epoch38400, loss = 0.0279\n",
            "epoch38410, loss = 0.0279\n",
            "epoch38420, loss = 0.0279\n",
            "epoch38430, loss = 0.0279\n",
            "epoch38440, loss = 0.0279\n",
            "epoch38450, loss = 0.0279\n",
            "epoch38460, loss = 0.0279\n",
            "epoch38470, loss = 0.0279\n",
            "epoch38480, loss = 0.0279\n",
            "epoch38490, loss = 0.0279\n",
            "epoch38500, loss = 0.0279\n",
            "epoch38510, loss = 0.0279\n",
            "epoch38520, loss = 0.0279\n",
            "epoch38530, loss = 0.0279\n",
            "epoch38540, loss = 0.0279\n",
            "epoch38550, loss = 0.0279\n",
            "epoch38560, loss = 0.0279\n",
            "epoch38570, loss = 0.0279\n",
            "epoch38580, loss = 0.0279\n",
            "epoch38590, loss = 0.0279\n",
            "epoch38600, loss = 0.0279\n",
            "epoch38610, loss = 0.0279\n",
            "epoch38620, loss = 0.0279\n",
            "epoch38630, loss = 0.0279\n",
            "epoch38640, loss = 0.0279\n",
            "epoch38650, loss = 0.0279\n",
            "epoch38660, loss = 0.0279\n",
            "epoch38670, loss = 0.0279\n",
            "epoch38680, loss = 0.0279\n",
            "epoch38690, loss = 0.0279\n",
            "epoch38700, loss = 0.0279\n",
            "epoch38710, loss = 0.0279\n",
            "epoch38720, loss = 0.0279\n",
            "epoch38730, loss = 0.0279\n",
            "epoch38740, loss = 0.0279\n",
            "epoch38750, loss = 0.0279\n",
            "epoch38760, loss = 0.0279\n",
            "epoch38770, loss = 0.0279\n",
            "epoch38780, loss = 0.0279\n",
            "epoch38790, loss = 0.0279\n",
            "epoch38800, loss = 0.0279\n",
            "epoch38810, loss = 0.0279\n",
            "epoch38820, loss = 0.0279\n",
            "epoch38830, loss = 0.0278\n",
            "epoch38840, loss = 0.0278\n",
            "epoch38850, loss = 0.0278\n",
            "epoch38860, loss = 0.0278\n",
            "epoch38870, loss = 0.0278\n",
            "epoch38880, loss = 0.0278\n",
            "epoch38890, loss = 0.0278\n",
            "epoch38900, loss = 0.0278\n",
            "epoch38910, loss = 0.0278\n",
            "epoch38920, loss = 0.0278\n",
            "epoch38930, loss = 0.0278\n",
            "epoch38940, loss = 0.0278\n",
            "epoch38950, loss = 0.0278\n",
            "epoch38960, loss = 0.0278\n",
            "epoch38970, loss = 0.0278\n",
            "epoch38980, loss = 0.0278\n",
            "epoch38990, loss = 0.0278\n",
            "epoch39000, loss = 0.0278\n",
            "epoch39010, loss = 0.0278\n",
            "epoch39020, loss = 0.0278\n",
            "epoch39030, loss = 0.0278\n",
            "epoch39040, loss = 0.0278\n",
            "epoch39050, loss = 0.0278\n",
            "epoch39060, loss = 0.0278\n",
            "epoch39070, loss = 0.0278\n",
            "epoch39080, loss = 0.0278\n",
            "epoch39090, loss = 0.0278\n",
            "epoch39100, loss = 0.0278\n",
            "epoch39110, loss = 0.0278\n",
            "epoch39120, loss = 0.0278\n",
            "epoch39130, loss = 0.0278\n",
            "epoch39140, loss = 0.0278\n",
            "epoch39150, loss = 0.0278\n",
            "epoch39160, loss = 0.0278\n",
            "epoch39170, loss = 0.0278\n",
            "epoch39180, loss = 0.0278\n",
            "epoch39190, loss = 0.0278\n",
            "epoch39200, loss = 0.0278\n",
            "epoch39210, loss = 0.0278\n",
            "epoch39220, loss = 0.0278\n",
            "epoch39230, loss = 0.0278\n",
            "epoch39240, loss = 0.0278\n",
            "epoch39250, loss = 0.0278\n",
            "epoch39260, loss = 0.0278\n",
            "epoch39270, loss = 0.0278\n",
            "epoch39280, loss = 0.0278\n",
            "epoch39290, loss = 0.0278\n",
            "epoch39300, loss = 0.0278\n",
            "epoch39310, loss = 0.0278\n",
            "epoch39320, loss = 0.0278\n",
            "epoch39330, loss = 0.0278\n",
            "epoch39340, loss = 0.0278\n",
            "epoch39350, loss = 0.0278\n",
            "epoch39360, loss = 0.0278\n",
            "epoch39370, loss = 0.0278\n",
            "epoch39380, loss = 0.0278\n",
            "epoch39390, loss = 0.0277\n",
            "epoch39400, loss = 0.0277\n",
            "epoch39410, loss = 0.0277\n",
            "epoch39420, loss = 0.0277\n",
            "epoch39430, loss = 0.0277\n",
            "epoch39440, loss = 0.0277\n",
            "epoch39450, loss = 0.0277\n",
            "epoch39460, loss = 0.0277\n",
            "epoch39470, loss = 0.0277\n",
            "epoch39480, loss = 0.0277\n",
            "epoch39490, loss = 0.0277\n",
            "epoch39500, loss = 0.0277\n",
            "epoch39510, loss = 0.0277\n",
            "epoch39520, loss = 0.0277\n",
            "epoch39530, loss = 0.0277\n",
            "epoch39540, loss = 0.0277\n",
            "epoch39550, loss = 0.0277\n",
            "epoch39560, loss = 0.0277\n",
            "epoch39570, loss = 0.0277\n",
            "epoch39580, loss = 0.0277\n",
            "epoch39590, loss = 0.0277\n",
            "epoch39600, loss = 0.0277\n",
            "epoch39610, loss = 0.0277\n",
            "epoch39620, loss = 0.0277\n",
            "epoch39630, loss = 0.0277\n",
            "epoch39640, loss = 0.0277\n",
            "epoch39650, loss = 0.0277\n",
            "epoch39660, loss = 0.0277\n",
            "epoch39670, loss = 0.0277\n",
            "epoch39680, loss = 0.0277\n",
            "epoch39690, loss = 0.0277\n",
            "epoch39700, loss = 0.0277\n",
            "epoch39710, loss = 0.0277\n",
            "epoch39720, loss = 0.0277\n",
            "epoch39730, loss = 0.0277\n",
            "epoch39740, loss = 0.0277\n",
            "epoch39750, loss = 0.0277\n",
            "epoch39760, loss = 0.0277\n",
            "epoch39770, loss = 0.0277\n",
            "epoch39780, loss = 0.0277\n",
            "epoch39790, loss = 0.0277\n",
            "epoch39800, loss = 0.0277\n",
            "epoch39810, loss = 0.0277\n",
            "epoch39820, loss = 0.0277\n",
            "epoch39830, loss = 0.0277\n",
            "epoch39840, loss = 0.0277\n",
            "epoch39850, loss = 0.0277\n",
            "epoch39860, loss = 0.0277\n",
            "epoch39870, loss = 0.0277\n",
            "epoch39880, loss = 0.0277\n",
            "epoch39890, loss = 0.0277\n",
            "epoch39900, loss = 0.0277\n",
            "epoch39910, loss = 0.0277\n",
            "epoch39920, loss = 0.0277\n",
            "epoch39930, loss = 0.0277\n",
            "epoch39940, loss = 0.0277\n",
            "epoch39950, loss = 0.0277\n",
            "epoch39960, loss = 0.0276\n",
            "epoch39970, loss = 0.0276\n",
            "epoch39980, loss = 0.0276\n",
            "epoch39990, loss = 0.0276\n",
            "epoch40000, loss = 0.0276\n",
            "epoch40010, loss = 0.0276\n",
            "epoch40020, loss = 0.0276\n",
            "epoch40030, loss = 0.0276\n",
            "epoch40040, loss = 0.0276\n",
            "epoch40050, loss = 0.0276\n",
            "epoch40060, loss = 0.0276\n",
            "epoch40070, loss = 0.0276\n",
            "epoch40080, loss = 0.0276\n",
            "epoch40090, loss = 0.0276\n",
            "epoch40100, loss = 0.0276\n",
            "epoch40110, loss = 0.0276\n",
            "epoch40120, loss = 0.0276\n",
            "epoch40130, loss = 0.0276\n",
            "epoch40140, loss = 0.0276\n",
            "epoch40150, loss = 0.0276\n",
            "epoch40160, loss = 0.0276\n",
            "epoch40170, loss = 0.0276\n",
            "epoch40180, loss = 0.0276\n",
            "epoch40190, loss = 0.0276\n",
            "epoch40200, loss = 0.0276\n",
            "epoch40210, loss = 0.0276\n",
            "epoch40220, loss = 0.0276\n",
            "epoch40230, loss = 0.0276\n",
            "epoch40240, loss = 0.0276\n",
            "epoch40250, loss = 0.0276\n",
            "epoch40260, loss = 0.0276\n",
            "epoch40270, loss = 0.0276\n",
            "epoch40280, loss = 0.0276\n",
            "epoch40290, loss = 0.0276\n",
            "epoch40300, loss = 0.0276\n",
            "epoch40310, loss = 0.0276\n",
            "epoch40320, loss = 0.0276\n",
            "epoch40330, loss = 0.0276\n",
            "epoch40340, loss = 0.0276\n",
            "epoch40350, loss = 0.0276\n",
            "epoch40360, loss = 0.0276\n",
            "epoch40370, loss = 0.0276\n",
            "epoch40380, loss = 0.0276\n",
            "epoch40390, loss = 0.0276\n",
            "epoch40400, loss = 0.0276\n",
            "epoch40410, loss = 0.0276\n",
            "epoch40420, loss = 0.0276\n",
            "epoch40430, loss = 0.0276\n",
            "epoch40440, loss = 0.0276\n",
            "epoch40450, loss = 0.0276\n",
            "epoch40460, loss = 0.0276\n",
            "epoch40470, loss = 0.0276\n",
            "epoch40480, loss = 0.0276\n",
            "epoch40490, loss = 0.0276\n",
            "epoch40500, loss = 0.0276\n",
            "epoch40510, loss = 0.0276\n",
            "epoch40520, loss = 0.0276\n",
            "epoch40530, loss = 0.0276\n",
            "epoch40540, loss = 0.0275\n",
            "epoch40550, loss = 0.0275\n",
            "epoch40560, loss = 0.0275\n",
            "epoch40570, loss = 0.0275\n",
            "epoch40580, loss = 0.0275\n",
            "epoch40590, loss = 0.0275\n",
            "epoch40600, loss = 0.0275\n",
            "epoch40610, loss = 0.0275\n",
            "epoch40620, loss = 0.0275\n",
            "epoch40630, loss = 0.0275\n",
            "epoch40640, loss = 0.0275\n",
            "epoch40650, loss = 0.0275\n",
            "epoch40660, loss = 0.0275\n",
            "epoch40670, loss = 0.0275\n",
            "epoch40680, loss = 0.0275\n",
            "epoch40690, loss = 0.0275\n",
            "epoch40700, loss = 0.0275\n",
            "epoch40710, loss = 0.0275\n",
            "epoch40720, loss = 0.0275\n",
            "epoch40730, loss = 0.0275\n",
            "epoch40740, loss = 0.0275\n",
            "epoch40750, loss = 0.0275\n",
            "epoch40760, loss = 0.0275\n",
            "epoch40770, loss = 0.0275\n",
            "epoch40780, loss = 0.0275\n",
            "epoch40790, loss = 0.0275\n",
            "epoch40800, loss = 0.0275\n",
            "epoch40810, loss = 0.0275\n",
            "epoch40820, loss = 0.0275\n",
            "epoch40830, loss = 0.0275\n",
            "epoch40840, loss = 0.0275\n",
            "epoch40850, loss = 0.0275\n",
            "epoch40860, loss = 0.0275\n",
            "epoch40870, loss = 0.0275\n",
            "epoch40880, loss = 0.0275\n",
            "epoch40890, loss = 0.0275\n",
            "epoch40900, loss = 0.0275\n",
            "epoch40910, loss = 0.0275\n",
            "epoch40920, loss = 0.0275\n",
            "epoch40930, loss = 0.0275\n",
            "epoch40940, loss = 0.0275\n",
            "epoch40950, loss = 0.0275\n",
            "epoch40960, loss = 0.0275\n",
            "epoch40970, loss = 0.0275\n",
            "epoch40980, loss = 0.0275\n",
            "epoch40990, loss = 0.0275\n",
            "epoch41000, loss = 0.0275\n",
            "epoch41010, loss = 0.0275\n",
            "epoch41020, loss = 0.0275\n",
            "epoch41030, loss = 0.0275\n",
            "epoch41040, loss = 0.0275\n",
            "epoch41050, loss = 0.0275\n",
            "epoch41060, loss = 0.0275\n",
            "epoch41070, loss = 0.0275\n",
            "epoch41080, loss = 0.0275\n",
            "epoch41090, loss = 0.0275\n",
            "epoch41100, loss = 0.0275\n",
            "epoch41110, loss = 0.0275\n",
            "epoch41120, loss = 0.0275\n",
            "epoch41130, loss = 0.0274\n",
            "epoch41140, loss = 0.0274\n",
            "epoch41150, loss = 0.0274\n",
            "epoch41160, loss = 0.0274\n",
            "epoch41170, loss = 0.0274\n",
            "epoch41180, loss = 0.0274\n",
            "epoch41190, loss = 0.0274\n",
            "epoch41200, loss = 0.0274\n",
            "epoch41210, loss = 0.0274\n",
            "epoch41220, loss = 0.0274\n",
            "epoch41230, loss = 0.0274\n",
            "epoch41240, loss = 0.0274\n",
            "epoch41250, loss = 0.0274\n",
            "epoch41260, loss = 0.0274\n",
            "epoch41270, loss = 0.0274\n",
            "epoch41280, loss = 0.0274\n",
            "epoch41290, loss = 0.0274\n",
            "epoch41300, loss = 0.0274\n",
            "epoch41310, loss = 0.0274\n",
            "epoch41320, loss = 0.0274\n",
            "epoch41330, loss = 0.0274\n",
            "epoch41340, loss = 0.0274\n",
            "epoch41350, loss = 0.0274\n",
            "epoch41360, loss = 0.0274\n",
            "epoch41370, loss = 0.0274\n",
            "epoch41380, loss = 0.0274\n",
            "epoch41390, loss = 0.0274\n",
            "epoch41400, loss = 0.0274\n",
            "epoch41410, loss = 0.0274\n",
            "epoch41420, loss = 0.0274\n",
            "epoch41430, loss = 0.0274\n",
            "epoch41440, loss = 0.0274\n",
            "epoch41450, loss = 0.0274\n",
            "epoch41460, loss = 0.0274\n",
            "epoch41470, loss = 0.0274\n",
            "epoch41480, loss = 0.0274\n",
            "epoch41490, loss = 0.0274\n",
            "epoch41500, loss = 0.0274\n",
            "epoch41510, loss = 0.0274\n",
            "epoch41520, loss = 0.0274\n",
            "epoch41530, loss = 0.0274\n",
            "epoch41540, loss = 0.0274\n",
            "epoch41550, loss = 0.0274\n",
            "epoch41560, loss = 0.0274\n",
            "epoch41570, loss = 0.0274\n",
            "epoch41580, loss = 0.0274\n",
            "epoch41590, loss = 0.0274\n",
            "epoch41600, loss = 0.0274\n",
            "epoch41610, loss = 0.0274\n",
            "epoch41620, loss = 0.0274\n",
            "epoch41630, loss = 0.0274\n",
            "epoch41640, loss = 0.0274\n",
            "epoch41650, loss = 0.0274\n",
            "epoch41660, loss = 0.0274\n",
            "epoch41670, loss = 0.0274\n",
            "epoch41680, loss = 0.0274\n",
            "epoch41690, loss = 0.0274\n",
            "epoch41700, loss = 0.0274\n",
            "epoch41710, loss = 0.0274\n",
            "epoch41720, loss = 0.0274\n",
            "epoch41730, loss = 0.0274\n",
            "epoch41740, loss = 0.0273\n",
            "epoch41750, loss = 0.0273\n",
            "epoch41760, loss = 0.0273\n",
            "epoch41770, loss = 0.0273\n",
            "epoch41780, loss = 0.0273\n",
            "epoch41790, loss = 0.0273\n",
            "epoch41800, loss = 0.0273\n",
            "epoch41810, loss = 0.0273\n",
            "epoch41820, loss = 0.0273\n",
            "epoch41830, loss = 0.0273\n",
            "epoch41840, loss = 0.0273\n",
            "epoch41850, loss = 0.0273\n",
            "epoch41860, loss = 0.0273\n",
            "epoch41870, loss = 0.0273\n",
            "epoch41880, loss = 0.0273\n",
            "epoch41890, loss = 0.0273\n",
            "epoch41900, loss = 0.0273\n",
            "epoch41910, loss = 0.0273\n",
            "epoch41920, loss = 0.0273\n",
            "epoch41930, loss = 0.0273\n",
            "epoch41940, loss = 0.0273\n",
            "epoch41950, loss = 0.0273\n",
            "epoch41960, loss = 0.0273\n",
            "epoch41970, loss = 0.0273\n",
            "epoch41980, loss = 0.0273\n",
            "epoch41990, loss = 0.0273\n",
            "epoch42000, loss = 0.0273\n",
            "epoch42010, loss = 0.0273\n",
            "epoch42020, loss = 0.0273\n",
            "epoch42030, loss = 0.0273\n",
            "epoch42040, loss = 0.0273\n",
            "epoch42050, loss = 0.0273\n",
            "epoch42060, loss = 0.0273\n",
            "epoch42070, loss = 0.0273\n",
            "epoch42080, loss = 0.0273\n",
            "epoch42090, loss = 0.0273\n",
            "epoch42100, loss = 0.0273\n",
            "epoch42110, loss = 0.0273\n",
            "epoch42120, loss = 0.0273\n",
            "epoch42130, loss = 0.0273\n",
            "epoch42140, loss = 0.0273\n",
            "epoch42150, loss = 0.0273\n",
            "epoch42160, loss = 0.0273\n",
            "epoch42170, loss = 0.0273\n",
            "epoch42180, loss = 0.0273\n",
            "epoch42190, loss = 0.0273\n",
            "epoch42200, loss = 0.0273\n",
            "epoch42210, loss = 0.0273\n",
            "epoch42220, loss = 0.0273\n",
            "epoch42230, loss = 0.0273\n",
            "epoch42240, loss = 0.0273\n",
            "epoch42250, loss = 0.0273\n",
            "epoch42260, loss = 0.0273\n",
            "epoch42270, loss = 0.0273\n",
            "epoch42280, loss = 0.0273\n",
            "epoch42290, loss = 0.0273\n",
            "epoch42300, loss = 0.0273\n",
            "epoch42310, loss = 0.0273\n",
            "epoch42320, loss = 0.0273\n",
            "epoch42330, loss = 0.0273\n",
            "epoch42340, loss = 0.0273\n",
            "epoch42350, loss = 0.0273\n",
            "epoch42360, loss = 0.0272\n",
            "epoch42370, loss = 0.0272\n",
            "epoch42380, loss = 0.0272\n",
            "epoch42390, loss = 0.0272\n",
            "epoch42400, loss = 0.0272\n",
            "epoch42410, loss = 0.0272\n",
            "epoch42420, loss = 0.0272\n",
            "epoch42430, loss = 0.0272\n",
            "epoch42440, loss = 0.0272\n",
            "epoch42450, loss = 0.0272\n",
            "epoch42460, loss = 0.0272\n",
            "epoch42470, loss = 0.0272\n",
            "epoch42480, loss = 0.0272\n",
            "epoch42490, loss = 0.0272\n",
            "epoch42500, loss = 0.0272\n",
            "epoch42510, loss = 0.0272\n",
            "epoch42520, loss = 0.0272\n",
            "epoch42530, loss = 0.0272\n",
            "epoch42540, loss = 0.0272\n",
            "epoch42550, loss = 0.0272\n",
            "epoch42560, loss = 0.0272\n",
            "epoch42570, loss = 0.0272\n",
            "epoch42580, loss = 0.0272\n",
            "epoch42590, loss = 0.0272\n",
            "epoch42600, loss = 0.0272\n",
            "epoch42610, loss = 0.0272\n",
            "epoch42620, loss = 0.0272\n",
            "epoch42630, loss = 0.0272\n",
            "epoch42640, loss = 0.0272\n",
            "epoch42650, loss = 0.0272\n",
            "epoch42660, loss = 0.0272\n",
            "epoch42670, loss = 0.0272\n",
            "epoch42680, loss = 0.0272\n",
            "epoch42690, loss = 0.0272\n",
            "epoch42700, loss = 0.0272\n",
            "epoch42710, loss = 0.0272\n",
            "epoch42720, loss = 0.0272\n",
            "epoch42730, loss = 0.0272\n",
            "epoch42740, loss = 0.0272\n",
            "epoch42750, loss = 0.0272\n",
            "epoch42760, loss = 0.0272\n",
            "epoch42770, loss = 0.0272\n",
            "epoch42780, loss = 0.0272\n",
            "epoch42790, loss = 0.0272\n",
            "epoch42800, loss = 0.0272\n",
            "epoch42810, loss = 0.0272\n",
            "epoch42820, loss = 0.0272\n",
            "epoch42830, loss = 0.0272\n",
            "epoch42840, loss = 0.0272\n",
            "epoch42850, loss = 0.0272\n",
            "epoch42860, loss = 0.0272\n",
            "epoch42870, loss = 0.0272\n",
            "epoch42880, loss = 0.0272\n",
            "epoch42890, loss = 0.0272\n",
            "epoch42900, loss = 0.0272\n",
            "epoch42910, loss = 0.0272\n",
            "epoch42920, loss = 0.0272\n",
            "epoch42930, loss = 0.0272\n",
            "epoch42940, loss = 0.0272\n",
            "epoch42950, loss = 0.0272\n",
            "epoch42960, loss = 0.0272\n",
            "epoch42970, loss = 0.0272\n",
            "epoch42980, loss = 0.0272\n",
            "epoch42990, loss = 0.0271\n",
            "epoch43000, loss = 0.0271\n",
            "epoch43010, loss = 0.0271\n",
            "epoch43020, loss = 0.0271\n",
            "epoch43030, loss = 0.0271\n",
            "epoch43040, loss = 0.0271\n",
            "epoch43050, loss = 0.0271\n",
            "epoch43060, loss = 0.0271\n",
            "epoch43070, loss = 0.0271\n",
            "epoch43080, loss = 0.0271\n",
            "epoch43090, loss = 0.0271\n",
            "epoch43100, loss = 0.0271\n",
            "epoch43110, loss = 0.0271\n",
            "epoch43120, loss = 0.0271\n",
            "epoch43130, loss = 0.0271\n",
            "epoch43140, loss = 0.0271\n",
            "epoch43150, loss = 0.0271\n",
            "epoch43160, loss = 0.0271\n",
            "epoch43170, loss = 0.0271\n",
            "epoch43180, loss = 0.0271\n",
            "epoch43190, loss = 0.0271\n",
            "epoch43200, loss = 0.0271\n",
            "epoch43210, loss = 0.0271\n",
            "epoch43220, loss = 0.0271\n",
            "epoch43230, loss = 0.0271\n",
            "epoch43240, loss = 0.0271\n",
            "epoch43250, loss = 0.0271\n",
            "epoch43260, loss = 0.0271\n",
            "epoch43270, loss = 0.0271\n",
            "epoch43280, loss = 0.0271\n",
            "epoch43290, loss = 0.0271\n",
            "epoch43300, loss = 0.0271\n",
            "epoch43310, loss = 0.0271\n",
            "epoch43320, loss = 0.0271\n",
            "epoch43330, loss = 0.0271\n",
            "epoch43340, loss = 0.0271\n",
            "epoch43350, loss = 0.0271\n",
            "epoch43360, loss = 0.0271\n",
            "epoch43370, loss = 0.0271\n",
            "epoch43380, loss = 0.0271\n",
            "epoch43390, loss = 0.0271\n",
            "epoch43400, loss = 0.0271\n",
            "epoch43410, loss = 0.0271\n",
            "epoch43420, loss = 0.0271\n",
            "epoch43430, loss = 0.0271\n",
            "epoch43440, loss = 0.0271\n",
            "epoch43450, loss = 0.0271\n",
            "epoch43460, loss = 0.0271\n",
            "epoch43470, loss = 0.0271\n",
            "epoch43480, loss = 0.0271\n",
            "epoch43490, loss = 0.0271\n",
            "epoch43500, loss = 0.0271\n",
            "epoch43510, loss = 0.0271\n",
            "epoch43520, loss = 0.0271\n",
            "epoch43530, loss = 0.0271\n",
            "epoch43540, loss = 0.0271\n",
            "epoch43550, loss = 0.0271\n",
            "epoch43560, loss = 0.0271\n",
            "epoch43570, loss = 0.0271\n",
            "epoch43580, loss = 0.0271\n",
            "epoch43590, loss = 0.0271\n",
            "epoch43600, loss = 0.0271\n",
            "epoch43610, loss = 0.0271\n",
            "epoch43620, loss = 0.0271\n",
            "epoch43630, loss = 0.0270\n",
            "epoch43640, loss = 0.0270\n",
            "epoch43650, loss = 0.0270\n",
            "epoch43660, loss = 0.0270\n",
            "epoch43670, loss = 0.0270\n",
            "epoch43680, loss = 0.0270\n",
            "epoch43690, loss = 0.0270\n",
            "epoch43700, loss = 0.0270\n",
            "epoch43710, loss = 0.0270\n",
            "epoch43720, loss = 0.0270\n",
            "epoch43730, loss = 0.0270\n",
            "epoch43740, loss = 0.0270\n",
            "epoch43750, loss = 0.0270\n",
            "epoch43760, loss = 0.0270\n",
            "epoch43770, loss = 0.0270\n",
            "epoch43780, loss = 0.0270\n",
            "epoch43790, loss = 0.0270\n",
            "epoch43800, loss = 0.0270\n",
            "epoch43810, loss = 0.0270\n",
            "epoch43820, loss = 0.0270\n",
            "epoch43830, loss = 0.0270\n",
            "epoch43840, loss = 0.0270\n",
            "epoch43850, loss = 0.0270\n",
            "epoch43860, loss = 0.0270\n",
            "epoch43870, loss = 0.0270\n",
            "epoch43880, loss = 0.0270\n",
            "epoch43890, loss = 0.0270\n",
            "epoch43900, loss = 0.0270\n",
            "epoch43910, loss = 0.0270\n",
            "epoch43920, loss = 0.0270\n",
            "epoch43930, loss = 0.0270\n",
            "epoch43940, loss = 0.0270\n",
            "epoch43950, loss = 0.0270\n",
            "epoch43960, loss = 0.0270\n",
            "epoch43970, loss = 0.0270\n",
            "epoch43980, loss = 0.0270\n",
            "epoch43990, loss = 0.0270\n",
            "epoch44000, loss = 0.0270\n",
            "epoch44010, loss = 0.0270\n",
            "epoch44020, loss = 0.0270\n",
            "epoch44030, loss = 0.0270\n",
            "epoch44040, loss = 0.0270\n",
            "epoch44050, loss = 0.0270\n",
            "epoch44060, loss = 0.0270\n",
            "epoch44070, loss = 0.0270\n",
            "epoch44080, loss = 0.0270\n",
            "epoch44090, loss = 0.0270\n",
            "epoch44100, loss = 0.0270\n",
            "epoch44110, loss = 0.0270\n",
            "epoch44120, loss = 0.0270\n",
            "epoch44130, loss = 0.0270\n",
            "epoch44140, loss = 0.0270\n",
            "epoch44150, loss = 0.0270\n",
            "epoch44160, loss = 0.0270\n",
            "epoch44170, loss = 0.0270\n",
            "epoch44180, loss = 0.0270\n",
            "epoch44190, loss = 0.0270\n",
            "epoch44200, loss = 0.0270\n",
            "epoch44210, loss = 0.0270\n",
            "epoch44220, loss = 0.0270\n",
            "epoch44230, loss = 0.0270\n",
            "epoch44240, loss = 0.0270\n",
            "epoch44250, loss = 0.0270\n",
            "epoch44260, loss = 0.0270\n",
            "epoch44270, loss = 0.0270\n",
            "epoch44280, loss = 0.0269\n",
            "epoch44290, loss = 0.0269\n",
            "epoch44300, loss = 0.0269\n",
            "epoch44310, loss = 0.0269\n",
            "epoch44320, loss = 0.0269\n",
            "epoch44330, loss = 0.0269\n",
            "epoch44340, loss = 0.0269\n",
            "epoch44350, loss = 0.0269\n",
            "epoch44360, loss = 0.0269\n",
            "epoch44370, loss = 0.0269\n",
            "epoch44380, loss = 0.0269\n",
            "epoch44390, loss = 0.0269\n",
            "epoch44400, loss = 0.0269\n",
            "epoch44410, loss = 0.0269\n",
            "epoch44420, loss = 0.0269\n",
            "epoch44430, loss = 0.0269\n",
            "epoch44440, loss = 0.0269\n",
            "epoch44450, loss = 0.0269\n",
            "epoch44460, loss = 0.0269\n",
            "epoch44470, loss = 0.0269\n",
            "epoch44480, loss = 0.0269\n",
            "epoch44490, loss = 0.0269\n",
            "epoch44500, loss = 0.0269\n",
            "epoch44510, loss = 0.0269\n",
            "epoch44520, loss = 0.0269\n",
            "epoch44530, loss = 0.0269\n",
            "epoch44540, loss = 0.0269\n",
            "epoch44550, loss = 0.0269\n",
            "epoch44560, loss = 0.0269\n",
            "epoch44570, loss = 0.0269\n",
            "epoch44580, loss = 0.0269\n",
            "epoch44590, loss = 0.0269\n",
            "epoch44600, loss = 0.0269\n",
            "epoch44610, loss = 0.0269\n",
            "epoch44620, loss = 0.0269\n",
            "epoch44630, loss = 0.0269\n",
            "epoch44640, loss = 0.0269\n",
            "epoch44650, loss = 0.0269\n",
            "epoch44660, loss = 0.0269\n",
            "epoch44670, loss = 0.0269\n",
            "epoch44680, loss = 0.0269\n",
            "epoch44690, loss = 0.0269\n",
            "epoch44700, loss = 0.0269\n",
            "epoch44710, loss = 0.0269\n",
            "epoch44720, loss = 0.0269\n",
            "epoch44730, loss = 0.0269\n",
            "epoch44740, loss = 0.0269\n",
            "epoch44750, loss = 0.0269\n",
            "epoch44760, loss = 0.0269\n",
            "epoch44770, loss = 0.0269\n",
            "epoch44780, loss = 0.0269\n",
            "epoch44790, loss = 0.0269\n",
            "epoch44800, loss = 0.0269\n",
            "epoch44810, loss = 0.0269\n",
            "epoch44820, loss = 0.0269\n",
            "epoch44830, loss = 0.0269\n",
            "epoch44840, loss = 0.0269\n",
            "epoch44850, loss = 0.0269\n",
            "epoch44860, loss = 0.0269\n",
            "epoch44870, loss = 0.0269\n",
            "epoch44880, loss = 0.0269\n",
            "epoch44890, loss = 0.0269\n",
            "epoch44900, loss = 0.0269\n",
            "epoch44910, loss = 0.0269\n",
            "epoch44920, loss = 0.0269\n",
            "epoch44930, loss = 0.0269\n",
            "epoch44940, loss = 0.0269\n",
            "epoch44950, loss = 0.0268\n",
            "epoch44960, loss = 0.0268\n",
            "epoch44970, loss = 0.0268\n",
            "epoch44980, loss = 0.0268\n",
            "epoch44990, loss = 0.0268\n",
            "epoch45000, loss = 0.0268\n",
            "epoch45010, loss = 0.0268\n",
            "epoch45020, loss = 0.0268\n",
            "epoch45030, loss = 0.0268\n",
            "epoch45040, loss = 0.0268\n",
            "epoch45050, loss = 0.0268\n",
            "epoch45060, loss = 0.0268\n",
            "epoch45070, loss = 0.0268\n",
            "epoch45080, loss = 0.0268\n",
            "epoch45090, loss = 0.0268\n",
            "epoch45100, loss = 0.0268\n",
            "epoch45110, loss = 0.0268\n",
            "epoch45120, loss = 0.0268\n",
            "epoch45130, loss = 0.0268\n",
            "epoch45140, loss = 0.0268\n",
            "epoch45150, loss = 0.0268\n",
            "epoch45160, loss = 0.0268\n",
            "epoch45170, loss = 0.0268\n",
            "epoch45180, loss = 0.0268\n",
            "epoch45190, loss = 0.0268\n",
            "epoch45200, loss = 0.0268\n",
            "epoch45210, loss = 0.0268\n",
            "epoch45220, loss = 0.0268\n",
            "epoch45230, loss = 0.0268\n",
            "epoch45240, loss = 0.0268\n",
            "epoch45250, loss = 0.0268\n",
            "epoch45260, loss = 0.0268\n",
            "epoch45270, loss = 0.0268\n",
            "epoch45280, loss = 0.0268\n",
            "epoch45290, loss = 0.0268\n",
            "epoch45300, loss = 0.0268\n",
            "epoch45310, loss = 0.0268\n",
            "epoch45320, loss = 0.0268\n",
            "epoch45330, loss = 0.0268\n",
            "epoch45340, loss = 0.0268\n",
            "epoch45350, loss = 0.0268\n",
            "epoch45360, loss = 0.0268\n",
            "epoch45370, loss = 0.0268\n",
            "epoch45380, loss = 0.0268\n",
            "epoch45390, loss = 0.0268\n",
            "epoch45400, loss = 0.0268\n",
            "epoch45410, loss = 0.0268\n",
            "epoch45420, loss = 0.0268\n",
            "epoch45430, loss = 0.0268\n",
            "epoch45440, loss = 0.0268\n",
            "epoch45450, loss = 0.0268\n",
            "epoch45460, loss = 0.0268\n",
            "epoch45470, loss = 0.0268\n",
            "epoch45480, loss = 0.0268\n",
            "epoch45490, loss = 0.0268\n",
            "epoch45500, loss = 0.0268\n",
            "epoch45510, loss = 0.0268\n",
            "epoch45520, loss = 0.0268\n",
            "epoch45530, loss = 0.0268\n",
            "epoch45540, loss = 0.0268\n",
            "epoch45550, loss = 0.0268\n",
            "epoch45560, loss = 0.0268\n",
            "epoch45570, loss = 0.0268\n",
            "epoch45580, loss = 0.0268\n",
            "epoch45590, loss = 0.0268\n",
            "epoch45600, loss = 0.0268\n",
            "epoch45610, loss = 0.0268\n",
            "epoch45620, loss = 0.0268\n",
            "epoch45630, loss = 0.0268\n",
            "epoch45640, loss = 0.0267\n",
            "epoch45650, loss = 0.0267\n",
            "epoch45660, loss = 0.0267\n",
            "epoch45670, loss = 0.0267\n",
            "epoch45680, loss = 0.0267\n",
            "epoch45690, loss = 0.0267\n",
            "epoch45700, loss = 0.0267\n",
            "epoch45710, loss = 0.0267\n",
            "epoch45720, loss = 0.0267\n",
            "epoch45730, loss = 0.0267\n",
            "epoch45740, loss = 0.0267\n",
            "epoch45750, loss = 0.0267\n",
            "epoch45760, loss = 0.0267\n",
            "epoch45770, loss = 0.0267\n",
            "epoch45780, loss = 0.0267\n",
            "epoch45790, loss = 0.0267\n",
            "epoch45800, loss = 0.0267\n",
            "epoch45810, loss = 0.0267\n",
            "epoch45820, loss = 0.0267\n",
            "epoch45830, loss = 0.0267\n",
            "epoch45840, loss = 0.0267\n",
            "epoch45850, loss = 0.0267\n",
            "epoch45860, loss = 0.0267\n",
            "epoch45870, loss = 0.0267\n",
            "epoch45880, loss = 0.0267\n",
            "epoch45890, loss = 0.0267\n",
            "epoch45900, loss = 0.0267\n",
            "epoch45910, loss = 0.0267\n",
            "epoch45920, loss = 0.0267\n",
            "epoch45930, loss = 0.0267\n",
            "epoch45940, loss = 0.0267\n",
            "epoch45950, loss = 0.0267\n",
            "epoch45960, loss = 0.0267\n",
            "epoch45970, loss = 0.0267\n",
            "epoch45980, loss = 0.0267\n",
            "epoch45990, loss = 0.0267\n",
            "epoch46000, loss = 0.0267\n",
            "epoch46010, loss = 0.0267\n",
            "epoch46020, loss = 0.0267\n",
            "epoch46030, loss = 0.0267\n",
            "epoch46040, loss = 0.0267\n",
            "epoch46050, loss = 0.0267\n",
            "epoch46060, loss = 0.0267\n",
            "epoch46070, loss = 0.0267\n",
            "epoch46080, loss = 0.0267\n",
            "epoch46090, loss = 0.0267\n",
            "epoch46100, loss = 0.0267\n",
            "epoch46110, loss = 0.0267\n",
            "epoch46120, loss = 0.0267\n",
            "epoch46130, loss = 0.0267\n",
            "epoch46140, loss = 0.0267\n",
            "epoch46150, loss = 0.0267\n",
            "epoch46160, loss = 0.0267\n",
            "epoch46170, loss = 0.0267\n",
            "epoch46180, loss = 0.0267\n",
            "epoch46190, loss = 0.0267\n",
            "epoch46200, loss = 0.0267\n",
            "epoch46210, loss = 0.0267\n",
            "epoch46220, loss = 0.0267\n",
            "epoch46230, loss = 0.0267\n",
            "epoch46240, loss = 0.0267\n",
            "epoch46250, loss = 0.0267\n",
            "epoch46260, loss = 0.0267\n",
            "epoch46270, loss = 0.0267\n",
            "epoch46280, loss = 0.0267\n",
            "epoch46290, loss = 0.0267\n",
            "epoch46300, loss = 0.0267\n",
            "epoch46310, loss = 0.0267\n",
            "epoch46320, loss = 0.0267\n",
            "epoch46330, loss = 0.0267\n",
            "epoch46340, loss = 0.0266\n",
            "epoch46350, loss = 0.0266\n",
            "epoch46360, loss = 0.0266\n",
            "epoch46370, loss = 0.0266\n",
            "epoch46380, loss = 0.0266\n",
            "epoch46390, loss = 0.0266\n",
            "epoch46400, loss = 0.0266\n",
            "epoch46410, loss = 0.0266\n",
            "epoch46420, loss = 0.0266\n",
            "epoch46430, loss = 0.0266\n",
            "epoch46440, loss = 0.0266\n",
            "epoch46450, loss = 0.0266\n",
            "epoch46460, loss = 0.0266\n",
            "epoch46470, loss = 0.0266\n",
            "epoch46480, loss = 0.0266\n",
            "epoch46490, loss = 0.0266\n",
            "epoch46500, loss = 0.0266\n",
            "epoch46510, loss = 0.0266\n",
            "epoch46520, loss = 0.0266\n",
            "epoch46530, loss = 0.0266\n",
            "epoch46540, loss = 0.0266\n",
            "epoch46550, loss = 0.0266\n",
            "epoch46560, loss = 0.0266\n",
            "epoch46570, loss = 0.0266\n",
            "epoch46580, loss = 0.0266\n",
            "epoch46590, loss = 0.0266\n",
            "epoch46600, loss = 0.0266\n",
            "epoch46610, loss = 0.0266\n",
            "epoch46620, loss = 0.0266\n",
            "epoch46630, loss = 0.0266\n",
            "epoch46640, loss = 0.0266\n",
            "epoch46650, loss = 0.0266\n",
            "epoch46660, loss = 0.0266\n",
            "epoch46670, loss = 0.0266\n",
            "epoch46680, loss = 0.0266\n",
            "epoch46690, loss = 0.0266\n",
            "epoch46700, loss = 0.0266\n",
            "epoch46710, loss = 0.0266\n",
            "epoch46720, loss = 0.0266\n",
            "epoch46730, loss = 0.0266\n",
            "epoch46740, loss = 0.0266\n",
            "epoch46750, loss = 0.0266\n",
            "epoch46760, loss = 0.0266\n",
            "epoch46770, loss = 0.0266\n",
            "epoch46780, loss = 0.0266\n",
            "epoch46790, loss = 0.0266\n",
            "epoch46800, loss = 0.0266\n",
            "epoch46810, loss = 0.0266\n",
            "epoch46820, loss = 0.0266\n",
            "epoch46830, loss = 0.0266\n",
            "epoch46840, loss = 0.0266\n",
            "epoch46850, loss = 0.0266\n",
            "epoch46860, loss = 0.0266\n",
            "epoch46870, loss = 0.0266\n",
            "epoch46880, loss = 0.0266\n",
            "epoch46890, loss = 0.0266\n",
            "epoch46900, loss = 0.0266\n",
            "epoch46910, loss = 0.0266\n",
            "epoch46920, loss = 0.0266\n",
            "epoch46930, loss = 0.0266\n",
            "epoch46940, loss = 0.0266\n",
            "epoch46950, loss = 0.0266\n",
            "epoch46960, loss = 0.0266\n",
            "epoch46970, loss = 0.0266\n",
            "epoch46980, loss = 0.0266\n",
            "epoch46990, loss = 0.0266\n",
            "epoch47000, loss = 0.0266\n",
            "epoch47010, loss = 0.0266\n",
            "epoch47020, loss = 0.0266\n",
            "epoch47030, loss = 0.0266\n",
            "epoch47040, loss = 0.0266\n",
            "epoch47050, loss = 0.0265\n",
            "epoch47060, loss = 0.0265\n",
            "epoch47070, loss = 0.0265\n",
            "epoch47080, loss = 0.0265\n",
            "epoch47090, loss = 0.0265\n",
            "epoch47100, loss = 0.0265\n",
            "epoch47110, loss = 0.0265\n",
            "epoch47120, loss = 0.0265\n",
            "epoch47130, loss = 0.0265\n",
            "epoch47140, loss = 0.0265\n",
            "epoch47150, loss = 0.0265\n",
            "epoch47160, loss = 0.0265\n",
            "epoch47170, loss = 0.0265\n",
            "epoch47180, loss = 0.0265\n",
            "epoch47190, loss = 0.0265\n",
            "epoch47200, loss = 0.0265\n",
            "epoch47210, loss = 0.0265\n",
            "epoch47220, loss = 0.0265\n",
            "epoch47230, loss = 0.0265\n",
            "epoch47240, loss = 0.0265\n",
            "epoch47250, loss = 0.0265\n",
            "epoch47260, loss = 0.0265\n",
            "epoch47270, loss = 0.0265\n",
            "epoch47280, loss = 0.0265\n",
            "epoch47290, loss = 0.0265\n",
            "epoch47300, loss = 0.0265\n",
            "epoch47310, loss = 0.0265\n",
            "epoch47320, loss = 0.0265\n",
            "epoch47330, loss = 0.0265\n",
            "epoch47340, loss = 0.0265\n",
            "epoch47350, loss = 0.0265\n",
            "epoch47360, loss = 0.0265\n",
            "epoch47370, loss = 0.0265\n",
            "epoch47380, loss = 0.0265\n",
            "epoch47390, loss = 0.0265\n",
            "epoch47400, loss = 0.0265\n",
            "epoch47410, loss = 0.0265\n",
            "epoch47420, loss = 0.0265\n",
            "epoch47430, loss = 0.0265\n",
            "epoch47440, loss = 0.0265\n",
            "epoch47450, loss = 0.0265\n",
            "epoch47460, loss = 0.0265\n",
            "epoch47470, loss = 0.0265\n",
            "epoch47480, loss = 0.0265\n",
            "epoch47490, loss = 0.0265\n",
            "epoch47500, loss = 0.0265\n",
            "epoch47510, loss = 0.0265\n",
            "epoch47520, loss = 0.0265\n",
            "epoch47530, loss = 0.0265\n",
            "epoch47540, loss = 0.0265\n",
            "epoch47550, loss = 0.0265\n",
            "epoch47560, loss = 0.0265\n",
            "epoch47570, loss = 0.0265\n",
            "epoch47580, loss = 0.0265\n",
            "epoch47590, loss = 0.0265\n",
            "epoch47600, loss = 0.0265\n",
            "epoch47610, loss = 0.0265\n",
            "epoch47620, loss = 0.0265\n",
            "epoch47630, loss = 0.0265\n",
            "epoch47640, loss = 0.0265\n",
            "epoch47650, loss = 0.0265\n",
            "epoch47660, loss = 0.0265\n",
            "epoch47670, loss = 0.0265\n",
            "epoch47680, loss = 0.0265\n",
            "epoch47690, loss = 0.0265\n",
            "epoch47700, loss = 0.0265\n",
            "epoch47710, loss = 0.0265\n",
            "epoch47720, loss = 0.0265\n",
            "epoch47730, loss = 0.0265\n",
            "epoch47740, loss = 0.0265\n",
            "epoch47750, loss = 0.0265\n",
            "epoch47760, loss = 0.0265\n",
            "epoch47770, loss = 0.0265\n",
            "epoch47780, loss = 0.0264\n",
            "epoch47790, loss = 0.0264\n",
            "epoch47800, loss = 0.0264\n",
            "epoch47810, loss = 0.0264\n",
            "epoch47820, loss = 0.0264\n",
            "epoch47830, loss = 0.0264\n",
            "epoch47840, loss = 0.0264\n",
            "epoch47850, loss = 0.0264\n",
            "epoch47860, loss = 0.0264\n",
            "epoch47870, loss = 0.0264\n",
            "epoch47880, loss = 0.0264\n",
            "epoch47890, loss = 0.0264\n",
            "epoch47900, loss = 0.0264\n",
            "epoch47910, loss = 0.0264\n",
            "epoch47920, loss = 0.0264\n",
            "epoch47930, loss = 0.0264\n",
            "epoch47940, loss = 0.0264\n",
            "epoch47950, loss = 0.0264\n",
            "epoch47960, loss = 0.0264\n",
            "epoch47970, loss = 0.0264\n",
            "epoch47980, loss = 0.0264\n",
            "epoch47990, loss = 0.0264\n",
            "epoch48000, loss = 0.0264\n",
            "epoch48010, loss = 0.0264\n",
            "epoch48020, loss = 0.0264\n",
            "epoch48030, loss = 0.0264\n",
            "epoch48040, loss = 0.0264\n",
            "epoch48050, loss = 0.0264\n",
            "epoch48060, loss = 0.0264\n",
            "epoch48070, loss = 0.0264\n",
            "epoch48080, loss = 0.0264\n",
            "epoch48090, loss = 0.0264\n",
            "epoch48100, loss = 0.0264\n",
            "epoch48110, loss = 0.0264\n",
            "epoch48120, loss = 0.0264\n",
            "epoch48130, loss = 0.0264\n",
            "epoch48140, loss = 0.0264\n",
            "epoch48150, loss = 0.0264\n",
            "epoch48160, loss = 0.0264\n",
            "epoch48170, loss = 0.0264\n",
            "epoch48180, loss = 0.0264\n",
            "epoch48190, loss = 0.0264\n",
            "epoch48200, loss = 0.0264\n",
            "epoch48210, loss = 0.0264\n",
            "epoch48220, loss = 0.0264\n",
            "epoch48230, loss = 0.0264\n",
            "epoch48240, loss = 0.0264\n",
            "epoch48250, loss = 0.0264\n",
            "epoch48260, loss = 0.0264\n",
            "epoch48270, loss = 0.0264\n",
            "epoch48280, loss = 0.0264\n",
            "epoch48290, loss = 0.0264\n",
            "epoch48300, loss = 0.0264\n",
            "epoch48310, loss = 0.0264\n",
            "epoch48320, loss = 0.0264\n",
            "epoch48330, loss = 0.0264\n",
            "epoch48340, loss = 0.0264\n",
            "epoch48350, loss = 0.0264\n",
            "epoch48360, loss = 0.0264\n",
            "epoch48370, loss = 0.0264\n",
            "epoch48380, loss = 0.0264\n",
            "epoch48390, loss = 0.0264\n",
            "epoch48400, loss = 0.0264\n",
            "epoch48410, loss = 0.0264\n",
            "epoch48420, loss = 0.0264\n",
            "epoch48430, loss = 0.0264\n",
            "epoch48440, loss = 0.0264\n",
            "epoch48450, loss = 0.0264\n",
            "epoch48460, loss = 0.0264\n",
            "epoch48470, loss = 0.0264\n",
            "epoch48480, loss = 0.0264\n",
            "epoch48490, loss = 0.0264\n",
            "epoch48500, loss = 0.0264\n",
            "epoch48510, loss = 0.0264\n",
            "epoch48520, loss = 0.0263\n",
            "epoch48530, loss = 0.0263\n",
            "epoch48540, loss = 0.0263\n",
            "epoch48550, loss = 0.0263\n",
            "epoch48560, loss = 0.0263\n",
            "epoch48570, loss = 0.0263\n",
            "epoch48580, loss = 0.0263\n",
            "epoch48590, loss = 0.0263\n",
            "epoch48600, loss = 0.0263\n",
            "epoch48610, loss = 0.0263\n",
            "epoch48620, loss = 0.0263\n",
            "epoch48630, loss = 0.0263\n",
            "epoch48640, loss = 0.0263\n",
            "epoch48650, loss = 0.0263\n",
            "epoch48660, loss = 0.0263\n",
            "epoch48670, loss = 0.0263\n",
            "epoch48680, loss = 0.0263\n",
            "epoch48690, loss = 0.0263\n",
            "epoch48700, loss = 0.0263\n",
            "epoch48710, loss = 0.0263\n",
            "epoch48720, loss = 0.0263\n",
            "epoch48730, loss = 0.0263\n",
            "epoch48740, loss = 0.0263\n",
            "epoch48750, loss = 0.0263\n",
            "epoch48760, loss = 0.0263\n",
            "epoch48770, loss = 0.0263\n",
            "epoch48780, loss = 0.0263\n",
            "epoch48790, loss = 0.0263\n",
            "epoch48800, loss = 0.0263\n",
            "epoch48810, loss = 0.0263\n",
            "epoch48820, loss = 0.0263\n",
            "epoch48830, loss = 0.0263\n",
            "epoch48840, loss = 0.0263\n",
            "epoch48850, loss = 0.0263\n",
            "epoch48860, loss = 0.0263\n",
            "epoch48870, loss = 0.0263\n",
            "epoch48880, loss = 0.0263\n",
            "epoch48890, loss = 0.0263\n",
            "epoch48900, loss = 0.0263\n",
            "epoch48910, loss = 0.0263\n",
            "epoch48920, loss = 0.0263\n",
            "epoch48930, loss = 0.0263\n",
            "epoch48940, loss = 0.0263\n",
            "epoch48950, loss = 0.0263\n",
            "epoch48960, loss = 0.0263\n",
            "epoch48970, loss = 0.0263\n",
            "epoch48980, loss = 0.0263\n",
            "epoch48990, loss = 0.0263\n",
            "epoch49000, loss = 0.0263\n",
            "epoch49010, loss = 0.0263\n",
            "epoch49020, loss = 0.0263\n",
            "epoch49030, loss = 0.0263\n",
            "epoch49040, loss = 0.0263\n",
            "epoch49050, loss = 0.0263\n",
            "epoch49060, loss = 0.0263\n",
            "epoch49070, loss = 0.0263\n",
            "epoch49080, loss = 0.0263\n",
            "epoch49090, loss = 0.0263\n",
            "epoch49100, loss = 0.0263\n",
            "epoch49110, loss = 0.0263\n",
            "epoch49120, loss = 0.0263\n",
            "epoch49130, loss = 0.0263\n",
            "epoch49140, loss = 0.0263\n",
            "epoch49150, loss = 0.0263\n",
            "epoch49160, loss = 0.0263\n",
            "epoch49170, loss = 0.0263\n",
            "epoch49180, loss = 0.0263\n",
            "epoch49190, loss = 0.0263\n",
            "epoch49200, loss = 0.0263\n",
            "epoch49210, loss = 0.0263\n",
            "epoch49220, loss = 0.0263\n",
            "epoch49230, loss = 0.0263\n",
            "epoch49240, loss = 0.0263\n",
            "epoch49250, loss = 0.0263\n",
            "epoch49260, loss = 0.0263\n",
            "epoch49270, loss = 0.0263\n",
            "epoch49280, loss = 0.0262\n",
            "epoch49290, loss = 0.0262\n",
            "epoch49300, loss = 0.0262\n",
            "epoch49310, loss = 0.0262\n",
            "epoch49320, loss = 0.0262\n",
            "epoch49330, loss = 0.0262\n",
            "epoch49340, loss = 0.0262\n",
            "epoch49350, loss = 0.0262\n",
            "epoch49360, loss = 0.0262\n",
            "epoch49370, loss = 0.0262\n",
            "epoch49380, loss = 0.0262\n",
            "epoch49390, loss = 0.0262\n",
            "epoch49400, loss = 0.0262\n",
            "epoch49410, loss = 0.0262\n",
            "epoch49420, loss = 0.0262\n",
            "epoch49430, loss = 0.0262\n",
            "epoch49440, loss = 0.0262\n",
            "epoch49450, loss = 0.0262\n",
            "epoch49460, loss = 0.0262\n",
            "epoch49470, loss = 0.0262\n",
            "epoch49480, loss = 0.0262\n",
            "epoch49490, loss = 0.0262\n",
            "epoch49500, loss = 0.0262\n",
            "epoch49510, loss = 0.0262\n",
            "epoch49520, loss = 0.0262\n",
            "epoch49530, loss = 0.0262\n",
            "epoch49540, loss = 0.0262\n",
            "epoch49550, loss = 0.0262\n",
            "epoch49560, loss = 0.0262\n",
            "epoch49570, loss = 0.0262\n",
            "epoch49580, loss = 0.0262\n",
            "epoch49590, loss = 0.0262\n",
            "epoch49600, loss = 0.0262\n",
            "epoch49610, loss = 0.0262\n",
            "epoch49620, loss = 0.0262\n",
            "epoch49630, loss = 0.0262\n",
            "epoch49640, loss = 0.0262\n",
            "epoch49650, loss = 0.0262\n",
            "epoch49660, loss = 0.0262\n",
            "epoch49670, loss = 0.0262\n",
            "epoch49680, loss = 0.0262\n",
            "epoch49690, loss = 0.0262\n",
            "epoch49700, loss = 0.0262\n",
            "epoch49710, loss = 0.0262\n",
            "epoch49720, loss = 0.0262\n",
            "epoch49730, loss = 0.0262\n",
            "epoch49740, loss = 0.0262\n",
            "epoch49750, loss = 0.0262\n",
            "epoch49760, loss = 0.0262\n",
            "epoch49770, loss = 0.0262\n",
            "epoch49780, loss = 0.0262\n",
            "epoch49790, loss = 0.0262\n",
            "epoch49800, loss = 0.0262\n",
            "epoch49810, loss = 0.0262\n",
            "epoch49820, loss = 0.0262\n",
            "epoch49830, loss = 0.0262\n",
            "epoch49840, loss = 0.0262\n",
            "epoch49850, loss = 0.0262\n",
            "epoch49860, loss = 0.0262\n",
            "epoch49870, loss = 0.0262\n",
            "epoch49880, loss = 0.0262\n",
            "epoch49890, loss = 0.0262\n",
            "epoch49900, loss = 0.0262\n",
            "epoch49910, loss = 0.0262\n",
            "epoch49920, loss = 0.0262\n",
            "epoch49930, loss = 0.0262\n",
            "epoch49940, loss = 0.0262\n",
            "epoch49950, loss = 0.0262\n",
            "epoch49960, loss = 0.0262\n",
            "epoch49970, loss = 0.0262\n",
            "epoch49980, loss = 0.0262\n",
            "epoch49990, loss = 0.0262\n",
            "epoch50000, loss = 0.0262\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "type acc is:  <class 'torch.Tensor'>\n",
            "accuracy = 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9-Dataset Dataloader**"
      ],
      "metadata": {
        "id": "sR20XV2xsvWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Design model (input, output size, forward pass)\n",
        "# 2) Construct loss and optimiser\n",
        "# 3) Training loop\n",
        "#   - forward pass: compute prediction and loss\n",
        "#   - backward pass: gradients\n",
        "#   - update weights\n",
        "\n",
        "\n",
        "# a better way to use the dataset is to divide the whole training data set into smaller batches\n",
        "# this way we loop over the epochs again and then we loop over all the batches\n",
        "# and then we get the x and y batch and we do the optimization based only on those batches\n",
        "# pytorch can do the batch calculatio and iteration for us\n",
        "\n",
        "# TERMS #\n",
        "# epoch = 1 forward and backward pass of ALL training samples\n",
        "# batch_size = number of training samples in one forward and backward pass\n",
        "# number of iterations = number of passes, each pass using [batch_size] number of samples\n",
        "# e.g. 100 samples, batch_size = 20 --> 100/20 = 5 iterations for 1 epoch\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# we can start implementing our own dataset which must inherit the pytorch Dataset class\n",
        "DATA_SRC = 'C:\\\\temp\\\\python\\\\python_working_with_data\\\\pytorchTutorial-master\\\\data\\\\wine\\\\wine.csv'\n",
        "# we want to predict the wine categories, we have 3 categoroes 1, 2, 3 in the first colum, the other columns are the features\n",
        "# let's load and split our columnts into X and y\n",
        "\n",
        "\n",
        "\n",
        "class WineDataset(Dataset):\n",
        "    # we must implement 3 things:\n",
        "    def __init__(self):\n",
        "        # data loading\n",
        "        xy = np.loadtxt(DATA_SRC, delimiter=',', dtype = np.float32, skiprows=1)\n",
        "        # let's split the whole dataset into X and y\n",
        "        # we are using slicing, we want all the samples but we do not want the first column in the inner array -> 1: from the first column all the way till the end\n",
        "        self.x = torch.from_numpy(xy[:, 1:]) # we also convert to torch\n",
        "        # for y we want tbhe first column and we want to wrp ainside an array as I want an array of arrays (a colmun vector) as opposed to an array of values\n",
        "        self.y = torch.from_numpy(xy[:, [0]]) # this way we have a column vectoer which has size n_samples, 1\n",
        "        # we also get the number of samples\n",
        "        self.n_samples = xy.shape[0] # number of samples is the first numebr of the tuple returned by shape\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index], self.y[index] # this will return a tuple\n",
        "        # this will allow for indexing later, we can call dataset with an index e.g. dataset[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "        # this will return the length of the dataset\n",
        "        # len(dataset)\n",
        "\n",
        "\n",
        "\n",
        "dataset = WineDataset()\n",
        "\n",
        "# let's have a look at the very first sample\n",
        "first_data = dataset[0]\n",
        "print(first_data)\n",
        "print(type(first_data)) # data type is a tuple of torch tensors\n",
        "features, labels = first_data[0], first_data[1]\n",
        "print(features) # data type is a tensor\n",
        "print(labels) # data type is a tensor with just one number\n",
        "\n",
        "# now we are going to use a dataloader\n",
        "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True) # num_workers set to 2 might make the process faster as it is using multiple sub-processes. shuffle set to true is very useful durign training. num_workers=2 creates an issue on this PC\n",
        "\n",
        "\n",
        "# we are going to see an example of data in the dataloader\n",
        "# now we are using this data loader and we will convert it to an iterator\n",
        "datatiter = iter(dataloader)\n",
        "\n",
        "# now we call a next function\n",
        "# data = datatiter.next() this i sthe syntax in pytorch 1.12\n",
        "# from pytorch 1.13 the only working syntax is:\n",
        "data = next(datatiter)\n",
        "# we will unpack it again\n",
        "print(data)\n",
        "print(type(data))\n",
        "features, labels = data[0], data[1] # i could have wriiten this as:  features, labels = data\n",
        "print(features)\n",
        "print(type(features))\n",
        "print(labels)\n",
        "print(type(labels))\n",
        "\n",
        "\n",
        "# let's define some hyper-parameters\n",
        "num_epochs = 2\n",
        "total_samples = len(dataset)\n",
        "batch_size = 4\n",
        "# lets get the number of iterations for each epoch (this will be total number of sampela divided by the bacth size)\n",
        "n_iterations = math.ceil(total_samples/batch_size) # we need to use match cei as without we would get 44.5 iteration, with math.ceil this will be 45\n",
        "print('total sample: ', total_samples, ' number iterations: ', n_iterations)\n",
        "\n",
        "# let's write some dummy training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (inputs, labels) in enumerate(dataloader):\n",
        "        # forward backward, upadte\n",
        "        if (i+1) % 5 == 0: # every 5th step I want to print some information\n",
        "            print(f'epoch {epoch + 1 }/{num_epochs}, step {i+1}/{n_iterations}, inputs {inputs.shape}')\n",
        "\n",
        "\n",
        "\n",
        "# pytorch has some built in datasets,\n",
        "\n",
        "dataset = torchvision.datasets.MNIST('aaaa', download=True)\n",
        "print(dataset)\n",
        "print(type(dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "TCUTSAP9sx25",
        "outputId": "19e64102-9314-4d73-e4af-4fcaf3878663"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-8aee1dd71611>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWineDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# let's have a look at the very first sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-8aee1dd71611>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# data loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_SRC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;31m# let's split the whole dataset into X and y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# we are using slicing, we want all the samples but we do not want the first column in the inner array -> 1: from the first column all the way till the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1336\u001b[0m         \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m     arr = _read(fname, dtype=dtype, comment=comment, delimiter=delimiter,\n\u001b[0m\u001b[1;32m   1339\u001b[0m                 \u001b[0mconverters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiplines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m                 \u001b[0munpack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    531\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    532\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path} not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: C:\\temp\\python\\python_working_with_data\\pytorchTutorial-master\\data\\wine\\wine.csv not found."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10-Dataset Transform**"
      ],
      "metadata": {
        "id": "xcCRETMks2nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "# if we use a torch built in dataset we can pass a transform argument to concert to Tensor and apply some transforms\n",
        "# in this case we use the MNSIT dataset and we aplly the ToTensor() transform whi will convert numpy arrays or images into Tensor\n",
        "# pytorch has alrealy a lot of transform implemented for us, the official documentation is:\n",
        "# https://pytorch.org/vision/stable/transforms.html\n",
        "\n",
        "Transforms can be applied to PIL images, tensors, ndarrays, or custom data\n",
        "during creation of the DataSet\n",
        "\n",
        "On Images\n",
        "---------\n",
        "CenterCrop, Grayscale, Pad, RandomAffine\n",
        "RandomCrop, RandomHorizontalFlip, RandomRotation\n",
        "Resize, Scale\n",
        "\n",
        "On Tensors\n",
        "----------\n",
        "LinearTransformation, Normalize, RandomErasing\n",
        "\n",
        "Conversion\n",
        "----------\n",
        "ToPILImage: from tensor or ndrarray\n",
        "ToTensor : from numpy.ndarray or PILImage\n",
        "\n",
        "Generic\n",
        "-------\n",
        "Use Lambda\n",
        "\n",
        "Custom\n",
        "------\n",
        "Write own class\n",
        "\n",
        "Compose multiple Transforms\n",
        "---------------------------\n",
        "composed = transforms.Compose([Rescale(256),\n",
        "                               RandomCrop(224)])\n",
        "'''\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# in the last tutorial we created a custom WineDataset class, now we are extending this class to apply our own transforms and apply our own transfer classes\n",
        "\n",
        "\n",
        "\n",
        "# we can start implementing our own dataset which must inherit the pytorch Dataset class\n",
        "DATA_SRC = 'C:\\\\temp\\\\python\\\\python_working_with_data\\\\pytorchTutorial-master\\\\data\\\\wine\\\\wine.csv'\n",
        "# we want to predict the wine categories, we have 3 categoroes 1, 2, 3 in the first colum, the other columns are the features\n",
        "# let's load and split our columnts into X and y\n",
        "\n",
        "# this class will load the data in the init constructor method, then we implemented the getitem whci will allow indexing\n",
        "# now we will extend the dataset, so our class should also support the transform argument which is optional so at the beginnign we say None\n",
        "# we want to make a change to the getitem method, so we can apply a transfor if available.\n",
        "class WineDataset(Dataset):\n",
        "    # we must implement 3 things:\n",
        "    def __init__(self, transform = None):\n",
        "        # data loading\n",
        "        xy = np.loadtxt(DATA_SRC, delimiter=',', dtype = np.float32, skiprows=1)\n",
        "        # let's split the whole dataset into X and y\n",
        "        # we are using slicing, we want all the samples but we do not want the first column in the inner array -> 1: from the first column all the way till the end\n",
        "        # self.x = torch.from_numpy(xy[:, 1:]) # we also convert to torch\n",
        "        # for y we want tbhe first column and we want to wrp ainside an array as I want an array of arrays (a colmun vector) as opposed to an array of values\n",
        "        # self.y = torch.from_numpy(xy[:, [0]]) # this way we have a column vectoer which has size n_samples, 1\n",
        "        # we also get the number of samples\n",
        "\n",
        "        # we are not converting to tensor here as we are going to use our custom transofrm class ToTensor()\n",
        "        self.x = xy[:, 1:]\n",
        "        self.y = xy[:, [0]]\n",
        "        self.n_samples = xy.shape[0] # number of samples is the first numebr of the tuple returned by shape\n",
        "\n",
        "        self.transform = transform # by default this attribute can be None\n",
        "\n",
        "    # we want to make a change to the getitem method, so we can apply a transfor if available.\n",
        "    def __getitem__(self, index):\n",
        "        # with tranform I am going to mage this change\n",
        "        # return self.x[index], self.y[index] # this will return a tuple\n",
        "        # this will allow for indexing later, we can call dataset with an index e.g. dataset[0]\n",
        "        sample = self.x[index], self.y[index]\n",
        "\n",
        "        if self.transform: # if transform not None (python considers None equal to False)\n",
        "            sample = self.transform(sample)\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "        # this will return the length of the dataset\n",
        "        # len(dataset)\n",
        "\n",
        "    # we can now create some custom transform classes\n",
        "    # we can write out own ToTensor class\n",
        "    # the only thing we need to implement is the __call__method\n",
        "class ToTensor():\n",
        "    def __call__(self, sample): # this will became a callable object which will perform an action when it is called\n",
        "        # we unpack our sample\n",
        "        inputs, targets = sample\n",
        "        return torch.from_numpy(inputs), torch.from_numpy(targets) # we will return a tuple\n",
        "\n",
        "\n",
        "# now we can use the transform in our dataset\n",
        "dataset = WineDataset(transform = None)\n",
        "first_data = dataset[0]\n",
        "features, labels = first_data\n",
        "print(features)\n",
        "print(type(features), type(labels))\n",
        "\n",
        "\n",
        "# now we can use the transform in our dataset\n",
        "dataset = WineDataset(transform = ToTensor())\n",
        "first_data = dataset[0]\n",
        "features, labels = first_data\n",
        "print(type(features), type(labels))\n",
        "\n",
        "# if we set the atrribute transform to None the datatype wil lbe a numpy ndarray\n",
        "\n",
        "#let's write another tranformation class\n",
        "class MulTransform():\n",
        "    def __init__(self, factor):\n",
        "        self.factor = factor # the constructor takes one parameter which is the number we are mutiplying for\n",
        "\n",
        "    def __call__(self, sample):\n",
        "          inputs, targets = sample\n",
        "          # we will only multiply the features\n",
        "          inputs *= self.factor\n",
        "          return inputs, targets\n",
        "\n",
        "\n",
        "# let's aplly a composed transfom to use both transform classes\n",
        "# we pass a list with all our transforms\n",
        "composed = torchvision.transforms.Compose([ToTensor(), MulTransform(2)])\n",
        "\n",
        "dataset = WineDataset(transform=composed)\n",
        "first_data = dataset[0]\n",
        "features, labels = first_data\n",
        "print(features) # each value was doubled with respect to the dataset where transform is set to None\n",
        "print(type(features), type(labels))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q0uN1ZBvs6li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11-Softmax Cross Entropy**"
      ],
      "metadata": {
        "id": "_czDOwlfs-lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# softmax basically squashes the output to be between 0 and 1\n",
        "# say we have a linear layer which return 2.0, 1.0, 0.1. we then apply the softmaw which squashes these values to 0.7, 0.2, 0.1 (returns the probability of each value)\n",
        "# we can then choose the class with the highest probability\n",
        "\n",
        "# let's see an implementatio of the softmax function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    return np.exp(x)/np.sum(np.exp(x), axis = 0)\n",
        "    # for a 1 dimentional array axis=0 sums all the values, for a 2-min array sums all the values vertically (axis=1 horizontally)\n",
        "    # https://stackoverflow.com/questions/40200070/what-does-axis-0-do-in-numpys-sum-function\n",
        "\n",
        "\n",
        "x = np.array([2.0, 1.0, 0.1])\n",
        "outputs = softmax(x)\n",
        "print('softmax numpy', outputs)\n",
        "\n",
        "# we can calculate the softmax in pytorch\n",
        "x = torch.tensor([2.0, 1.0, 0.1], dtype = torch.float32)\n",
        "outputs = torch.softmax(x, dim=0) # we need to specify the direction so it computes along the first axis\n",
        "print('softmax torch', outputs)\n",
        "\n",
        "# the softmax is often combined with the cross entropy loss\n",
        "# cross-entropy can be used in an multu class problem\n",
        "# the cross-entropy is a measure of how far we are from the one hot encoding vector, the farther the higher the cross entropy\n",
        "\n",
        "# an implementation of the cross entropy functoin with numpy is as follows\n",
        "\n",
        "def cross_entropy(actual, predicted):\n",
        "    loss = -np.sum(actual * np.log(predicted))\n",
        "    return loss # float(predicted.shape[0]) - we could normalize dividing by the number of samples but we are not doing it here\n",
        "\n",
        "# y must be one hot encoded\n",
        "# if class 0: [1, 0, 0]\n",
        "# if class 1: [0, 1, 0]\n",
        "# if class 2: [0, 0, 1]\n",
        "Y = np.array([1, 0, 0])\n",
        "\n",
        "# y_pred has probabilities\n",
        "Y_pred_good = np.array([0.7, 0.2, 0.1])\n",
        "Y_pred_bad = np.array([0.1, 0.3, 0.6])\n",
        "l1 = cross_entropy(Y, Y_pred_good)\n",
        "l2 = cross_entropy(Y, Y_pred_bad)\n",
        "print(f'Loss1 numpy: {l1:.4f}')\n",
        "print(f'Loss2 numpy: {l2:.4f}') # the loss is  much higher (sample is very different form the one-hot encoded vector)\n",
        "\n",
        "\n",
        "# let's look at pytorch implemetation for cross entropy\n",
        "# careful!!\n",
        "# mm.CrossEntropyLoss applies nn.LogSoftmax + nn.NLLLoss (negative log likelihood loss)\n",
        "# -> No Softmax in the last layer\n",
        "# Y has class labels, not one-Hot (I should only put the correct class label)\n",
        "# Y_pred has raw scores (logits), no softmax\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "Y = torch.tensor([0]) # -> the true resutl is class 0 not one hote encoded anymore\n",
        "Y_pred_good = torch.tensor([[2.0, 1.0, 0.1]]) # careful!! this tensor has the size n_sample x n_classes (in this case we have 1 sample and 3 possible classes) -> we use the raw value, we don't apply the softmax\n",
        "Y_pred_bad = torch.tensor([[0.5, 2.0, 0.3]]) # this is a bad prediction\n",
        "\n",
        "l1 = loss(Y_pred_good, Y)\n",
        "l2 = loss(Y_pred_bad, Y)\n",
        "\n",
        "print(l1.item()) # I am calling the item method as the tensor has only one value\n",
        "print(l2.item())\n",
        "\n",
        "# _, because we do nto need this\n",
        "_, predictions1 = torch.max(Y_pred_good, 1) # we want the max along the first dimension\n",
        "_, predictions2 = torch.max(Y_pred_bad, 1) # we want the max along the first dimension\n",
        "print(predictions1) # in this case we choose class no. 0\n",
        "print(predictions2) # in this case we choose class no. 1\n",
        "\n",
        "# the loss in pytorch allow for multiple samples\n",
        "# 3 samples and 3 possible classes\n",
        "\n",
        "Y = torch.tensor([2, 0, 1])\n",
        "#  n_sample x n_classes\n",
        "Y_pred_good = torch.tensor([[0.1, 1.0, 2.1],[2.0, 1.0, 0.1],[0.1, 3.0, 0.1]]) # for example for the 2nd sample the value for the 0 class has the highest value\n",
        "Y_pred_bad = torch.tensor([[2.1, 1.0, 0.1],[1.0, 1.0, 2.1],[0.1, 3.0, 0.1]]) # for example for the 2nd sample the value for the 0 class has the highest value\n",
        "l1 = loss(Y_pred_good, Y)\n",
        "l2 = loss(Y_pred_bad, Y)\n",
        "\n",
        "print(l1.item()) # I am calling the item method as the tensor has only one value\n",
        "print(l2.item())\n",
        "\n",
        "# _, because we do nto need this, torch.max return a tuple, we want to ignore the first value\n",
        "# without _, this woudl return\n",
        "'''\n",
        "torch.return_types.max(\n",
        "values=tensor([2.1000, 2.0000, 3.0000]),\n",
        "indices=tensor([2, 0, 1])) # item at index #2 is the max\n",
        "'''\n",
        "_, predictions1 = torch.max(Y_pred_good, 1) # we want the max along the first dimension #\n",
        "_, predictions2 = torch.max(Y_pred_bad, 1) # we want the max along the first dimension\n",
        "print(predictions1) # in this case we choose class no. 0\n",
        "print(predictions2) # in this case we choose class no. 1 # we get the corrct prediciotn only for the 3rd sample\n",
        "\n",
        "\n",
        "# Binary classification\n",
        "class NeuralNet1(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(NeuralNet1, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear2(out)\n",
        "        # sigmoid at the end\n",
        "        y_pred = torch.sigmoid(out)\n",
        "        return y_pred\n",
        "\n",
        "model = NeuralNet1(input_size=28*28, hidden_size=5)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Multiclass problem\n",
        "class NeuralNet2(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet2, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear2(out)\n",
        "        # no softmax at the end\n",
        "        return out\n",
        "\n",
        "model = NeuralNet2(input_size=28*28, hidden_size=5, num_classes=3)\n",
        "criterion = nn.CrossEntropyLoss()  # (applies Softmax)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jOUCTKRctEsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12-Plot Activation Functions**"
      ],
      "metadata": {
        "id": "pqoKo5oWtGYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# activations functions decide whether a neuron should be activated or not\n",
        "# without activation functions the neural network would just be a stacked liner model\n",
        "\n",
        "\n",
        "\n",
        "##### Sigmoid typically used in the last layer of a binary classification problem\n",
        "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
        "\n",
        "x=np.linspace(-10,10,10)\n",
        "\n",
        "y=np.linspace(-10,10,100)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(y,sigmoid(y),'b', label='linspace(-10,10,100)')\n",
        "\n",
        "plt.grid(linestyle='--')\n",
        "\n",
        "plt.xlabel('X Axis')\n",
        "\n",
        "plt.ylabel('Y Axis')\n",
        "\n",
        "plt.title('Sigmoid Function')\n",
        "\n",
        "plt.xticks([-4, -3, -2, -1, 0, 1, 2, 3, 4])\n",
        "plt.yticks([-2, -1, 0, 1, 2])\n",
        "\n",
        "plt.ylim(-2, 2)\n",
        "plt.xlim(-4, 4)\n",
        "\n",
        "plt.show()\n",
        "#plt.savefig('sigmoid.png')\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "##### TanH\n",
        "tanh = lambda x: 2*sigmoid(2*x)-1\n",
        "\n",
        "x=np.linspace(-10,10,10)\n",
        "\n",
        "y=np.linspace(-10,10,100)\n",
        "\n",
        "plt.plot(y,tanh(y),'b', label='linspace(-10,10,100)')\n",
        "\n",
        "plt.grid(linestyle='--')\n",
        "\n",
        "plt.xlabel('X Axis')\n",
        "\n",
        "plt.ylabel('Y Axis')\n",
        "\n",
        "plt.title('TanH Function')\n",
        "\n",
        "plt.xticks([-4, -3, -2, -1, 0, 1, 2, 3, 4])\n",
        "plt.yticks([-4, -3, -2, -1, 0, 1, 2, 3, 4])\n",
        "\n",
        "plt.ylim(-4, 4)\n",
        "plt.xlim(-4, 4)\n",
        "\n",
        "plt.show()\n",
        "#plt.savefig('tanh.png')\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "##### ReLU # it wil output innputs for values > 0 and return 0 otherwise\n",
        "# MOST USED!!! rule of thum, if you do not know what value ot use, then use RELU as activation function for hidden layer\n",
        "relu = lambda x: np.where(x>=0, x, 0)\n",
        "\n",
        "x=np.linspace(-10,10,10)\n",
        "\n",
        "y=np.linspace(-10,10,1000)\n",
        "\n",
        "plt.plot(y,relu(y),'b', label='linspace(-10,10,100)')\n",
        "\n",
        "plt.grid(linestyle='--')\n",
        "\n",
        "plt.xlabel('X Axis')\n",
        "\n",
        "plt.ylabel('Y Axis')\n",
        "\n",
        "plt.title('ReLU')\n",
        "\n",
        "plt.xticks([-4, -3, -2, -1, 0, 1, 2, 3, 4])\n",
        "plt.yticks([-4, -3, -2, -1, 0, 1, 2, 3, 4])\n",
        "\n",
        "plt.ylim(-4, 4)\n",
        "plt.xlim(-4, 4)\n",
        "\n",
        "plt.show()\n",
        "#plt.savefig('relu.png')\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "##### Leaky ReLU\n",
        "# slightly modified and imporved version of the RELU function, trying to solve the problem of the vanishing problem\n",
        "# when you see wrights are not updating trying use the LEaky RELU as opposed to the RELU\n",
        "leakyrelu = lambda x: np.where(x>=0, x, 0.1*x)\n",
        "\n",
        "x=np.linspace(-10,10,10)\n",
        "\n",
        "y=np.linspace(-10,10,1000)\n",
        "\n",
        "plt.plot(y,leakyrelu(y),'b', label='linspace(-10,10,100)')\n",
        "\n",
        "plt.grid(linestyle='--')\n",
        "\n",
        "plt.xlabel('X Axis')\n",
        "\n",
        "plt.ylabel('Y Axis')\n",
        "\n",
        "plt.title('Leaky ReLU')\n",
        "\n",
        "plt.xticks([-4, -3, -2, -1, 0, 1, 2, 3, 4])\n",
        "plt.yticks([-4, -3, -2, -1, 0, 1, 2, 3, 4])\n",
        "\n",
        "plt.ylim(-4, 4)\n",
        "plt.xlim(-4, 4)\n",
        "\n",
        "plt.show()\n",
        "#plt.savefig('lrelu.png')\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "\n",
        "##### Binary Step\n",
        "bstep = lambda x: np.where(x>=0, 1, 0)\n",
        "\n",
        "x=np.linspace(-10,10,10)\n",
        "\n",
        "y=np.linspace(-10,10,1000)\n",
        "\n",
        "plt.plot(y,bstep(y),'b', label='linspace(-10,10,100)')\n",
        "\n",
        "plt.grid(linestyle='--')\n",
        "\n",
        "plt.xlabel('X Axis')\n",
        "\n",
        "plt.ylabel('Y Axis')\n",
        "\n",
        "plt.title('Step Function')\n",
        "\n",
        "plt.xticks([-4, -3, -2, -1, 0, 1, 2, 3, 4])\n",
        "plt.yticks([-2, -1, 0, 1, 2])\n",
        "\n",
        "plt.ylim(-2, 2)\n",
        "plt.xlim(-4, 4)\n",
        "\n",
        "plt.show()\n",
        "#plt.savefig('step.png')\n",
        "\n",
        "print('done')"
      ],
      "metadata": {
        "id": "Bo6ZzcBntLC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13-Feed Forward Network**"
      ],
      "metadata": {
        "id": "Vylirv5itV3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST\n",
        "# Dataloader, Transformation\n",
        "# Multilayer Neural Net, activation function\n",
        "# Loss and Optimizer\n",
        "# Training Loop (batch training)\n",
        "# Model evaluation\n",
        "# GPU support\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# device config\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# we will push our tensors on the device and thsi will guaranteer it will run on the GPU if it is supported\n",
        "\n",
        "# let's define some hyper parameters\n",
        "input_size = 784 # this is because our images have a size 28x28, then we will flat this array to be a 1-D tensor\n",
        "# let's define a hidden-size\n",
        "hidden_size = 100\n",
        "num_classes = 10 # we have digits from 0 to 9\n",
        "num_epochs = 2\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# let's import the famous MNIST dataset from the torch library\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root = './data', transform=transforms.ToTensor(), train = True, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root = './data', transform=transforms.ToTensor(), train = False) # no need to download it again\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) # no need to shuffle the data for testing\n",
        "\n",
        "# let's have a look at an example of this dataset\n",
        "examples = iter(train_loader)\n",
        "samples, labels = next(examples)\n",
        "print(samples.shape, labels.shape) # sample will have a shape [100, 1, 28, 28] -> 100 samples (our batch size is 100), 1 channel (only gray colour), width and depth of pics\n",
        "\n",
        "\n",
        "\n",
        "for i in range (6):\n",
        "    plt.subplot(2, 3, i+1) # i+1 is for the position of the picture in the 2 x 3 array\n",
        "    plt.imshow(samples[i][0], cmap='gray')\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# we want to classify these digits, for this we want to setup a fully connected neural network with one hidden layer\n",
        "class NeuralNet(nn.Module): # the class must be derived from nn.Module\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "\n",
        "        # we create our layers\n",
        "        self.l1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.l2(out)\n",
        "\n",
        "        # we have to be careful as we do not want to apply the softmax function which is included in the Cross Entropy loss\n",
        "        return out\n",
        "\n",
        "# we call out model\n",
        "model = NeuralNet(input_size, hidden_size, num_classes)\n",
        "\n",
        "# we create the loss and out optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "# now we can do our training loop\n",
        "n_total_steps = len(train_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader): # the data is a tuple of the images\n",
        "        # we need to reshape our images first as if we have a look at the shape this is 100, 1, 28, 28\n",
        "        # our input size is 784, out image tensor needs the size 100, 784\n",
        "        # we need to reshare out tensor first\n",
        "        images = images.reshape(-1, 28*28).to(device)# we will push it to the device if available\n",
        "        labels = labels.to(device) # we also push the lables to device\n",
        "\n",
        "        # forward\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # backwards\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'epoch{epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss= {loss.item():.4f}')\n",
        "\n",
        "# test\n",
        "# for testign we do not want to compute the gradient\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, 28*28).to(device)# we will push it to the device if available\n",
        "        labels = labels.to(device) # we also push the lables to device\n",
        "        outputs = model(images)\n",
        "\n",
        "        # we return the value and th eindex, we are just interested in the index, so we skip the values\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        n_samples += labels.shape[0] # this will give the number of samples in th batch, it shold be 100\n",
        "        n_correct += (predictions.eq(labels)).sum().item()\n",
        "        print(n_correct)\n",
        "    print('num samples', n_samples)\n",
        "    print('n_correct', n_correct)\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'accuracy = {acc}')"
      ],
      "metadata": {
        "id": "rvUn8B6dtbeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14-CNN**"
      ],
      "metadata": {
        "id": "RxR8_dVgtdYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "num_epochs = 5\n",
        "batch_size = 4\n",
        "learning_rate = 0.001\n",
        "\n",
        "# dataset has PILImage images of range [0, 1].\n",
        "# We transform them to Tensors of normalized range [-1, 1]\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                          shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
        "                                         shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # -> n, 3, 32, 32\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5\n",
        "        x = x.view(-1, 16 * 5 * 5)            # -> n, 400\n",
        "        x = F.relu(self.fc1(x))               # -> n, 120\n",
        "        x = F.relu(self.fc2(x))               # -> n, 84\n",
        "        x = self.fc3(x)                       # -> n, 10\n",
        "        return x\n",
        "\n",
        "\n",
        "model = ConvNet().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
        "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 2000 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print('Finished Training')\n",
        "PATH = './cnn.pth'\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    n_class_correct = [0 for i in range(10)]\n",
        "    n_class_samples = [0 for i in range(10)]\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            label = labels[i]\n",
        "            pred = predicted[i]\n",
        "            if (label == pred):\n",
        "                n_class_correct[label] += 1\n",
        "            n_class_samples[label] += 1\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "    for i in range(10):\n",
        "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "        print(f'Accuracy of {classes[i]}: {acc} %')\n",
        "\n"
      ],
      "metadata": {
        "id": "Yv7juuuwtgTN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}